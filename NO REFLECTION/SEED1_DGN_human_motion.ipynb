{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c9b56",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for the running of DYNAMICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7edd8365-2fe3-4354-b373-38befe37df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # Use GPU 1 for the operation\n",
    "\n",
    "\n",
    "seed = 32\n",
    "\n",
    "# 1) MUST do this before importing torch:\n",
    "os.environ[\"PYTHONHASHSEED\"]         = str(seed)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75740aa6",
   "metadata": {},
   "source": [
    "All of the code that is present in this notebook are present in the other python files of this Dynamical folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ee16f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c0afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import pickle as pkl\n",
    "import random\n",
    "import shutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4be8b",
   "metadata": {},
   "source": [
    "### Dictionary that sets the model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b91ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings ={\n",
    "        \"batch_size\": 100,\n",
    "        \"epochs\": 600,\n",
    "        \"lr\": 5e-4,\n",
    "        \"nf\": 64,\n",
    "        \"model\": \"gmn\",\n",
    "        \"attention\": 0,\n",
    "        \"n_layers\": 4,\n",
    "        \"max_testing_samples\": 600,\n",
    "        \"max_training_samples\": 200,\n",
    "        \"data_dir\": \"/Users/visharma/Desktop/Repositories/Dynamical_codebase/HUMAN_WALK/data\",\n",
    "        \"norm_diff\": False,\n",
    "        \"weight_decay\": 1e-10,\n",
    "        \"tanh\": False,\n",
    "        \"learnable\": False,\n",
    "        \"finite_diff\":True,\n",
    "        \"time_step\":1.0,\n",
    "        \"end_time_step\": 30.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c81ab9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, partition='train', max_samples=600, data_dir='', nsteps=1):\n",
    "        self.partition = partition\n",
    "        self.data_dir = data_dir\n",
    "        self.nsteps = nsteps\n",
    "\n",
    "        # --- load raw data --------------------------------------\n",
    "        with open(os.path.join(data_dir, 'motion.pkl'), 'rb') as f:\n",
    "            edges, X = pkl.load(f)\n",
    "\n",
    "        # your smoothing / central_diff code here...\n",
    "        Ps, Vs, As = self.central_diff(X)\n",
    "\n",
    "        # trial IDs must match exactly\n",
    "        train_case_id = [20,1,17,13,14,9,4,2,7,5,16]\n",
    "        val_case_id   = [3,8,11,12,15,18]\n",
    "        test_case_id  = [6,19,21,0,22,10]\n",
    "\n",
    "        # --- load or create competitor splits (fixed for central_diff) ----------\n",
    "        split_path = os.path.join(data_dir, f'split_n{self.nsteps}.pkl')\n",
    "        try:\n",
    "            with open(split_path, 'rb') as f:\n",
    "                train_mapping, val_mapping, test_mapping = pkl.load(f)\n",
    "                print(\"Loaded competitor split!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Generating competitor split…\")\n",
    "\n",
    "            def make_map(case_ids):\n",
    "                mapping = {}\n",
    "                for i in case_ids:\n",
    "                    core_len = Ps[i].shape[0]                    # <<— use length after central_diff\n",
    "                    safe_max = core_len - self.nsteps*30 - 1\n",
    "                    if safe_max < 0:\n",
    "                        raise ValueError(f\"Trial {i} too short for look-ahead of {self.nsteps} steps.\")\n",
    "                    # competitor caps at 300\n",
    "                    itv = min(300, safe_max + 1)                # +1 because j in [0..safe_max]\n",
    "                    pool = np.arange(itv)                       # j ∈ [0..itv-1]\n",
    "                    mapping[i] = np.random.choice(pool, size=100, replace=False)\n",
    "                return mapping\n",
    "\n",
    "            train_mapping = make_map(train_case_id)\n",
    "            val_mapping   = make_map(val_case_id)\n",
    "            test_mapping  = make_map(test_case_id)\n",
    "\n",
    "            with open(split_path, 'wb') as f:\n",
    "                pkl.dump((train_mapping, val_mapping, test_mapping), f)\n",
    "            print(\"Saved competitor split!\")\n",
    "\n",
    "        # pick the mapping you need\n",
    "        if   partition == 'train': mapping = train_mapping\n",
    "        elif partition == 'val'  : mapping = val_mapping\n",
    "        elif partition == 'test' : mapping = test_mapping\n",
    "        else: raise ValueError(f\"Unknown partition {partition!r}\")\n",
    "\n",
    "        # now proceed exactly as before, using `mapping` instead of your make_mapping\n",
    "        each_len = max_samples // len(mapping)\n",
    "        in_graphs = []\n",
    "        for i, pool in mapping.items():\n",
    "            for j in pool[:each_len]:\n",
    "                # note: they use delta_frame; you have nsteps*30, so this is identical\n",
    "                cur_x_t   = Ps[i][j]\n",
    "                cur_v_t   = Vs[i][j]\n",
    "                cur_v_tm1 = Vs[i][j-1]\n",
    "                y_dv      = Vs[i][j + self.nsteps*30] - Vs[i][j]\n",
    "                y_dx      = Ps[i][j + self.nsteps*30] - Ps[i][j]\n",
    "                y_pos_end = Ps[i][j + self.nsteps*30]\n",
    "                y_vel_end = Vs[i][j + self.nsteps*30]\n",
    "\n",
    "                in_graphs.append(self.create_in_graph(\n",
    "                    edges,\n",
    "                    x=(cur_x_t, cur_v_t, cur_v_tm1),\n",
    "                    y=(y_dv, y_dx, y_pos_end, y_vel_end)\n",
    "                ))\n",
    "\n",
    "        self.in_graphs = in_graphs\n",
    "        print(f\"[HumanDataset:{partition}] built {len(in_graphs)} samples\")\n",
    "\n",
    "    def central_diff(self, Xs, dt: float = 1.0, window_length: int = 41):\n",
    "        Ps, Vs, As = [], [], []\n",
    "        for x in Xs:\n",
    "            v      = (x[2:] - x[:-2]) / (2*dt)\n",
    "            a      = (x[2:] - 2*x[1:-1] + x[:-2]) / (dt**2)\n",
    "            p      = x[1:-1]                      # align to v,a\n",
    "            Ps.append(p)\n",
    "            Vs.append(v)\n",
    "            As.append(a)\n",
    "        return Ps, Vs, As\n",
    "\n",
    "        \n",
    "    def get_foot_nodes(self, nodes):\n",
    "        foot_indices = np.argsort(nodes[:,1])[:6]\n",
    "        foot_pos = nodes[foot_indices]\n",
    "        return foot_pos, foot_indices\n",
    "    \n",
    "    def reflected_nodes(self, nodes, z0=0, epsilon=1e-3):\n",
    "        reflected = nodes.copy()\n",
    "        reflected[:,1] = 2*z0 - nodes[:,1] - epsilon\n",
    "        distances = reflected[:,1] - nodes[:,1]\n",
    "        return reflected, distances\n",
    "    \n",
    "    def find_min(self, nodes):\n",
    "        return np.min(nodes, axis=0)\n",
    "    \n",
    "\n",
    "    def create_edges(self, N, edges):\n",
    "        atom_edges = torch.zeros(N, N).int()\n",
    "        for edge in edges:\n",
    "            atom_edges[edge[0], edge[1]] = 1\n",
    "            atom_edges[edge[1], edge[0]] = 1\n",
    "\n",
    "        atom_edges2 = atom_edges @ atom_edges\n",
    "        self.atom_edge = atom_edges\n",
    "        self.atom_edge2 = atom_edges2\n",
    "        edge_attr = []\n",
    "        # Initialize edges and edge_attributes\n",
    "        rows, cols = [], []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    if atom_edges[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([1])\n",
    "                        assert not atom_edges2[i][j]\n",
    "                    if atom_edges2[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([2])\n",
    "                        assert not atom_edges[i][j]\n",
    "\n",
    "        edges = [rows, cols] \n",
    "        edge_attr = torch.Tensor(np.array(edge_attr))  # [edge, 3]\n",
    "        edge_idx =torch.tensor(edges, dtype=torch.long)  # [2, M]   \n",
    "        return edge_idx,edge_attr     \n",
    "    \n",
    "    \n",
    "    def create_in_graph(self, edges,x,y):\n",
    "        pos_t, vel_t, vel_tm1 = x\n",
    "        y_dv,y_dx,y_pos_end,y_vel_end = y\n",
    "\n",
    "        edge_idx,edge_attr = self.create_edges(pos_t.shape[0], edges)\n",
    "\n",
    "        # # Get the ground node\n",
    "        # z0_t = self.find_min(pos_t)[1]\n",
    "        # z0_end = self.find_min(y_end)[1]\n",
    "        # # Center the y-positions around z0 for input and target\n",
    "        # pos_t -= np.array([0, z0_t, 0]) \n",
    "        # y_end -= np.array([0, z0_end, 0])\n",
    "\n",
    "        # Get the foot node positions and indices\n",
    "        # foot_nodes_positions, foot_nodes_indices = self.get_foot_nodes(pos_t)\n",
    "        # foot_nodes_reflected, foot_distances = self.reflected_nodes(foot_nodes_positions,z0=0.0)\n",
    "        \n",
    "        # current_largest_node_index = pos_t.shape[0]\n",
    "        # reflected_nodes_indices = []\n",
    "        # for reflected_node in range(foot_nodes_indices.shape[0]):\n",
    "        #     reflected_node_index = current_largest_node_index\n",
    "        #     current_largest_node_index += 1\n",
    "        #     reflected_nodes_indices.append(reflected_node_index)\n",
    "        \n",
    "        \n",
    "        # # Set lists to torch tensors\n",
    "        # reflected_nodes_indices = torch.tensor(reflected_nodes_indices)\n",
    "        # foot_nodes_indices = torch.tensor(foot_nodes_indices)\n",
    "        pos_t = torch.tensor(pos_t)\n",
    "        vel_t = torch.tensor(vel_t)\n",
    "        vel_tm1 = torch.tensor(vel_tm1)\n",
    "\n",
    "        y_dv = torch.tensor(y_dv)\n",
    "        y_dx = torch.tensor(y_dx)\n",
    "        y_pos_end = torch.tensor(y_pos_end)\n",
    "        y_vel_end = torch.tensor(y_vel_end)\n",
    "        \n",
    "        \n",
    "        # foot_nodes_reflected = torch.tensor(foot_nodes_reflected)\n",
    "        \n",
    "        # Set the node type of feet to one\n",
    "        node_type = torch.ones(pos_t.shape[0],1)\n",
    "        # node_type[foot_nodes_indices] = 1\n",
    "        # # Make reflected nodes of type 2\n",
    "        # new_node_type = torch.vstack((node_type,2*torch.ones_like(reflected_nodes_indices).unsqueeze(1))) \n",
    "        \n",
    "        # New bi-dir edge indexes\n",
    "        # new_edges_ref = torch.hstack((foot_nodes_indices.unsqueeze(1), reflected_nodes_indices.unsqueeze(1))) # connect foot edges to their reflections\n",
    "        # new_edges_ref = new_edges_ref.t()  # now [2, M]\n",
    "        # rev_new_edges_ref = new_edges_ref.flip(0)  # reverse the order to match edge index format\n",
    "        # new_edges_bidir_ref = torch.cat((new_edges_ref, rev_new_edges_ref), dim=1)  # add reverse edges\n",
    "        # new_edge_index = torch.cat([edge_idx, new_edges_bidir_ref], dim=1) # add new edges to the graph edge index\n",
    "        # s,r = new_edge_index\n",
    "\n",
    "        # we add the 1 as edge attr for these edges as they are 1 hop\n",
    "        # new_edge_attr = torch.vstack((edge_attr, torch.ones((new_edges_bidir_ref.shape[1], 1))))  # add new edge attributes\n",
    "        # for differentiating reflected edges we use another features i.e. type_sender*type_receiver\n",
    "        # new_edge_attr = torch.hstack((new_edge_attr,\n",
    "        #                               new_node_type[s]*new_node_type[r]))\n",
    "        # new_pos_t = torch.vstack((pos_t, foot_nodes_reflected))\n",
    "        # new_vel_t = torch.vstack((vel_t,torch.zeros_like(foot_nodes_reflected)))\n",
    "        # new_vel_tm1 = torch.vstack((vel_tm1,torch.zeros_like(foot_nodes_reflected)))\n",
    "\n",
    "        \n",
    "        # in_graph = Data(x=new_pos_t,edge_index=new_edge_index,edge_attr=new_edge_attr)\n",
    "        # in_graph.node_vel_t = new_vel_t\n",
    "        # in_graph.node_vel_tm1 = new_vel_tm1\n",
    "        # in_graph.y_dv = y_dv\n",
    "        # in_graph.y_dx = y_dx\n",
    "        # in_graph.y_end = y_end\n",
    "        # in_graph.node_type = new_node_type\n",
    "\n",
    "        in_graph = Data(edge_index=edge_idx, edge_attr=edge_attr)\n",
    "        in_graph.pos = pos_t\n",
    "        in_graph.vel = vel_t\n",
    "        in_graph.prev_vel = vel_tm1\n",
    "        in_graph.y_dv = y_dv\n",
    "        in_graph.y_dx = y_dx\n",
    "        in_graph.end_pos = y_pos_end\n",
    "        in_graph.end_vel = y_vel_end\n",
    "        in_graph.node_type = node_type\n",
    "        \n",
    "        return in_graph     \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.in_graphs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.in_graphs[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be11086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded competitor split!\n",
      "[HumanDataset:train] built 198 samples\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "Loaded competitor split!\n",
      "[HumanDataset:val] built 600 samples\n"
     ]
    }
   ],
   "source": [
    "dataset_train = HumanDataset(partition='train', max_samples=model_settings[\"max_training_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)\n",
    "dataset_test = HumanDataset(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)\n",
    "dataset_val = HumanDataset(partition='val', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c270d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All edges are bidirectional.\n"
     ]
    }
   ],
   "source": [
    "gt = next(iter(dataset_val))\n",
    "# Convert to set of tuples\n",
    "edges = set((i.item(), j.item()) for i, j in zip(gt.edge_index[0], gt.edge_index[1]))\n",
    "\n",
    "# Check for missing reverse edges\n",
    "missing = [(j, i) for (i, j) in edges if (j, i) not in edges]\n",
    "\n",
    "if len(missing) == 0:\n",
    "    print(\" All edges are bidirectional.\")\n",
    "else:\n",
    "    print(f\"{len(missing)} edge(s) are not bidirectional:\")\n",
    "    print(missing[:10])  # print a few missing edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cc051",
   "metadata": {},
   "source": [
    "The next cell creates the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7f784c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(data):\n",
    "    graph_current = data\n",
    "    \n",
    "    return graph_current\n",
    "\n",
    "class GraphFromRawDataset(Dataset):\n",
    "    def __init__(self, raw_dataset):\n",
    "        self.raw_dataset = raw_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.raw_dataset[idx]\n",
    "        return create_graph_data(data)\n",
    "\n",
    "def create_dataloaders_from_raw(dataset, M, shuffle=True):\n",
    "    \"\"\"\n",
    "    M: number of graphs to batch in training (use 1 for test/val)\n",
    "    \"\"\"\n",
    "    # Wrap raw datasets into PyTorch Dataset\n",
    "    dataset = GraphFromRawDataset(dataset)\n",
    "\n",
    "    # Create dataloaders\n",
    "    loader = DataLoader(dataset, batch_size=M, shuffle=shuffle, collate_fn=Batch.from_data_list)\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28af6d2c-14cb-4f14-b0cf-b6a14387d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloaders_from_raw(dataset_train,M=model_settings[\"batch_size\"])\n",
    "val_loader = create_dataloaders_from_raw(dataset_val,M=model_settings[\"batch_size\"], shuffle=False)\n",
    "test_loader = create_dataloaders_from_raw(dataset_test,M=model_settings[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f481e76-50e2-42df-96f9-372eadad2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_max_edge(train_loader):\n",
    "    \"\"\"\n",
    "    Calculate min/max statistics for graph properties using the ConsecutiveGraphDataset.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    timestep_dict : dict\n",
    "        Dictionary containing graphs with keys as timesteps.\n",
    "    time_step_increment : int, optional\n",
    "        Time step increment to use (default: 1).\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    tuple:\n",
    "        Min/max statistics for various physical properties.\n",
    "    \"\"\"\n",
    "    # Initialize lists to collect data\n",
    "    all_edge_dx = []\n",
    "    all_node_v_t = []\n",
    "    all_node_v_tm1 = []\n",
    "    all_node_dv = []\n",
    "    all_node_dx = []\n",
    "    \n",
    "    # Process all valid timesteps\n",
    "    for batch in train_loader:\n",
    "        batched_graph = batch\n",
    "        \n",
    "        senders,receivers = batched_graph.edge_index\n",
    "\n",
    "        edge_dx = batched_graph.pos[receivers] - batched_graph.pos[senders]\n",
    "        \n",
    "        # Extract positions and velocities\n",
    "        node_vel_t = batched_graph.vel.float()\n",
    "        node_vel_tm1 = batched_graph.prev_vel.float()\n",
    "        \n",
    "        # Calculate displacements and acceleration changes\n",
    "        mask_body = (batched_graph.node_type!=2).squeeze()\n",
    "        node_dv = (batched_graph.y_dv).float()\n",
    "        node_dx = (batched_graph.y_dx).float()\n",
    "        \n",
    "        # Collect data\n",
    "        all_edge_dx.append(edge_dx)\n",
    "        all_node_v_t.append(node_vel_t)\n",
    "        all_node_v_tm1.append(node_vel_tm1)\n",
    "        all_node_dv.append(node_dv)\n",
    "        all_node_dx.append(node_dx)\n",
    "    \n",
    "    # Concatenate all collected data\n",
    "    all_edge_dx = torch.cat(all_edge_dx, dim=0)\n",
    "    all_node_v_t = torch.cat(all_node_v_t, dim=0)\n",
    "    all_node_v_tm1 = torch.cat(all_node_v_tm1, dim=0)\n",
    "    all_node_dv= torch.cat(all_node_dv, dim=0)\n",
    "    all_node_dx = torch.cat(all_node_dx,dim=0)\n",
    "    \n",
    "    # Compute norms\n",
    "    norm_edge_dx = all_edge_dx.norm(dim=1)\n",
    "    norm_node_v_t = all_node_v_t.norm(dim=1)\n",
    "    norm_node_v_tm1 = all_node_v_tm1.norm(dim=1)\n",
    "    norm_node_dv = all_node_dv.norm(dim=1)\n",
    "    norm_node_dx = all_node_dx.norm(dim=1)\n",
    "    \n",
    "    # Compute min and max values of the norms\n",
    "    min_edge_dx = norm_edge_dx.min()\n",
    "    max_edge_dx = norm_edge_dx.max()\n",
    "\n",
    "    min_node_v_t = norm_node_v_t.min()\n",
    "    max_node_v_t = norm_node_v_t.max()\n",
    "\n",
    "    min_node_v_tm1 = norm_node_v_tm1.min()\n",
    "    max_node_v_tm1 = norm_node_v_tm1.max()\n",
    "\n",
    "    mean_node_dv = norm_node_dv.mean()\n",
    "    std_node_dv = norm_node_dv.std()\n",
    "\n",
    "    mean_node_dx = norm_node_dx.mean()\n",
    "    std_node_dx = norm_node_dx.std()\n",
    "\n",
    "    # Collect statistics in tuples\n",
    "    stat_edge_dx = (min_edge_dx, max_edge_dx)\n",
    "    stat_node_v_t = (min_node_v_t, max_node_v_t)\n",
    "    stat_node_v_tm1 = (min_node_v_tm1, max_node_v_tm1)\n",
    "    stat_node_dv = (mean_node_dv, std_node_dv)\n",
    "    stat_node_dx = (mean_node_dx, std_node_dx)\n",
    "    \n",
    "    return stat_edge_dx, stat_node_v_t, stat_node_dv, stat_node_dx\n",
    "\n",
    "def move_train_stats_to_device(train_stats, device):\n",
    "    def move_to_device(stat):\n",
    "        if len(stat)==2:\n",
    "            min_val, max_val = stat\n",
    "            return min_val.to(device), max_val.to(device)\n",
    "        else: \n",
    "            max_val = stat\n",
    "            return max_val.to(device)\n",
    "        \n",
    "\n",
    "    return tuple(move_to_device(stat) for stat in train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13699dea",
   "metadata": {},
   "source": [
    "## Dynamical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da637d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = calculate_min_max_edge(train_loader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_stats = move_train_stats_to_device(stats, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b93cc",
   "metadata": {},
   "source": [
    "### Components for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ebc83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_d(in_size, hidden_size, out_size, num_layers=1, lay_norm=True, use_sigmoid=False, use_softmax=False):\n",
    "    \"\"\"\n",
    "    Builds a multi-layer perceptron (MLP) with configurable depth and optional layer normalization and sigmoid or softmax activation.\n",
    "\n",
    "    Args:\n",
    "        in_size (int): The size of the input feature vector.\n",
    "        hidden_size (int): The size of the hidden layers.\n",
    "        out_size (int): The size of the output layer.\n",
    "        num_layers (int): The number of layers in the MLP.\n",
    "        lay_norm (bool): Flag to add layer normalization after the last linear layer.\n",
    "        use_sigmoid (bool): Flag to add a sigmoid activation layer at the output.\n",
    "        use_softmax (bool): Flag to add a softmax activation layer at the output.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The constructed MLP model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If both use_sigmoid and use_softmax are True.\n",
    "    \"\"\"\n",
    "    if use_sigmoid and use_softmax:\n",
    "        raise ValueError(\"Only one of use_sigmoid or use_softmax can be true.\")\n",
    "    layers = [nn.Linear(in_size, hidden_size), nn.ReLU()]\n",
    "    \n",
    "    # Add intermediate layers\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "    # Add the output layer\n",
    "    layers.append(nn.Linear(hidden_size, out_size))\n",
    "\n",
    "    # Create the MLP module\n",
    "    module = nn.Sequential(*layers)\n",
    "\n",
    "    # Optionally add layer normalization\n",
    "    if lay_norm:\n",
    "        module = nn.Sequential(module, nn.LayerNorm(normalized_shape=out_size))\n",
    "\n",
    "    # Optionally add sigmoid activation\n",
    "    if use_sigmoid:\n",
    "        module = nn.Sequential(module, nn.Sigmoid())\n",
    "\n",
    "    # Optionally add softmax activation\n",
    "    if use_softmax:\n",
    "        module = nn.Sequential(module, nn.Softmax(dim=-1))\n",
    "\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18ebb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefFrameCalc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefFrameCalc, self).__init__()\n",
    "\n",
    "    def forward(self, edge_index,senders_pos,receivers_pos, senders_vel,receivers_vel):\n",
    "        \n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        vector_a = (receivers_pos - senders_pos)/ torch.clamp((receivers_pos - senders_pos).norm(dim=1, keepdim=True), min=epsilon)\n",
    "\n",
    "        #prelimnary vectors\n",
    "        b_a = torch.cross(receivers_vel-senders_vel,vector_a,dim=1)\n",
    "        b_a = b_a / torch.clamp(b_a.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        b_c = (senders_vel + receivers_vel)\n",
    "        b_c = b_c / torch.clamp(b_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        \n",
    "        b = b_a + b_c \n",
    "\n",
    "        # Compute the parallel component of b\n",
    "        b_prl_dot = torch.einsum('ij,ij->i', b, vector_a).unsqueeze(1)\n",
    "        b_prl = b_prl_dot * vector_a\n",
    "\n",
    "        # Compute the perpendicular component of b\n",
    "        b_prp = b - b_prl\n",
    "\n",
    "        vector_b = torch.cross(b_prp, vector_a,dim=1) #perp to a and a new vector b_prp\n",
    "        vector_c = torch.cross(b_prl, vector_b,dim=1) #perp to a and b\n",
    "        \n",
    "        vector_b = vector_b / torch.clamp(vector_b.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        vector_c = vector_c / torch.clamp(vector_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "   \n",
    "        return vector_a, vector_b, vector_c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12582806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.node_encoder = build_mlp_d(2, latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "    def forward(self,node_scalar_feat):\n",
    "        node_latent = self.node_encoder(node_scalar_feat)  \n",
    "        return node_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "679e5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionEncoder(nn.Module):\n",
    "    \"\"\"Message passing.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size):\n",
    "        super(InteractionEncoder, self).__init__()\n",
    "        self.edge_feat_encoder = build_mlp_d(6, latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "        self.edge_encoder = build_mlp_d(2, latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "        self.interaction_encoder = build_mlp_d(3*latent_size,latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "    def forward(self, edge_index, edge_dx_,edge_attr, vector_a, vector_b, vector_c,\n",
    "                senders_v_t_, senders_v_tm1_,\n",
    "                receivers_v_t_, receivers_v_tm1_,\n",
    "                node_latent):\n",
    "\n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        node_v_t_senders_a = torch.einsum('ij,ij->i', senders_v_t_, vector_a).unsqueeze(1)\n",
    "        node_v_t_senders_b = torch.einsum('ij,ij->i', senders_v_t_, vector_b).unsqueeze(1)\n",
    "        node_v_t_senders_c = torch.einsum('ij,ij->i', senders_v_t_, vector_c).unsqueeze(1)\n",
    "\n",
    "        node_v_tm1_senders_a = torch.einsum('ij,ij->i', senders_v_tm1_, vector_a).unsqueeze(1)\n",
    "        node_v_tm1_senders_b = torch.einsum('ij,ij->i', senders_v_tm1_, vector_b).unsqueeze(1)\n",
    "        node_v_tm1_senders_c = torch.einsum('ij,ij->i', senders_v_tm1_, vector_c).unsqueeze(1)     \n",
    "        \n",
    "        node_v_t_receivers_a = torch.einsum('ij,ij->i', receivers_v_t_, -vector_a).unsqueeze(1)\n",
    "        node_v_t_receivers_b = torch.einsum('ij,ij->i', receivers_v_t_, -vector_b).unsqueeze(1)\n",
    "        node_v_t_receivers_c = torch.einsum('ij,ij->i', receivers_v_t_, -vector_c).unsqueeze(1)\n",
    "\n",
    "        node_v_tm1_receivers_a = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_a).unsqueeze(1)\n",
    "        node_v_tm1_receivers_b = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_b).unsqueeze(1)\n",
    "        node_v_tm1_receivers_c = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_c).unsqueeze(1)       \n",
    "        \n",
    "        edge_dx_a_s = edge_dx_.norm(dim=1,keepdim=True)\n",
    "\n",
    "        senders_features = torch.hstack((\n",
    "            node_v_t_senders_a, node_v_t_senders_b, node_v_t_senders_c,\n",
    "            node_v_tm1_senders_a, node_v_tm1_senders_b, node_v_tm1_senders_c,\n",
    "        ))\n",
    "\n",
    "        receivers_features = torch.hstack((\n",
    "            node_v_t_receivers_a, node_v_t_receivers_b, node_v_t_receivers_c,\n",
    "            node_v_tm1_receivers_a, node_v_tm1_receivers_b, node_v_tm1_receivers_c,\n",
    "        ))\n",
    "        \n",
    "        edge_latent = self.edge_encoder(torch.hstack((edge_dx_a_s, edge_attr)))\n",
    "\n",
    "        senders_latent = self.edge_feat_encoder(senders_features)\n",
    "        receivers_latent = self.edge_feat_encoder(receivers_features)\n",
    "\n",
    "        interaction_latent = self.interaction_encoder(torch.hstack((senders_latent + receivers_latent,\n",
    "                                                                    node_latent[senders]+node_latent[receivers],\n",
    "                                                                    edge_latent)))\n",
    "\n",
    "        return interaction_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "368fce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(InteractionDecoder, self).__init__()\n",
    "        self.i1_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=2, lay_norm=False)\n",
    "        self.i2_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=2, lay_norm=False)\n",
    "\n",
    "    def forward(self, edge_index, senders_pos, receivers_pos, vector_a, vector_b, vector_c, interaction_latent, node_latent):\n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        coeff_f = self.i1_decoder(interaction_latent)\n",
    "        coeff_t = self.i2_decoder(interaction_latent)\n",
    "\n",
    "        fij = (coeff_f[:, 0:1] * vector_a + \n",
    "              coeff_f[:, 1:2] * vector_b + \n",
    "              coeff_f[:, 2:] * vector_c)\n",
    "        \n",
    "        \n",
    "        tij = (coeff_t[:, 0:1] * vector_a + \n",
    "              coeff_t[:, 1:2] * vector_b + \n",
    "              coeff_t[:, 2:] * vector_c)        \n",
    "        return fij, tij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a29b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_Internal_Dv_Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(Node_Internal_Dv_Decoder, self).__init__()\n",
    "        self.m_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=2, lay_norm=False)\n",
    "        self.i_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=2, lay_norm=False)\n",
    "        self.dv_ext_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=2, lay_norm=False)\n",
    "    def forward(self,edge_index,node_latent,fij,tij):\n",
    "        m_inv = self.m_inv_decoder(node_latent) # decode inverse of mass\n",
    "        i_inv = self.i_inv_decoder(node_latent) # decode inverse of inertia\n",
    "        senders,receivers = edge_index   \n",
    "        \n",
    "        out_fij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_fij = out_fij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]).to(device), fij.to(device))\n",
    "        node_dv_int = m_inv * (out_fij) + self.dv_ext_decoder(node_latent)\n",
    "        \n",
    "        out_tij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_tij = out_tij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]).to(device), tij.to(device))\n",
    "        node_dw_int = i_inv * out_tij       \n",
    "\n",
    "        return node_dv_int, node_dw_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70559816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scaler, self).__init__()\n",
    "        '''\n",
    "        Scales the velocity and angular velocity features by maximum magnitude of respective field in training data\n",
    "        Scales the magnitude of the edge_vector_dx using min-max scaling. (keeping the direction of edge_vector_dx same)\n",
    "        '''\n",
    "\n",
    "    def forward(self, senders_v_t, senders_v_tm1,receivers_v_t, receivers_v_tm1,edge_dx,train_stats):\n",
    "        stat_edge_dx, stat_node_v_t, _,_= train_stats\n",
    "        \n",
    "        senders_v_t_ = senders_v_t/stat_node_v_t[1].detach()\n",
    "        senders_v_tm1_ = senders_v_tm1/stat_node_v_t[1].detach()\n",
    "        receivers_v_t_ = receivers_v_t/stat_node_v_t[1].detach()\n",
    "        receivers_v_tm1_ = receivers_v_tm1/stat_node_v_t[1].detach()\n",
    "        norm_edge_dx = edge_dx.norm(dim=1, keepdim=True)\n",
    "        edge_dx_ = (((norm_edge_dx-stat_edge_dx[0])/(stat_edge_dx[1]-stat_edge_dx[0]))*(edge_dx/norm_edge_dx)).detach()\n",
    "        return senders_v_t_, senders_v_tm1_,receivers_v_t_, receivers_v_tm1_,edge_dx_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60303a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction_Block(torch.nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Interaction_Block, self).__init__()\n",
    "        self.interaction_encoder = InteractionEncoder(latent_size)\n",
    "        self.interaction_decoder = InteractionDecoder(latent_size)\n",
    "        self.internal_dv_decoder = Node_Internal_Dv_Decoder(latent_size)\n",
    "        self.layer_norm = nn.LayerNorm(latent_size)\n",
    "\n",
    "    def forward(self, edge_index, senders_pos, receivers_pos, edge_dx_, edge_attr,vector_a, vector_b, vector_c, \n",
    "                senders_v_t_, senders_v_tm1_,\n",
    "                receivers_v_t_, receivers_v_tm1_,\n",
    "                node_latent, residue=None, latent_history=False):\n",
    "            interaction_latent = self.interaction_encoder(edge_index, edge_dx_,edge_attr,\n",
    "                                                          vector_a, vector_b, vector_c,\n",
    "                                                          senders_v_t_, senders_v_tm1_,\n",
    "                                                          receivers_v_t_, receivers_v_tm1_,\n",
    "                                                          node_latent)\n",
    "\n",
    "            if latent_history:\n",
    "                interaction_latent = interaction_latent + residue\n",
    "                interaction_latent = self.layer_norm(interaction_latent)\n",
    "            \n",
    "            edge_interaction_force, edge_interaction_tau= self.interaction_decoder(\n",
    "                edge_index, senders_pos, receivers_pos, vector_a, vector_b, vector_c, interaction_latent, node_latent\n",
    "            )\n",
    "            node_dv_int_decoded, node_dw_int_decoded = self.internal_dv_decoder(\n",
    "                edge_index, node_latent, edge_interaction_force, edge_interaction_tau\n",
    "            )\n",
    "        \n",
    "            return node_dv_int_decoded, node_dw_int_decoded, interaction_latent\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d132beaf-8cfb-4a78-9181-8b58c9d119d5",
   "metadata": {},
   "source": [
    "class DynamicsSolver(torch.nn.Module):\n",
    "    def __init__(self, sample_step, train_stats, num_jumps=1, num_msgs=1, latent_size=128):\n",
    "        super(DynamicsSolver, self).__init__()\n",
    "        self.refframecalc = RefFrameCalc()\n",
    "        self.scaler = Scaler()\n",
    "        self.node_encoder = NodeEncoder(latent_size)\n",
    "        self.interaction_proc_layer = Interaction_Block(latent_size)\n",
    "        self.interaction_init_layer = Interaction_Block(latent_size)\n",
    "        self.num_messages = num_msgs\n",
    "        self.sub_tstep = sample_step / num_msgs\n",
    "        self.train_stats = train_stats\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # Initialize graph data for processing\n",
    "        device = graph.pos.device\n",
    "        graph = graph.to(device)\n",
    "        #node_type = torch.ones_like(graph.disp[:,0:1]).float()# get_node_type(graph).float()\n",
    "        node_type = graph.node_type.float()\n",
    "        pos = graph.pos.float()\n",
    "        vel = graph.vel.float()\n",
    "        prev_vel = graph.prev_vel.float()\n",
    "        \n",
    "        edge_index = graph.edge_index.long()\n",
    "        senders, receivers = edge_index\n",
    "        senders_pos = pos[senders]\n",
    "        receivers_pos = pos[receivers]\n",
    "        edge_dx = receivers_pos - senders_pos\n",
    "        edge_attr = graph.edge_attr.float()\n",
    "        \n",
    "        mask_reflected_node = (graph.node_type!=2).squeeze()\n",
    "\n",
    "        node_v_t = vel\n",
    "        node_w_t = getattr(graph, 'node_w_t',torch.zeros_like(node_v_t))\n",
    "        node_th_t = getattr(graph, 'node_th_t', torch.zeros_like(node_v_t))\n",
    "\n",
    "        senders_v_t = node_v_t[senders].float()\n",
    "        receivers_v_t = node_v_t[receivers].float()\n",
    "\n",
    "        senders_v_tm1 = prev_vel[senders].float()\n",
    "        receivers_v_tm1 = prev_vel[receivers].float()\n",
    "\n",
    "        senders_w_t = node_w_t[senders]\n",
    "        receivers_w_t = node_w_t[receivers]\n",
    "        \n",
    "        senders_th_t = node_th_t[senders]\n",
    "        receivers_th_t = node_th_t[receivers]        \n",
    "\n",
    "        node_disp = torch.zeros_like(node_v_t)\n",
    "        node_vf = torch.zeros_like(node_v_t)\n",
    "        node_wf = torch.zeros_like(node_v_t)\n",
    "\n",
    "        sum_node_dv = torch.zeros_like(node_v_t)\n",
    "        sum_node_dx = torch.zeros_like(node_v_t)\n",
    "        \n",
    "        node_latent = self.node_encoder(torch.hstack((node_type,vel[:,1:2])))\n",
    "\n",
    "\n",
    "        for i in range(self.num_messages):\n",
    "            (\n",
    "                senders_v_t_,\n",
    "                senders_v_tm1_,\n",
    "                receivers_v_t_,\n",
    "                receivers_v_tm1_,\n",
    "                edge_dx_,\n",
    "            ) = self.scaler(\n",
    "                senders_v_t,\n",
    "                senders_v_tm1,\n",
    "                receivers_v_t,\n",
    "                receivers_v_tm1,\n",
    "                edge_dx,\n",
    "                self.train_stats,\n",
    "            )\n",
    "\n",
    "\n",
    "            vector_a, vector_b, vector_c = self.refframecalc(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    senders_v_t_,\n",
    "                    receivers_v_t_,\n",
    "                    senders_w_t,\n",
    "                    receivers_w_t\n",
    "                    \n",
    "                )\n",
    "\n",
    "            if i == 0:\n",
    "                node_dv_int_decoded, node_dw_int_decoded,residue= self.interaction_init_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    senders_w_t,\n",
    "                    senders_th_t,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    receivers_w_t,\n",
    "                    receivers_th_t,\n",
    "                    node_latent,\n",
    "                    latent_history=False,\n",
    "                )\n",
    "            else:\n",
    "                node_dv_int_decoded, node_dw_int_decoded, residue = self.interaction_proc_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    senders_w_t,\n",
    "                    senders_th_t,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    receivers_w_t,\n",
    "                    receivers_th_t,\n",
    "                    node_latent,\n",
    "                    residue=residue,\n",
    "                    latent_history=True,\n",
    "                )\n",
    "\n",
    "            sum_node_dv [mask_reflected_node]= sum_node_dv [mask_reflected_node] + node_dv_int_decoded[mask_reflected_node]\n",
    "                \n",
    "            node_vf[mask_reflected_node]= node_v_t[mask_reflected_node] + node_dv_int_decoded[mask_reflected_node]\n",
    "            node_wf[mask_reflected_node]= node_w_t[mask_reflected_node] + node_dw_int_decoded[mask_reflected_node]\n",
    "\n",
    "            node_disp= (\n",
    "                (node_v_t + node_vf) * 0.5 * self.sub_tstep\n",
    "            )\n",
    "\n",
    "            node_th_t = node_th_t + (node_wf + node_w_t)* 0.5 * self.sub_tstep\n",
    "\n",
    "            sum_node_dx [mask_reflected_node]=sum_node_dx [mask_reflected_node]+ ((node_v_t + node_vf) * 0.5 * self.sub_tstep)[mask_reflected_node]\n",
    "\n",
    "            senders_disp = node_disp[senders]\n",
    "            receivers_disp = node_disp[receivers]\n",
    "\n",
    "            senders_pos = senders_disp + senders_pos\n",
    "            receivers_pos = receivers_disp + receivers_pos\n",
    "\n",
    "\n",
    "            node_v_tm1 = node_v_t.clone()\n",
    "\n",
    "            node_v_t = node_vf.clone()\n",
    "            node_w_t = node_wf.clone()\n",
    "\n",
    "            \n",
    "\n",
    "            senders_v_tm1 = senders_v_t.clone()\n",
    "            senders_v_t = node_v_t[senders].clone()\n",
    "            senders_w_t = node_w_t[senders].clone()\n",
    "            senders_th_t = node_th_t[senders].clone()\n",
    "\n",
    "            receivers_v_tm1 = receivers_v_t.clone()\n",
    "            receivers_v_t = node_v_t[receivers].clone()\n",
    "            receivers_w_t = node_w_t[receivers].clone()\n",
    "            receivers_th_t = node_th_t[receivers].clone()\n",
    "\n",
    "            edge_dx = receivers_pos - senders_pos\n",
    "        return sum_node_dv,sum_node_dx, node_v_tm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02aa8291-95c8-4bfe-8d55-2ebd230d2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsSolver(torch.nn.Module):\n",
    "    def __init__(self, sample_step, train_stats, num_jumps=1, num_msgs=1, latent_size=128):\n",
    "        super(DynamicsSolver, self).__init__()\n",
    "        self.refframecalc = RefFrameCalc()\n",
    "        self.scaler = Scaler()\n",
    "        self.node_encoder = NodeEncoder(latent_size)\n",
    "        self.interaction_proc_layer = Interaction_Block(latent_size)\n",
    "        self.interaction_init_layer = Interaction_Block(latent_size)\n",
    "        self.num_messages = num_msgs\n",
    "        self.sub_tstep = sample_step / num_msgs\n",
    "        self.train_stats = train_stats\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # Initialize graph data for processing\n",
    "        device = graph.pos.device\n",
    "        graph = graph.to(device)\n",
    "        node_type = graph.node_type.float()\n",
    "        pos = graph.pos.float()\n",
    "        vel = graph.vel.float()\n",
    "        prev_vel = graph.prev_vel.float()\n",
    "        \n",
    "        edge_index = graph.edge_index.long()\n",
    "        senders, receivers = edge_index\n",
    "        senders_pos = pos[senders]\n",
    "        receivers_pos = pos[receivers]\n",
    "        edge_dx = receivers_pos - senders_pos\n",
    "        edge_attr = graph.edge_attr.float()\n",
    "        \n",
    "        mask_reflected_node = (graph.node_type!=2).squeeze()\n",
    "\n",
    "        node_v_t = vel\n",
    "\n",
    "        senders_v_t = node_v_t[senders].float()\n",
    "        receivers_v_t = node_v_t[receivers].float()\n",
    "\n",
    "        senders_v_tm1 = prev_vel[senders].float()\n",
    "        receivers_v_tm1 = prev_vel[receivers].float()      \n",
    "\n",
    "        node_disp = torch.zeros_like(node_v_t)\n",
    "        node_vf = torch.zeros_like(node_v_t)\n",
    "\n",
    "        sum_node_dv = torch.zeros_like(node_v_t)\n",
    "        sum_node_dx = torch.zeros_like(node_v_t)\n",
    "        \n",
    "        node_latent = self.node_encoder(torch.hstack((node_type,vel[:,1:2])))\n",
    "\n",
    "        for i in range(self.num_messages):\n",
    "            (\n",
    "                senders_v_t_,\n",
    "                senders_v_tm1_,\n",
    "                receivers_v_t_,\n",
    "                receivers_v_tm1_,\n",
    "                edge_dx_,\n",
    "            ) = self.scaler(\n",
    "                senders_v_t,\n",
    "                senders_v_tm1,\n",
    "                receivers_v_t,\n",
    "                receivers_v_tm1,\n",
    "                edge_dx,\n",
    "                self.train_stats,\n",
    "            )\n",
    "\n",
    "\n",
    "            vector_a, vector_b, vector_c = self.refframecalc(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    senders_v_t_,\n",
    "                    receivers_v_t_\n",
    "                    \n",
    "                )\n",
    "\n",
    "            if i == 0:\n",
    "                node_dv_int_decoded, node_dw_int_decoded,residue= self.interaction_init_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    node_latent,\n",
    "                    latent_history=False,\n",
    "                )\n",
    "            else:\n",
    "                node_dv_int_decoded, node_dw_int_decoded, residue = self.interaction_proc_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    node_latent,\n",
    "                    residue=residue,\n",
    "                    latent_history=True,\n",
    "                )\n",
    "\n",
    "            sum_node_dv [mask_reflected_node]= sum_node_dv [mask_reflected_node] + node_dv_int_decoded[mask_reflected_node]\n",
    "                \n",
    "            node_vf[mask_reflected_node]= node_v_t[mask_reflected_node] + node_dv_int_decoded[mask_reflected_node]\n",
    "\n",
    "            node_disp= (\n",
    "                (node_v_t + node_vf) * 0.5 * self.sub_tstep\n",
    "            )\n",
    "\n",
    "            #node_th_t = node_th_t + (node_wf + node_w_t)* 0.5 * self.sub_tstep\n",
    "\n",
    "            sum_node_dx [mask_reflected_node]=sum_node_dx [mask_reflected_node]+ ((node_v_t + node_vf) * 0.5 * self.sub_tstep)[mask_reflected_node]\n",
    "\n",
    "            senders_disp = node_disp[senders]\n",
    "            receivers_disp = node_disp[receivers]\n",
    "\n",
    "            senders_pos = senders_disp + senders_pos\n",
    "            receivers_pos = receivers_disp + receivers_pos\n",
    "\n",
    "\n",
    "            node_v_tm1 = node_v_t.clone()\n",
    "\n",
    "            node_v_t = node_vf.clone()\n",
    "            \n",
    "\n",
    "            senders_v_tm1 = senders_v_t.clone()\n",
    "            senders_v_t = node_v_t[senders].clone()\n",
    "            #senders_w_t = node_w_t[senders].clone()\n",
    "            #senders_th_t = node_th_t[senders].clone()\n",
    "\n",
    "            receivers_v_tm1 = receivers_v_t.clone()\n",
    "            receivers_v_t = node_v_t[receivers].clone()\n",
    "            #receivers_w_t = node_w_t[receivers].clone()\n",
    "            #receivers_th_t = node_th_t[receivers].clone()\n",
    "\n",
    "            edge_dx = receivers_pos - senders_pos\n",
    "        return sum_node_dv,sum_node_dx, node_v_tm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cd859dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model, device, mode='test', plot=False, frequency=1, model_name='PIGNN', experiment_name='Val'):\n",
    "    with torch.no_grad():\n",
    "        res = 0.\n",
    "        res_counter = 0\n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            for i in range(1):  # Iterates 10 times: 0 -> predicts 31, ..., 9 -> predicts 40\n",
    "                if i == 0:\n",
    "                    test_graph = test_batch.to(device)\n",
    "                    graph_t0 = test_graph\n",
    "                    # Clone to ensure ground truth remains unchanged\n",
    "                    end_pos = test_graph.end_pos.clone()  \n",
    "\n",
    "                node_dv,node_dx,_= model(graph_t0.detach())\n",
    "                new_vel =  graph_t0.vel + node_dv\n",
    "                new_pos =  graph_t0.pos + node_dx\n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = graph_t0.vel.clone()\n",
    "                graph_t0.pos = new_pos.clone()\n",
    "                graph_t0.vel = new_vel.clone()\n",
    "\n",
    "            loss = F.mse_loss(new_pos, end_pos)\n",
    "            batch_size = test_graph.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "\n",
    "        mean_pos_error = res / res_counter\n",
    "    return mean_pos_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc02010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device, train_stats):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.train_history = []\n",
    "        self.extr_test_history = []\n",
    "        self.gen_test_history = []\n",
    "        self.cur_dir = os.getcwd()\n",
    "        self.model_dir = os.path.join(self.cur_dir, 'saved_models')\n",
    "        self.gen_loss = 0.\n",
    "        self.train_stats = train_stats\n",
    "        self.mean_node_dv = self.train_stats[2][0]\n",
    "        self.mean_node_disp = self.train_stats[3][0]\n",
    "        self.std_node_dv = self.train_stats[2][1]\n",
    "        self.std_node_disp = self.train_stats[3][1]\n",
    "        # Initialize best test loss and best epoch.\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = None        \n",
    "\n",
    "    def train(self, train_graph):\n",
    "        self.model.train()\n",
    "        pred_node_dvel,pred_node_disp,_= self.model(train_graph.to(self.device))\n",
    "\n",
    "        actual_node_dvel = (train_graph.y_dv).float().to(self.device)\n",
    "        actual_node_disp = (train_graph.y_dx).float().to(self.device)\n",
    "\n",
    "        pred_node_dvel_ = (pred_node_dvel-self.mean_node_dv.detach())/self.std_node_dv.detach()\n",
    "        pred_node_disp_ = (pred_node_disp-self.mean_node_disp.detach())/self.std_node_disp.detach()\n",
    "        actual_node_dvel_ = (actual_node_dvel - self.mean_node_dv.detach())/self.std_node_dv.detach()\n",
    "        actual_node_disp_ = (actual_node_disp-self.mean_node_disp.detach())/self.std_node_disp.detach()\n",
    "\n",
    "        loss = F.mse_loss(pred_node_disp_,actual_node_disp_) + F.mse_loss(pred_node_dvel_,actual_node_dvel_)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.loss = loss.cpu().detach().numpy()\n",
    "        self.train_history.append(self.loss)\n",
    "    def test(self, test_loader, mode='val', epoch=None):\n",
    "        self.model.eval()\n",
    "        mean_pos_error = evaluate(test_loader,\n",
    "                            self.model,\n",
    "                            self.device)\n",
    "        \n",
    "        \n",
    "        if mode == 'val':\n",
    "            self.val_loss_pos = mean_pos_error\n",
    "            self.gen_test_history.append(self.val_loss_pos)\n",
    "            # Update best test loss and best epoch if the current loss is lower\n",
    "            if epoch is not None and self.val_loss_pos < self.best_val_loss:\n",
    "                self.best_val_loss = self.val_loss_pos\n",
    "                self.best_epoch = epoch\n",
    "                self.save_model(epoch)\n",
    "        if mode == 'test':\n",
    "            self.test_loss_pos = mean_pos_error\n",
    "\n",
    "    def save_model(self,iteration):\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        self.path = os.path.join(self.model_dir, \n",
    "                                 f'GenLoss_{self.val_loss_pos:.5f}mm_iter{iteration}.pth')\n",
    "        torch.save(self.model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3b6dbe9-823b-457f-b6b1-4653ebfe9036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE_TIME_STEP =  30.0\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if model_settings[\"finite_diff\"]:\n",
    "    SAMPLE_TIME_STEP = 30*model_settings[\"time_step\"]\n",
    "else:\n",
    "    SAMPLE_TIME_STEP = 30*model_settings[\"time_step_actual\"]\n",
    "print(f'SAMPLE_TIME_STEP = ',SAMPLE_TIME_STEP)\n",
    "N= 1\n",
    "MODEL = DynamicsSolver(SAMPLE_TIME_STEP, train_stats, num_jumps=N, num_msgs=4, latent_size=64)\n",
    "optimizer = torch.optim.Adam(MODEL.parameters(), lr = 5e-4)\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def ensure_model_on_device(model, device):\n",
    "    for module in model.modules():\n",
    "        # Check if any parameter of the module is not on the desired device\n",
    "        if any(p.device != device for p in module.parameters()):\n",
    "            # Move the entire module to the device\n",
    "            module.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensure_model_on_device(MODEL, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9070b11-5732-452a-a2bc-5cd6b4b95c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(MODEL, \n",
    "                  optimizer, \n",
    "                  device,\n",
    "                  train_stats\n",
    "                 )\n",
    "# Clear PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2bfb79c-b757-4954-94aa-55546d3685af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▍  | 744/1000 [11:10<03:50,  1.11it/s, TrainLoss (*1e3)=2.58958e+02, ValLoss=8.85035549e-01, BestVal=8.68726e-01(Ep670), TestLoss=7.80401216e-01, LR=5.00e-04]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# ——— Validation & Test ———\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# trainer.test now returns the mean positional error\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch%\u001b[32m5\u001b[39m==\u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     val_loss = trainer.val_loss_pos\n\u001b[32m     21\u001b[39m     trainer.test(test_loader, mode=\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mTrainer.test\u001b[39m\u001b[34m(self, test_loader, mode, epoch)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_loader, mode=\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m, epoch=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     42\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.eval()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     mean_pos_error = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     49\u001b[39m         \u001b[38;5;28mself\u001b[39m.val_loss_pos = mean_pos_error\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(test_loader, model, device, mode, plot, frequency, model_name, experiment_name)\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Clone to ensure ground truth remains unchanged\u001b[39;00m\n\u001b[32m     12\u001b[39m     end_pos = test_graph.end_pos.clone()  \n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m node_dv,node_dx,_= \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_t0\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m new_vel =  graph_t0.vel + node_dv\n\u001b[32m     16\u001b[39m new_pos =  graph_t0.pos + node_dx\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nn_control_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nn_control_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mDynamicsSolver.forward\u001b[39m\u001b[34m(self, graph)\u001b[39m\n\u001b[32m     64\u001b[39m vector_a, vector_b, vector_c = \u001b[38;5;28mself\u001b[39m.refframecalc(\n\u001b[32m     65\u001b[39m         edge_index,\n\u001b[32m     66\u001b[39m         senders_pos,\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m     )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     node_dv_int_decoded, node_dw_int_decoded,residue= \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minteraction_init_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43msenders_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreceivers_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43medge_dx_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvector_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvector_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvector_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43msenders_v_t_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43msenders_v_tm1_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreceivers_v_t_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreceivers_v_tm1_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnode_latent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatent_history\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     91\u001b[39m     node_dv_int_decoded, node_dw_int_decoded, residue = \u001b[38;5;28mself\u001b[39m.interaction_proc_layer(\n\u001b[32m     92\u001b[39m         edge_index,\n\u001b[32m     93\u001b[39m         senders_pos,\n\u001b[32m   (...)\u001b[39m\u001b[32m    106\u001b[39m         latent_history=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    107\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nn_control_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nn_control_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mInteraction_Block.forward\u001b[39m\u001b[34m(self, edge_index, senders_pos, receivers_pos, edge_dx_, edge_attr, vector_a, vector_b, vector_c, senders_v_t_, senders_v_tm1_, receivers_v_t_, receivers_v_tm1_, node_latent, residue, latent_history)\u001b[39m\n\u001b[32m     20\u001b[39m     interaction_latent = interaction_latent + residue\n\u001b[32m     21\u001b[39m     interaction_latent = \u001b[38;5;28mself\u001b[39m.layer_norm(interaction_latent)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m edge_interaction_force, edge_interaction_tau= \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minteraction_decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msenders_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreceivers_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteraction_latent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_latent\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m node_dv_int_decoded, node_dw_int_decoded = \u001b[38;5;28mself\u001b[39m.internal_dv_decoder(\n\u001b[32m     27\u001b[39m     edge_index, node_latent, edge_interaction_force, edge_interaction_tau\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m node_dv_int_decoded, node_dw_int_decoded, interaction_latent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nn_control_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nn_control_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# initialize to large values\n",
    "val_loss = np.inf\n",
    "test_loss = np.inf\n",
    "\n",
    "# wrap the epoch loop in tqdm\n",
    "with tqdm(range(1, epochs+1), desc='Training') as pbar:\n",
    "    for epoch in pbar:\n",
    "        # ——— Training ———\n",
    "        for train_batch in train_loader:\n",
    "            trainer.train(train_batch)\n",
    "\n",
    "        # ——— Validation & Test ———\n",
    "        # trainer.test now returns the mean positional error\n",
    "        if epoch%5==0:\n",
    "            trainer.test(val_loader,  mode='val',  epoch=epoch)\n",
    "            val_loss = trainer.val_loss_pos\n",
    "            trainer.test(test_loader, mode='test')\n",
    "            test_loss = trainer.test_loss_pos\n",
    "\n",
    "        # ——— Learning rate ———\n",
    "        current_lr = trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # ——— Update the progress bar ———\n",
    "        pbar.set_postfix({\n",
    "            'TrainLoss (*1e3)': f'{trainer.loss * 1e3:.5e}',\n",
    "            'ValLoss':           f'{val_loss:.8e}',\n",
    "            'BestVal':           f'{trainer.best_val_loss:.5e}(Ep{trainer.best_epoch})',\n",
    "            'TestLoss':          f'{test_loss:.8e}',\n",
    "            'LR':                f'{current_lr:.2e}',\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a029910e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5ae1f111b0>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainer.train_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc1330dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/visharma/gnn/WORKING_DIR/2025_MASTER_STUDENTS/HUMAN_MOTION/NO_REF/SEED1/saved_models'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e0ef6",
   "metadata": {},
   "source": [
    "## Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f200b9b",
   "metadata": {},
   "source": [
    "This next cell has the functions that will, given a frame of the human, use the model to predict eventual position of the human after 30 time steps. It then also gives some sample animations of the predictions alongside the ground truth of the final position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1529bd0",
   "metadata": {},
   "source": [
    "### Main loop that runs the test rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f32a7e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best file: GenLoss_0.86873mm_iter670.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Regular expression pattern to extract loss and epoch\n",
    "pattern = r'GenLoss_(\\d+\\.\\d+)mm_iter(\\d+)\\.pth'\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_epoch = None\n",
    "best_file = None\n",
    "for fn in os.listdir(trainer.model_dir):\n",
    "    m = re.search(pattern, fn)\n",
    "    if m:\n",
    "        l = float(m.group(1))\n",
    "        if l < best_loss:\n",
    "            best_loss = l\n",
    "            best_epoch = int(m.group(2))\n",
    "            best_file = fn\n",
    "\n",
    "if best_file is None:\n",
    "    raise RuntimeError(\"No checkpoint found!\")\n",
    "path = os.path.join(trainer.model_dir, best_file)\n",
    "trainer.model.load_state_dict(torch.load(path))\n",
    "# Print the best result in the desired format.\n",
    "print(f'Best file: {best_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab4f95-c407-4430-88c5-0ff13b1285e6",
   "metadata": {},
   "source": [
    "# EVALUATION RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a73d106-a728-4eb9-bbe6-cd5d604b1cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7837719279977903\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "trainer.test(val_loader, mode='val')\n",
    "print(trainer.val_loss_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2b3f5fe-984f-4a1f-8c4b-c67ff7f2b100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7102607665307371\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "trainer.test(test_loader, mode='val')\n",
    "print(trainer.val_loss_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de2f23f8-8495-40d6-8166-ccd767407a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rollout(test_loader, model, device, nsteps=1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = 0.\n",
    "        res_counter = 0\n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            for i in range(nsteps):  # Iterates 10 times: 0 -> predicts 31, ..., 9 -> predicts 40\n",
    "                if i == 0:\n",
    "                    test_graph = test_batch.to(device)\n",
    "                    graph_t0 = test_graph\n",
    "                    # Clone to ensure ground truth remains unchanged\n",
    "                    end_pos = test_graph.end_pos.clone()  \n",
    "\n",
    "                node_dv,node_dx,vtm1= model(graph_t0.detach())\n",
    "                new_vel =  graph_t0.vel + node_dv\n",
    "                new_pos =  graph_t0.pos + node_dx\n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = vtm1.clone()\n",
    "                graph_t0.pos = new_pos.clone()\n",
    "                graph_t0.vel = new_vel.clone()\n",
    "\n",
    "            loss = F.mse_loss(new_pos, end_pos)\n",
    "            batch_size = test_graph.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "\n",
    "        mean_pos_error = res / res_counter\n",
    "    return mean_pos_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6af236bb-e575-4a07-9666-846ee1f07d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "loss for rollout 1 steps 0.7102607665307371\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "loss for rollout 2 steps 2.974272731887134\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "loss for rollout 3 steps 5.717915099066056\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "loss for rollout 4 steps 8.582023967062513\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for nstep in [1,2,3,4]:\n",
    "    \n",
    "    dataset_eval = HumanDataset(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"],nsteps=nstep)\n",
    "    \n",
    "    dataloader_eval = create_dataloaders_from_raw(dataset_eval,200,shuffle=False)\n",
    "    \n",
    "    eval_error = evaluate_rollout(dataloader_eval, trainer.model, device, nsteps=nstep)\n",
    "    \n",
    "    print(f'loss for rollout {nstep} steps {eval_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e44e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_rollout(test_loader, model, device, nsteps=1):\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        res = 0.0\n",
    "        res_counter = 0\n",
    "        batches = 0\n",
    "        t_wall0 = time.perf_counter()\n",
    "        t_cpu0  = time.process_time()        \n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            batches += 1\n",
    "            for i in range(nsteps):\n",
    "                if i == 0:\n",
    "                    test_graph = test_batch.to(device)\n",
    "                    graph_t0 = test_graph\n",
    "                    end_pos = test_graph.end_pos.clone()  # keep GT unchanged\n",
    "\n",
    "                node_dv, node_dx, vtm1 = model(graph_t0.detach())\n",
    "                new_vel = graph_t0.vel + node_dv\n",
    "                new_pos = graph_t0.pos + node_dx\n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = vtm1.clone()\n",
    "                graph_t0.pos = new_pos.clone()\n",
    "                graph_t0.vel = new_vel.clone()\n",
    "\n",
    "            break\n",
    "\n",
    "            loss = F.mse_loss(new_pos, end_pos)\n",
    "            batch_size = test_graph.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "\n",
    "        mean_pos_error = res / res_counter if res_counter > 0 else float(\"nan\")\n",
    "\n",
    "    # timing + summary\n",
    "    t_wall = time.perf_counter() - t_wall0\n",
    "    t_cpu  = time.process_time() - t_cpu0\n",
    "    total_steps = batches * nsteps\n",
    "\n",
    "    print(\"===== Rollout Evaluation Timing =====\")\n",
    "    print(f\"Device:               {device}\")\n",
    "    print(f\"Batches processed:    {batches}\")\n",
    "    print(f\"nsteps per batch:     {nsteps}\")\n",
    "    print(f\"Total rollout steps:  {total_steps}\")\n",
    "    print(f\"Total wall time:      {t_wall:.3f} s\")\n",
    "    print(f\"Total CPU time:       {t_cpu:.3f} s  (CPU/Wall = {t_cpu/t_wall:.3f})\")\n",
    "    if total_steps > 0 and t_wall > 0:\n",
    "        print(f\"Latency/step:         {t_wall/total_steps*1000:.3f} ms\")\n",
    "        print(f\"Throughput:           {total_steps/t_wall:.2f} steps/s\")\n",
    "    print(f\"Mean Position Error:  {mean_pos_error:.6e}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    return mean_pos_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533dbfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for nstep in [1]:\n",
    "    \n",
    "    dataset_eval = HumanDataset(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"],nsteps=nstep)\n",
    "    \n",
    "    dataloader_eval = create_dataloaders_from_raw(dataset_eval,1,shuffle=False)\n",
    "    \n",
    "    eval_error = evaluate_rollout(dataloader_eval, trainer.model, device, nsteps=nstep)\n",
    "    \n",
    "    print(f'loss for rollout {nstep} steps {eval_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee52fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c725db9-f613-4100-ae66-0707e5b44a46",
   "metadata": {},
   "source": [
    "# VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47a5eb-065c-4e2b-9476-09a227f6ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDatasetSeq(torch.utils.data.Dataset):\n",
    "    def __init__(self, partition='train', max_samples=600, data_dir='', nsteps=1):\n",
    "        self.partition = partition\n",
    "        self.data_dir = data_dir\n",
    "        self.nsteps = nsteps\n",
    "\n",
    "        # --- load raw data --------------------------------------\n",
    "        with open(os.path.join(data_dir, 'motion.pkl'), 'rb') as f:\n",
    "            edges, X = pkl.load(f)\n",
    "\n",
    "        # your smoothing / central_diff code here...\n",
    "        Ps, Vs, As = self.central_diff(X)\n",
    "\n",
    "        # trial IDs must match exactly\n",
    "        train_case_id = [20,1,17,13,14,9,4,2,7,5,16]\n",
    "        val_case_id   = [3,8,11,12,15,18]\n",
    "        test_case_id  = [6,19,21,0,22,10]\n",
    "\n",
    "        # --- load or create competitor splits (fixed for central_diff) ----------\n",
    "        split_path = os.path.join(data_dir, f'split_n{self.nsteps}.pkl')\n",
    "        try:\n",
    "            with open(split_path, 'rb') as f:\n",
    "                train_mapping, val_mapping, test_mapping = pkl.load(f)\n",
    "                print(\"Loaded competitor split!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Generating competitor split…\")\n",
    "\n",
    "            def make_map(case_ids):\n",
    "                mapping = {}\n",
    "                for i in case_ids:\n",
    "                    core_len = Ps[i].shape[0]                    # <<— use length after central_diff\n",
    "                    safe_max = core_len - self.nsteps*30 - 1\n",
    "                    if safe_max < 0:\n",
    "                        raise ValueError(f\"Trial {i} too short for look-ahead of {self.nsteps} steps.\")\n",
    "                    # competitor caps at 300\n",
    "                    itv = min(300, safe_max + 1)                # +1 because j in [0..safe_max]\n",
    "                    pool = np.arange(itv)                       # j ∈ [0..itv-1]\n",
    "                    mapping[i] = np.random.choice(pool, size=100, replace=False)\n",
    "                return mapping\n",
    "\n",
    "            train_mapping = make_map(train_case_id)\n",
    "            val_mapping   = make_map(val_case_id)\n",
    "            test_mapping  = make_map(test_case_id)\n",
    "\n",
    "            with open(split_path, 'wb') as f:\n",
    "                pkl.dump((train_mapping, val_mapping, test_mapping), f)\n",
    "            print(\"Saved competitor split!\")\n",
    "\n",
    "        # pick the mapping you need\n",
    "        if   partition == 'train': mapping = train_mapping\n",
    "        elif partition == 'val'  : mapping = val_mapping\n",
    "        elif partition == 'test' : mapping = test_mapping\n",
    "        else: raise ValueError(f\"Unknown partition {partition!r}\")\n",
    "\n",
    "        # now proceed exactly as before, using `mapping` instead of your make_mapping\n",
    "        each_len = max_samples // len(mapping)\n",
    "        in_graphs = []\n",
    "        for i, pool in mapping.items():\n",
    "            for j in pool[:each_len]:\n",
    "                # note: they use delta_frame; you have nsteps*30, so this is identical\n",
    "                cur_x_t   = Ps[i][j]\n",
    "                cur_v_t   = Vs[i][j]\n",
    "                cur_v_tm1 = Vs[i][j-1]\n",
    "                y_dv      = Vs[i][j + self.nsteps*30] - Vs[i][j]\n",
    "                y_dx      = Ps[i][j + self.nsteps*30] - Ps[i][j]\n",
    "                gt_seq = [ Ps[i][j + k*30] for k in range(self.nsteps+1) ]   # list of (31,3) arrays\n",
    "                y_pos_end = Ps[i][j + self.nsteps*30]\n",
    "                y_vel_end = Vs[i][j + self.nsteps*30]\n",
    "\n",
    "                in_graphs.append(self.create_in_graph(\n",
    "                    edges,\n",
    "                    x=(cur_x_t, cur_v_t, cur_v_tm1),\n",
    "                    y=(y_dv, y_dx, y_pos_end, y_vel_end),\n",
    "                    gt_seq = gt_seq\n",
    "                ))\n",
    "\n",
    "        self.in_graphs = in_graphs\n",
    "        print(f\"[HumanDataset:{partition}] built {len(in_graphs)} samples\")\n",
    "\n",
    "    def central_diff(self, Xs, dt: float = 1.0, window_length: int = 41):\n",
    "        Ps, Vs, As = [], [], []\n",
    "        for x in Xs:\n",
    "            v      = (x[2:] - x[:-2]) / (2*dt)\n",
    "            a      = (x[2:] - 2*x[1:-1] + x[:-2]) / (dt**2)\n",
    "            p      = x[1:-1]                      # align to v,a\n",
    "            Ps.append(p)\n",
    "            Vs.append(v)\n",
    "            As.append(a)\n",
    "        return Ps, Vs, As\n",
    "\n",
    "        \n",
    "    def get_foot_nodes(self, nodes):\n",
    "        foot_indices = np.argsort(nodes[:,1])[:6]\n",
    "        foot_pos = nodes[foot_indices]\n",
    "        return foot_pos, foot_indices\n",
    "    \n",
    "    def reflected_nodes(self, nodes, z0=0, epsilon=1e-3):\n",
    "        reflected = nodes.copy()\n",
    "        reflected[:,1] = 2*z0 - nodes[:,1] - epsilon\n",
    "        distances = reflected[:,1] - nodes[:,1]\n",
    "        return reflected, distances\n",
    "    \n",
    "    def find_min(self, nodes):\n",
    "        return np.min(nodes, axis=0)\n",
    "    \n",
    "\n",
    "    def create_edges(self, N, edges):\n",
    "        atom_edges = torch.zeros(N, N).int()\n",
    "        for edge in edges:\n",
    "            atom_edges[edge[0], edge[1]] = 1\n",
    "            atom_edges[edge[1], edge[0]] = 1\n",
    "\n",
    "        atom_edges2 = atom_edges @ atom_edges\n",
    "        self.atom_edge = atom_edges\n",
    "        self.atom_edge2 = atom_edges2\n",
    "        edge_attr = []\n",
    "        # Initialize edges and edge_attributes\n",
    "        rows, cols = [], []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    if atom_edges[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([1])\n",
    "                        assert not atom_edges2[i][j]\n",
    "                    if atom_edges2[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([2])\n",
    "                        assert not atom_edges[i][j]\n",
    "\n",
    "        edges = [rows, cols] \n",
    "        edge_attr = torch.Tensor(np.array(edge_attr))  # [edge, 3]\n",
    "        edge_idx =torch.tensor(edges, dtype=torch.long)  # [2, M]   \n",
    "        return edge_idx,edge_attr     \n",
    "    \n",
    "    \n",
    "    def create_in_graph(self, edges,x,y,gt_seq):\n",
    "        pos_t, vel_t, vel_tm1 = x\n",
    "        y_dv,y_dx,y_pos_end,y_vel_end = y\n",
    "\n",
    "        edge_idx,edge_attr = self.create_edges(pos_t.shape[0], edges)\n",
    "\n",
    "        # # Get the ground node\n",
    "        # z0_t = self.find_min(pos_t)[1]\n",
    "        # z0_end = self.find_min(y_end)[1]\n",
    "        # # Center the y-positions around z0 for input and target\n",
    "        # pos_t -= np.array([0, z0_t, 0]) \n",
    "        # y_end -= np.array([0, z0_end, 0])\n",
    "\n",
    "        # Get the foot node positions and indices\n",
    "        # foot_nodes_positions, foot_nodes_indices = self.get_foot_nodes(pos_t)\n",
    "        # foot_nodes_reflected, foot_distances = self.reflected_nodes(foot_nodes_positions,z0=0.0)\n",
    "        \n",
    "        # current_largest_node_index = pos_t.shape[0]\n",
    "        # reflected_nodes_indices = []\n",
    "        # for reflected_node in range(foot_nodes_indices.shape[0]):\n",
    "        #     reflected_node_index = current_largest_node_index\n",
    "        #     current_largest_node_index += 1\n",
    "        #     reflected_nodes_indices.append(reflected_node_index)\n",
    "        \n",
    "        \n",
    "        # # Set lists to torch tensors\n",
    "        # reflected_nodes_indices = torch.tensor(reflected_nodes_indices)\n",
    "        # foot_nodes_indices = torch.tensor(foot_nodes_indices)\n",
    "        pos_t = torch.tensor(pos_t)\n",
    "        vel_t = torch.tensor(vel_t)\n",
    "        vel_tm1 = torch.tensor(vel_tm1)\n",
    "\n",
    "        y_dv = torch.tensor(y_dv)\n",
    "        y_dx = torch.tensor(y_dx)\n",
    "        y_pos_end = torch.tensor(y_pos_end)\n",
    "        y_vel_end = torch.tensor(y_vel_end)\n",
    "        \n",
    "        \n",
    "        # foot_nodes_reflected = torch.tensor(foot_nodes_reflected)\n",
    "        \n",
    "        # Set the node type of feet to one\n",
    "        node_type = torch.ones(pos_t.shape[0],1)\n",
    "        # node_type[foot_nodes_indices] = 1\n",
    "        # # Make reflected nodes of type 2\n",
    "        # new_node_type = torch.vstack((node_type,2*torch.ones_like(reflected_nodes_indices).unsqueeze(1))) \n",
    "        \n",
    "        # New bi-dir edge indexes\n",
    "        # new_edges_ref = torch.hstack((foot_nodes_indices.unsqueeze(1), reflected_nodes_indices.unsqueeze(1))) # connect foot edges to their reflections\n",
    "        # new_edges_ref = new_edges_ref.t()  # now [2, M]\n",
    "        # rev_new_edges_ref = new_edges_ref.flip(0)  # reverse the order to match edge index format\n",
    "        # new_edges_bidir_ref = torch.cat((new_edges_ref, rev_new_edges_ref), dim=1)  # add reverse edges\n",
    "        # new_edge_index = torch.cat([edge_idx, new_edges_bidir_ref], dim=1) # add new edges to the graph edge index\n",
    "        # s,r = new_edge_index\n",
    "\n",
    "        # we add the 1 as edge attr for these edges as they are 1 hop\n",
    "        # new_edge_attr = torch.vstack((edge_attr, torch.ones((new_edges_bidir_ref.shape[1], 1))))  # add new edge attributes\n",
    "        # for differentiating reflected edges we use another features i.e. type_sender*type_receiver\n",
    "        # new_edge_attr = torch.hstack((new_edge_attr,\n",
    "        #                               new_node_type[s]*new_node_type[r]))\n",
    "        # new_pos_t = torch.vstack((pos_t, foot_nodes_reflected))\n",
    "        # new_vel_t = torch.vstack((vel_t,torch.zeros_like(foot_nodes_reflected)))\n",
    "        # new_vel_tm1 = torch.vstack((vel_tm1,torch.zeros_like(foot_nodes_reflected)))\n",
    "\n",
    "        \n",
    "        # in_graph = Data(x=new_pos_t,edge_index=new_edge_index,edge_attr=new_edge_attr)\n",
    "        # in_graph.node_vel_t = new_vel_t\n",
    "        # in_graph.node_vel_tm1 = new_vel_tm1\n",
    "        # in_graph.y_dv = y_dv\n",
    "        # in_graph.y_dx = y_dx\n",
    "        # in_graph.y_end = y_end\n",
    "        # in_graph.node_type = new_node_type\n",
    "\n",
    "        in_graph = Data(edge_index=edge_idx, edge_attr=edge_attr)\n",
    "        in_graph.pos = pos_t\n",
    "        in_graph.vel = vel_t\n",
    "        in_graph.prev_vel = vel_tm1\n",
    "        in_graph.y_dv = y_dv\n",
    "        in_graph.y_dx = y_dx\n",
    "        in_graph.end_pos = y_pos_end\n",
    "        in_graph.end_vel = y_vel_end\n",
    "        in_graph.node_type = node_type\n",
    "        in_graph.gt_seq = gt_seq\n",
    "        \n",
    "        return in_graph     \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.in_graphs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.in_graphs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aaccb3-2c30-44b3-92d3-6cfbd52c2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def visualize_multi_step(\n",
    "    test_loader,\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    steps=(1,2,3,4),\n",
    "    num_graphs=10,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    For each of `num_graphs` random graphs from test_loader:\n",
    "      • Plot initial_vs_gt.png once (vs GT at step=1)\n",
    "      • Then do a single graph rollout, saving pred_vs_gt_step{n}.png\n",
    "        for each n in `steps`, comparing to GT at that same step.\n",
    "    Uses absolute coordinates (no centering).\n",
    "    Assumes each Data has `gt_seq` as a list of length T+1,\n",
    "    each element shape (n_nodes,3).\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    model.eval()\n",
    "\n",
    "    # flatten loader\n",
    "    all_graphs = []\n",
    "    for batch in test_loader:\n",
    "        all_graphs.extend(batch.to_data_list())\n",
    "    if not all_graphs:\n",
    "        raise RuntimeError(\"No graphs in loader\")\n",
    "\n",
    "    # sample indices\n",
    "    chosen = random.sample(range(len(all_graphs)), min(num_graphs, len(all_graphs)))\n",
    "\n",
    "    # skeleton edges for the 31-body joints\n",
    "    skeleton31 = [\n",
    "      [1,0],[2,1],[3,2],[4,3],[5,4],\n",
    "      [6,0],[7,6],[8,7],[9,8],[10,9],\n",
    "      [11,0],[12,11],[13,12],[14,13],[15,14],\n",
    "      [16,15],[17,13],[18,17],[19,18],[20,19],\n",
    "      [21,20],[22,21],[23,20],[24,13],[25,24],\n",
    "      [26,25],[27,26],[28,27],[29,28],[30,27]\n",
    "    ]\n",
    "\n",
    "    for idx in chosen:\n",
    "        data = all_graphs[idx].to(device)\n",
    "        base = f\"./RESULTS/TrajectoryPlotsRefNodes/graph_{idx}\"\n",
    "        os.makedirs(base, exist_ok=True)\n",
    "\n",
    "        # — stack gt_seq list → np array [T+1,31,3] —\n",
    "        seq_list = data.gt_seq\n",
    "        seq_np = []\n",
    "        for arr in seq_list:\n",
    "            if torch.is_tensor(arr):\n",
    "                seq_np.append(arr.cpu().numpy())\n",
    "            else:\n",
    "                seq_np.append(np.array(arr))\n",
    "        gt_seq = np.stack(seq_np, axis=0)[:, :31, :]  # shape [T+1,31,3]\n",
    "\n",
    "        # initial pose (absolute)\n",
    "        init31 = data.pos[:31].cpu().numpy()\n",
    "\n",
    "        # compute axis limits from initial + all selected GT steps\n",
    "        all_pts = np.vstack([init31] + [gt_seq[k] for k in steps])\n",
    "        pad = 2.0\n",
    "        x_min, x_max = all_pts[:,2].min() - pad, all_pts[:,2].max() + pad\n",
    "        y_min, y_max = all_pts[:,0].min() - pad, all_pts[:,0].max() + pad\n",
    "        z_min, z_max = all_pts[:,1].min() - pad, all_pts[:,1].max() + pad\n",
    "\n",
    "        # — Plot initial_vs_gt.png (vs GT at step=1) —\n",
    "        gt1 = gt_seq[0]\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax  = fig.add_subplot(111, projection='3d')\n",
    "        xx, yy = np.meshgrid([x_min,x_max], [y_min,y_max])\n",
    "        ax.plot_surface(xx, yy, np.zeros_like(xx), color='gray', alpha=0.2, linewidth=0)\n",
    "        ax.scatter(init31[:,2], init31[:,0], init31[:,1],\n",
    "                   c='red', s=30, edgecolors='k', alpha=0.5, label='Initial')\n",
    "        for a,b in skeleton31:\n",
    "            ax.plot([gt1[a,2], gt1[b,2]],\n",
    "                    [gt1[a,0], gt1[b,0]],\n",
    "                    [gt1[a,1], gt1[b,1]],\n",
    "                    c='red', alpha=0.6, linestyle='-', linewidth=2)\n",
    "\n",
    "        mid_x = (x_min + x_max)/2\n",
    "        mid_y = (y_min + y_max)/2\n",
    "        mid_z = (z_min + z_max)/2\n",
    "        \n",
    "        ax.set_xlim(mid_x-20, mid_x+20)\n",
    "        ax.set_ylim(mid_y-20, mid_y+20)\n",
    "        ax.set_zlim(mid_z-20, mid_z+20)\n",
    "        ax.set_box_aspect((1,1,1))\n",
    "        ax.set_xlabel(\"X\",fontsize = 16); ax.set_ylabel(\"Y\",fontsize = 16); ax.set_zlabel(\"Z\",fontsize = 16)\n",
    "        ax.set_title(\"Initial\",fontsize = 18)\n",
    "        ax.legend(loc='upper left',fontsize = 18)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base, 'initial_vs_gt.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # — rollout & per-step plotting —\n",
    "        graph = data.clone().to(device)\n",
    "        for step in steps:\n",
    "            dv, dx,prev_vel = model(graph.detach())\n",
    "            graph.prev_vel = prev_vel\n",
    "            graph.vel      = graph.vel + dv\n",
    "            graph.pos      = graph.pos + dx\n",
    "\n",
    "            pred31 = graph.pos[:31].detach().cpu().numpy()\n",
    "            gt_k   = gt_seq[step]\n",
    "\n",
    "            fig = plt.figure(figsize=(6,6))\n",
    "            ax  = fig.add_subplot(111, projection='3d')\n",
    "            # optional ground plane: uncomment if desired\n",
    "            ax.plot_surface(xx, yy, np.zeros_like(xx), color='gray', alpha=0.2, linewidth=0)\n",
    "\n",
    "            ax.scatter(pred31[:,2], pred31[:,0], pred31[:,1],\n",
    "                       c='blue', s=30, edgecolors='k', alpha=0.5,\n",
    "                       label=f'Pred (step={step})')\n",
    "            ax.scatter(gt_k[:,2], gt_k[:,0], gt_k[:,1],\n",
    "                       c='red',  s=30, edgecolors='k', alpha=0.5,\n",
    "                       label=f'GT (step={step})')\n",
    "            for a,b in skeleton31:\n",
    "                ax.plot([pred31[a,2], pred31[b,2]],\n",
    "                        [pred31[a,0], pred31[b,0]],\n",
    "                        [pred31[a,1], pred31[b,1]],\n",
    "                        c='blue', alpha=0.6, linewidth=2)\n",
    "                ax.plot([gt_k[a,2], gt_k[b,2]],\n",
    "                        [gt_k[a,0], gt_k[b,0]],\n",
    "                        [gt_k[a,1], gt_k[b,1]],\n",
    "                        c='red', alpha=0.6, linestyle='-',linewidth=2)\n",
    "\n",
    "            ax.set_xlim(mid_x-20, mid_x+20)\n",
    "            ax.set_ylim(mid_y-20, mid_y+20)\n",
    "            ax.set_zlim(mid_z-20, mid_z+20)\n",
    "            ax.set_box_aspect((1,1,1))\n",
    "            ax.set_xlabel(\"X\",fontsize = 16); ax.set_ylabel(\"Y\",fontsize = 16); ax.set_zlabel(\"Z\",fontsize = 16)\n",
    "            ax.set_title(f\"Prediction vs GT — {step} steps\",fontsize = 18)\n",
    "            ax.legend(loc='upper left',fontsize = 18)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(base, f'pred_vs_gt_step{step}.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "        # optional: compute final MSE in absolute frame\n",
    "        mask_cuda = (data.node_type[:31] != 2).squeeze()\n",
    "        mask_cpu  = mask_cuda.cpu().numpy()\n",
    "        final_pred = graph.pos[:31][mask_cuda]\n",
    "        final_gt_np = gt_seq[steps[-1]][mask_cpu]\n",
    "        final_gt    = torch.from_numpy(final_gt_np).to(device)\n",
    "        mse = F.mse_loss(final_pred, final_gt).item()\n",
    "        print(f\"Graph {idx}, final step={steps[-1]}, MSE={mse:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81124734-e9b8-48a9-8cc9-9d9adf182d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_eval = HumanDatasetSeq(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"],nsteps=4)\n",
    "\n",
    "loader = create_dataloaders_from_raw(dataset_eval,200,shuffle=False)\n",
    "\n",
    "visualize_multi_step(\n",
    "    loader,\n",
    "    trainer.model,\n",
    "    device,\n",
    "    steps=[1,2,3,4],\n",
    "    num_graphs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933f41f-06b5-4d90-8c4e-dee0613c00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "\n",
    "# Base directory containing graph folders\n",
    "base_dir = './RESULTS/TrajectoryPlotsRefNodes'\n",
    "\n",
    "# Iterate through each subfolder (graph_*)\n",
    "for graph_folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, graph_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    # Collect all PNG files in sorted order\n",
    "    png_files = sorted(glob.glob(os.path.join(folder_path, '*.png')))\n",
    "    if not png_files:\n",
    "        continue\n",
    "    \n",
    "    # Read each image\n",
    "    images = []\n",
    "    for png in png_files:\n",
    "        try:\n",
    "            img = imageio.imread(png)\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not read {png}: {e}\")\n",
    "    \n",
    "    # Save as infinite-loop GIF\n",
    "    gif_path = os.path.join(folder_path, 'rollout.gif')\n",
    "    imageio.mimsave(gif_path, images, duration=0.7, loop=0)\n",
    "    print(f\"Created {gif_path} with {len(images)} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18200e-7984-4132-b233-ee9c893d2615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e08202-4ea4-4a11-83da-6338bf9a3611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_control_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
