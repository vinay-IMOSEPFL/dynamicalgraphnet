{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c9b56",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for the running of DYNAMICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7edd8365-2fe3-4354-b373-38befe37df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Confirm settings\n",
    "# -----------------------------\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75740aa6",
   "metadata": {},
   "source": [
    "All of the code that is present in this notebook are present in the other python files of this Dynamical folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ee16f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c0afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import pickle as pkl\n",
    "import random\n",
    "import shutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4be8b",
   "metadata": {},
   "source": [
    "### Dictionary that sets the model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b91ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings ={\n",
    "        \"batch_size\": 100,\n",
    "        \"epochs\": 600,\n",
    "        \"lr\": 5e-4,\n",
    "        \"nf\": 64,\n",
    "        \"model\": \"gmn\",\n",
    "        \"attention\": 0,\n",
    "        \"n_layers\": 4,\n",
    "        \"max_testing_samples\": 600,\n",
    "        \"max_training_samples\": 200,\n",
    "        \"data_dir\": \"/Users/visharma/Desktop/Repositories/Dynamical_codebase/HUMAN_WALK/data\",\n",
    "        \"norm_diff\": False,\n",
    "        \"weight_decay\": 1e-10,\n",
    "        \"tanh\": False,\n",
    "        \"learnable\": False,\n",
    "        \"finite_diff\":True,\n",
    "        \"time_step\":1.0,\n",
    "        \"end_time_step\": 30.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c81ab9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76d86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, partition='train', max_samples=600, data_dir='', nsteps=1):\n",
    "        self.partition = partition\n",
    "        self.data_dir = data_dir\n",
    "        self.nsteps = nsteps\n",
    "\n",
    "        # --- load raw data --------------------------------------\n",
    "        with open(os.path.join(data_dir, 'motion.pkl'), 'rb') as f:\n",
    "            edges, X = pkl.load(f)\n",
    "\n",
    "        # your smoothing / central_diff code here...\n",
    "        Ps, Vs, As = self.central_diff(X)\n",
    "\n",
    "        # trial IDs must match exactly\n",
    "        train_case_id = [20,1,17,13,14,9,4,2,7,5,16]\n",
    "        val_case_id   = [3,8,11,12,15,18]\n",
    "        test_case_id  = [6,19,21,0,22,10]\n",
    "\n",
    "        # --- load or create competitor splits (fixed for central_diff) ----------\n",
    "        split_path = os.path.join(data_dir, f'split_n{self.nsteps}.pkl')\n",
    "        try:\n",
    "            with open(split_path, 'rb') as f:\n",
    "                train_mapping, val_mapping, test_mapping = pkl.load(f)\n",
    "                print(\"Loaded competitor split!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Generating competitor split…\")\n",
    "\n",
    "            def make_map(case_ids):\n",
    "                mapping = {}\n",
    "                for i in case_ids:\n",
    "                    core_len = Ps[i].shape[0]                    # <<— use length after central_diff\n",
    "                    safe_max = core_len - self.nsteps*30 - 1\n",
    "                    if safe_max < 0:\n",
    "                        raise ValueError(f\"Trial {i} too short for look-ahead of {self.nsteps} steps.\")\n",
    "                    # competitor caps at 300\n",
    "                    itv = min(300, safe_max + 1)                # +1 because j in [0..safe_max]\n",
    "                    pool = np.arange(itv)                       # j ∈ [0..itv-1]\n",
    "                    mapping[i] = np.random.choice(pool, size=100, replace=False)\n",
    "                return mapping\n",
    "\n",
    "            train_mapping = make_map(train_case_id)\n",
    "            val_mapping   = make_map(val_case_id)\n",
    "            test_mapping  = make_map(test_case_id)\n",
    "\n",
    "            with open(split_path, 'wb') as f:\n",
    "                pkl.dump((train_mapping, val_mapping, test_mapping), f)\n",
    "            print(\"Saved competitor split!\")\n",
    "\n",
    "        # pick the mapping you need\n",
    "        if   partition == 'train': mapping = train_mapping\n",
    "        elif partition == 'val'  : mapping = val_mapping\n",
    "        elif partition == 'test' : mapping = test_mapping\n",
    "        else: raise ValueError(f\"Unknown partition {partition!r}\")\n",
    "\n",
    "        # now proceed exactly as before, using `mapping` instead of your make_mapping\n",
    "        each_len = max_samples // len(mapping)\n",
    "        in_graphs = []\n",
    "        for i, pool in mapping.items():\n",
    "            for j in pool[:each_len]:\n",
    "                # note: they use delta_frame; you have nsteps*30, so this is identical\n",
    "                cur_x_t   = Ps[i][j]\n",
    "                cur_v_t   = Vs[i][j]\n",
    "                cur_v_tm1 = Vs[i][j-1]\n",
    "                y_dv      = Vs[i][j + self.nsteps*30] - Vs[i][j]\n",
    "                y_dx      = Ps[i][j + self.nsteps*30] - Ps[i][j]\n",
    "                y_pos_end = Ps[i][j + self.nsteps*30]\n",
    "                y_vel_end = Vs[i][j + self.nsteps*30]\n",
    "\n",
    "                in_graphs.append(self.create_in_graph(\n",
    "                    edges,\n",
    "                    x=(cur_x_t, cur_v_t, cur_v_tm1),\n",
    "                    y=(y_dv, y_dx, y_pos_end, y_vel_end)\n",
    "                ))\n",
    "\n",
    "        self.in_graphs = in_graphs\n",
    "        print(f\"[HumanDataset:{partition}] built {len(in_graphs)} samples\")\n",
    "\n",
    "    def central_diff(self, Xs, dt: float = 1.0, window_length: int = 41):\n",
    "        Ps, Vs, As = [], [], []\n",
    "        for x in Xs:\n",
    "            v      = (x[2:] - x[:-2]) / (2*dt)\n",
    "            a      = (x[2:] - 2*x[1:-1] + x[:-2]) / (dt**2)\n",
    "            p      = x[1:-1]                      # align to v,a\n",
    "            Ps.append(p)\n",
    "            Vs.append(v)\n",
    "            As.append(a)\n",
    "        return Ps, Vs, As\n",
    "\n",
    "        \n",
    "    def get_foot_nodes(self, nodes):\n",
    "        foot_indices = np.argsort(nodes[:,1])[:6]\n",
    "        foot_pos = nodes[foot_indices]\n",
    "        return foot_pos, foot_indices\n",
    "    \n",
    "    def reflected_nodes(self, nodes, z0=0, epsilon=1e-3):\n",
    "        reflected = nodes.copy()\n",
    "        reflected[:,1] = 2*z0 - nodes[:,1] - epsilon\n",
    "        distances = reflected[:,1] - nodes[:,1]\n",
    "        return reflected, distances\n",
    "    \n",
    "    def find_min(self, nodes):\n",
    "        return np.min(nodes, axis=0)\n",
    "    \n",
    "\n",
    "    def create_edges(self, N, edges):\n",
    "        atom_edges = torch.zeros(N, N).int()\n",
    "        for edge in edges:\n",
    "            atom_edges[edge[0], edge[1]] = 1\n",
    "            atom_edges[edge[1], edge[0]] = 1\n",
    "\n",
    "        atom_edges2 = atom_edges @ atom_edges\n",
    "        self.atom_edge = atom_edges\n",
    "        self.atom_edge2 = atom_edges2\n",
    "        edge_attr = []\n",
    "        # Initialize edges and edge_attributes\n",
    "        rows, cols = [], []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    if atom_edges[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([1])\n",
    "                        assert not atom_edges2[i][j]\n",
    "                    if atom_edges2[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([2])\n",
    "                        assert not atom_edges[i][j]\n",
    "\n",
    "        edges = [rows, cols] \n",
    "        edge_attr = torch.Tensor(np.array(edge_attr))  # [edge, 3]\n",
    "        edge_idx =torch.tensor(edges, dtype=torch.long)  # [2, M]   \n",
    "        return edge_idx,edge_attr     \n",
    "    \n",
    "    \n",
    "    def create_in_graph(self, edges,x,y):\n",
    "        pos_t, vel_t, vel_tm1 = x\n",
    "        y_dv,y_dx,y_pos_end,y_vel_end = y\n",
    "\n",
    "        edge_idx,edge_attr = self.create_edges(pos_t.shape[0], edges)\n",
    "\n",
    "        # # Get the ground node\n",
    "        # z0_t = self.find_min(pos_t)[1]\n",
    "        # z0_end = self.find_min(y_end)[1]\n",
    "        # # Center the y-positions around z0 for input and target\n",
    "        # pos_t -= np.array([0, z0_t, 0]) \n",
    "        # y_end -= np.array([0, z0_end, 0])\n",
    "\n",
    "        # Get the foot node positions and indices\n",
    "        # foot_nodes_positions, foot_nodes_indices = self.get_foot_nodes(pos_t)\n",
    "        # foot_nodes_reflected, foot_distances = self.reflected_nodes(foot_nodes_positions,z0=0.0)\n",
    "        \n",
    "        # current_largest_node_index = pos_t.shape[0]\n",
    "        # reflected_nodes_indices = []\n",
    "        # for reflected_node in range(foot_nodes_indices.shape[0]):\n",
    "        #     reflected_node_index = current_largest_node_index\n",
    "        #     current_largest_node_index += 1\n",
    "        #     reflected_nodes_indices.append(reflected_node_index)\n",
    "        \n",
    "        \n",
    "        # # Set lists to torch tensors\n",
    "        # reflected_nodes_indices = torch.tensor(reflected_nodes_indices)\n",
    "        # foot_nodes_indices = torch.tensor(foot_nodes_indices)\n",
    "        pos_t = torch.tensor(pos_t)\n",
    "        vel_t = torch.tensor(vel_t)\n",
    "        vel_tm1 = torch.tensor(vel_tm1)\n",
    "\n",
    "        y_dv = torch.tensor(y_dv)\n",
    "        y_dx = torch.tensor(y_dx)\n",
    "        y_pos_end = torch.tensor(y_pos_end)\n",
    "        y_vel_end = torch.tensor(y_vel_end)\n",
    "        \n",
    "        \n",
    "        # foot_nodes_reflected = torch.tensor(foot_nodes_reflected)\n",
    "        \n",
    "        # Set the node type of feet to one\n",
    "        node_type = torch.ones(pos_t.shape[0],1)\n",
    "        # node_type[foot_nodes_indices] = 1\n",
    "        # # Make reflected nodes of type 2\n",
    "        # new_node_type = torch.vstack((node_type,2*torch.ones_like(reflected_nodes_indices).unsqueeze(1))) \n",
    "        \n",
    "        # New bi-dir edge indexes\n",
    "        # new_edges_ref = torch.hstack((foot_nodes_indices.unsqueeze(1), reflected_nodes_indices.unsqueeze(1))) # connect foot edges to their reflections\n",
    "        # new_edges_ref = new_edges_ref.t()  # now [2, M]\n",
    "        # rev_new_edges_ref = new_edges_ref.flip(0)  # reverse the order to match edge index format\n",
    "        # new_edges_bidir_ref = torch.cat((new_edges_ref, rev_new_edges_ref), dim=1)  # add reverse edges\n",
    "        # new_edge_index = torch.cat([edge_idx, new_edges_bidir_ref], dim=1) # add new edges to the graph edge index\n",
    "        # s,r = new_edge_index\n",
    "\n",
    "        # we add the 1 as edge attr for these edges as they are 1 hop\n",
    "        # new_edge_attr = torch.vstack((edge_attr, torch.ones((new_edges_bidir_ref.shape[1], 1))))  # add new edge attributes\n",
    "        # for differentiating reflected edges we use another features i.e. type_sender*type_receiver\n",
    "        # new_edge_attr = torch.hstack((new_edge_attr,\n",
    "        #                               new_node_type[s]*new_node_type[r]))\n",
    "        # new_pos_t = torch.vstack((pos_t, foot_nodes_reflected))\n",
    "        # new_vel_t = torch.vstack((vel_t,torch.zeros_like(foot_nodes_reflected)))\n",
    "        # new_vel_tm1 = torch.vstack((vel_tm1,torch.zeros_like(foot_nodes_reflected)))\n",
    "\n",
    "        \n",
    "        # in_graph = Data(x=new_pos_t,edge_index=new_edge_index,edge_attr=new_edge_attr)\n",
    "        # in_graph.node_vel_t = new_vel_t\n",
    "        # in_graph.node_vel_tm1 = new_vel_tm1\n",
    "        # in_graph.y_dv = y_dv\n",
    "        # in_graph.y_dx = y_dx\n",
    "        # in_graph.y_end = y_end\n",
    "        # in_graph.node_type = new_node_type\n",
    "\n",
    "        in_graph = Data(edge_index=edge_idx, edge_attr=edge_attr)\n",
    "        in_graph.pos = pos_t\n",
    "        in_graph.vel = vel_t\n",
    "        in_graph.prev_vel = vel_tm1\n",
    "        in_graph.y_dv = y_dv\n",
    "        in_graph.y_dx = y_dx\n",
    "        in_graph.end_pos = y_pos_end\n",
    "        in_graph.end_vel = y_vel_end\n",
    "        in_graph.node_type = node_type\n",
    "        \n",
    "        return in_graph     \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.in_graphs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.in_graphs[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be11086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded competitor split!\n",
      "[HumanDataset:train] built 198 samples\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "Loaded competitor split!\n",
      "[HumanDataset:val] built 600 samples\n"
     ]
    }
   ],
   "source": [
    "dataset_train = HumanDataset(partition='train', max_samples=model_settings[\"max_training_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)\n",
    "dataset_test = HumanDataset(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)\n",
    "dataset_val = HumanDataset(partition='val', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c270d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All edges are bidirectional.\n"
     ]
    }
   ],
   "source": [
    "gt = next(iter(dataset_val))\n",
    "# Convert to set of tuples\n",
    "edges = set((i.item(), j.item()) for i, j in zip(gt.edge_index[0], gt.edge_index[1]))\n",
    "\n",
    "# Check for missing reverse edges\n",
    "missing = [(j, i) for (i, j) in edges if (j, i) not in edges]\n",
    "\n",
    "if len(missing) == 0:\n",
    "    print(\" All edges are bidirectional.\")\n",
    "else:\n",
    "    print(f\"{len(missing)} edge(s) are not bidirectional:\")\n",
    "    print(missing[:10])  # print a few missing edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cc051",
   "metadata": {},
   "source": [
    "The next cell creates the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7f784c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(data):\n",
    "    graph_current = data\n",
    "    \n",
    "    return graph_current\n",
    "\n",
    "class GraphFromRawDataset(Dataset):\n",
    "    def __init__(self, raw_dataset):\n",
    "        self.raw_dataset = raw_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.raw_dataset[idx]\n",
    "        return create_graph_data(data)\n",
    "\n",
    "def create_dataloaders_from_raw(dataset, M, shuffle=True):\n",
    "    \"\"\"\n",
    "    M: number of graphs to batch in training (use 1 for test/val)\n",
    "    \"\"\"\n",
    "    # Wrap raw datasets into PyTorch Dataset\n",
    "    dataset = GraphFromRawDataset(dataset)\n",
    "\n",
    "    # Create dataloaders\n",
    "    loader = DataLoader(dataset, batch_size=M, shuffle=shuffle, collate_fn=Batch.from_data_list)\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28af6d2c-14cb-4f14-b0cf-b6a14387d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloaders_from_raw(dataset_train,M=model_settings[\"batch_size\"])\n",
    "val_loader = create_dataloaders_from_raw(dataset_val,M=model_settings[\"batch_size\"], shuffle=False)\n",
    "test_loader = create_dataloaders_from_raw(dataset_test,M=model_settings[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f481e76-50e2-42df-96f9-372eadad2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_max_edge(train_loader):\n",
    "    \"\"\"\n",
    "    Calculate min/max statistics for graph properties using the ConsecutiveGraphDataset.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    timestep_dict : dict\n",
    "        Dictionary containing graphs with keys as timesteps.\n",
    "    time_step_increment : int, optional\n",
    "        Time step increment to use (default: 1).\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    tuple:\n",
    "        Min/max statistics for various physical properties.\n",
    "    \"\"\"\n",
    "    # Initialize lists to collect data\n",
    "    all_edge_dx = []\n",
    "    all_node_v_t = []\n",
    "    all_node_v_tm1 = []\n",
    "    all_node_dv = []\n",
    "    all_node_dx = []\n",
    "    \n",
    "    # Process all valid timesteps\n",
    "    for batch in train_loader:\n",
    "        batched_graph = batch\n",
    "        \n",
    "        senders,receivers = batched_graph.edge_index\n",
    "\n",
    "        edge_dx = batched_graph.pos[receivers] - batched_graph.pos[senders]\n",
    "        \n",
    "        # Extract positions and velocities\n",
    "        node_vel_t = batched_graph.vel.float()\n",
    "        node_vel_tm1 = batched_graph.prev_vel.float()\n",
    "        \n",
    "        # Calculate displacements and acceleration changes\n",
    "        mask_body = (batched_graph.node_type!=2).squeeze()\n",
    "        node_dv = (batched_graph.y_dv).float()\n",
    "        node_dx = (batched_graph.y_dx).float()\n",
    "        \n",
    "        # Collect data\n",
    "        all_edge_dx.append(edge_dx)\n",
    "        all_node_v_t.append(node_vel_t)\n",
    "        all_node_v_tm1.append(node_vel_tm1)\n",
    "        all_node_dv.append(node_dv)\n",
    "        all_node_dx.append(node_dx)\n",
    "    \n",
    "    # Concatenate all collected data\n",
    "    all_edge_dx = torch.cat(all_edge_dx, dim=0)\n",
    "    all_node_v_t = torch.cat(all_node_v_t, dim=0)\n",
    "    all_node_v_tm1 = torch.cat(all_node_v_tm1, dim=0)\n",
    "    all_node_dv= torch.cat(all_node_dv, dim=0)\n",
    "    all_node_dx = torch.cat(all_node_dx,dim=0)\n",
    "    \n",
    "    # Compute norms\n",
    "    norm_edge_dx = all_edge_dx.norm(dim=1)\n",
    "    norm_node_v_t = all_node_v_t.norm(dim=1)\n",
    "    norm_node_v_tm1 = all_node_v_tm1.norm(dim=1)\n",
    "    norm_node_dv = all_node_dv.norm(dim=1)\n",
    "    norm_node_dx = all_node_dx.norm(dim=1)\n",
    "    \n",
    "    # Compute min and max values of the norms\n",
    "    min_edge_dx = norm_edge_dx.min()\n",
    "    max_edge_dx = norm_edge_dx.max()\n",
    "\n",
    "    min_node_v_t = norm_node_v_t.min()\n",
    "    max_node_v_t = norm_node_v_t.max()\n",
    "\n",
    "    min_node_v_tm1 = norm_node_v_tm1.min()\n",
    "    max_node_v_tm1 = norm_node_v_tm1.max()\n",
    "\n",
    "    mean_node_dv = norm_node_dv.mean()\n",
    "    std_node_dv = norm_node_dv.std()\n",
    "\n",
    "    mean_node_dx = norm_node_dx.mean()\n",
    "    std_node_dx = norm_node_dx.std()\n",
    "\n",
    "    # Collect statistics in tuples\n",
    "    stat_edge_dx = (min_edge_dx, max_edge_dx)\n",
    "    stat_node_v_t = (min_node_v_t, max_node_v_t)\n",
    "    stat_node_v_tm1 = (min_node_v_tm1, max_node_v_tm1)\n",
    "    stat_node_dv = (mean_node_dv, std_node_dv)\n",
    "    stat_node_dx = (mean_node_dx, std_node_dx)\n",
    "    \n",
    "    return stat_edge_dx, stat_node_v_t, stat_node_dv, stat_node_dx\n",
    "\n",
    "def move_train_stats_to_device(train_stats, device):\n",
    "    def move_to_device(stat):\n",
    "        if len(stat)==2:\n",
    "            min_val, max_val = stat\n",
    "            return min_val.to(device), max_val.to(device)\n",
    "        else: \n",
    "            max_val = stat\n",
    "            return max_val.to(device)\n",
    "        \n",
    "\n",
    "    return tuple(move_to_device(stat) for stat in train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13699dea",
   "metadata": {},
   "source": [
    "## Dynamical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da637d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = calculate_min_max_edge(train_loader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_stats = move_train_stats_to_device(stats, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b93cc",
   "metadata": {},
   "source": [
    "### Components for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ebc83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_d(in_size, hidden_size, out_size, num_layers=1, lay_norm=True, use_sigmoid=False, use_softmax=False):\n",
    "    \"\"\"\n",
    "    Builds a multi-layer perceptron (MLP) with configurable depth and optional layer normalization and sigmoid or softmax activation.\n",
    "\n",
    "    Args:\n",
    "        in_size (int): The size of the input feature vector.\n",
    "        hidden_size (int): The size of the hidden layers.\n",
    "        out_size (int): The size of the output layer.\n",
    "        num_layers (int): The number of layers in the MLP.\n",
    "        lay_norm (bool): Flag to add layer normalization after the last linear layer.\n",
    "        use_sigmoid (bool): Flag to add a sigmoid activation layer at the output.\n",
    "        use_softmax (bool): Flag to add a softmax activation layer at the output.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The constructed MLP model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If both use_sigmoid and use_softmax are True.\n",
    "    \"\"\"\n",
    "    if use_sigmoid and use_softmax:\n",
    "        raise ValueError(\"Only one of use_sigmoid or use_softmax can be true.\")\n",
    "    layers = [nn.Linear(in_size, hidden_size), nn.ReLU()]\n",
    "    \n",
    "    # Add intermediate layers\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "    # Add the output layer\n",
    "    layers.append(nn.Linear(hidden_size, out_size))\n",
    "\n",
    "    # Create the MLP module\n",
    "    module = nn.Sequential(*layers)\n",
    "\n",
    "    # Optionally add layer normalization\n",
    "    if lay_norm:\n",
    "        module = nn.Sequential(module, nn.LayerNorm(normalized_shape=out_size))\n",
    "\n",
    "    # Optionally add sigmoid activation\n",
    "    if use_sigmoid:\n",
    "        module = nn.Sequential(module, nn.Sigmoid())\n",
    "\n",
    "    # Optionally add softmax activation\n",
    "    if use_softmax:\n",
    "        module = nn.Sequential(module, nn.Softmax(dim=-1))\n",
    "\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8c8ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RefFrameCalc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefFrameCalc, self).__init__()\n",
    "\n",
    "    def forward(self, edge_index, senders_pos, receivers_pos, senders_vel, receivers_vel, senders_omega, receivers_omega):\n",
    "        senders, receivers = edge_index\n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        diff = receivers_pos - senders_pos\n",
    "        vector_a = diff / torch.clamp(diff.norm(dim=1, keepdim=True), min=epsilon)\n",
    "\n",
    "        b_a = torch.cross(receivers_vel - senders_vel, vector_a, dim=1)\n",
    "        b_a = b_a / torch.clamp(b_a.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        \n",
    "        b_c = (senders_vel + receivers_vel)\n",
    "        b_c = b_c / torch.clamp(b_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        \n",
    "        # Omega terms\n",
    "        b_a_ = torch.cross(receivers_omega - senders_omega, vector_a, dim=1)\n",
    "        b_a_ = b_a_ / torch.clamp(b_a_.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        \n",
    "        b_c_ = (senders_omega + receivers_omega)\n",
    "        b_c_ = b_c_ / torch.clamp(b_c_.norm(dim=1, keepdim=True), min=epsilon)\n",
    "\n",
    "        b = b_a + b_c + b_a_ + b_c_ \n",
    "\n",
    "        # Parallel and Perpendicular components\n",
    "        b_prl_dot = torch.einsum('ij,ij->i', b, vector_a).unsqueeze(1)\n",
    "        b_prl = b_prl_dot * vector_a\n",
    "        b_prp = b - b_prl\n",
    "\n",
    "        vector_b = torch.cross(b_prp, vector_a, dim=1)\n",
    "        vector_c = torch.cross(b_prl, vector_b, dim=1)\n",
    "        \n",
    "        vector_b = vector_b / torch.clamp(vector_b.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        vector_c = vector_c / torch.clamp(vector_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "   \n",
    "        return vector_a, vector_b, vector_c\n",
    "\n",
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.node_encoder = build_mlp_d(2, latent_size, latent_size, num_layers=1, lay_norm=True)\n",
    "    def forward(self, node_scalar_feat):\n",
    "        return self.node_encoder(node_scalar_feat)\n",
    "\n",
    "class InteractionEncoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(InteractionEncoder, self).__init__()\n",
    "        self.edge_feat_encoder = build_mlp_d(9, latent_size, latent_size, num_layers=1, lay_norm=True)\n",
    "        self.edge_encoder = build_mlp_d(3, latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "        self.interaction_encoder = build_mlp_d(3*latent_size, latent_size, latent_size, num_layers=1, lay_norm=True)\n",
    "\n",
    "    def forward(self, edge_index, edge_dx_, edge_attr, vector_a, vector_b, vector_c,\n",
    "                senders_v_t, senders_v_tm1, senders_w_t, senders_th_t,\n",
    "                receivers_v_t, receivers_v_tm1, receivers_w_t, receivers_th_t,\n",
    "                node_latent):\n",
    "        \n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        def project(vec, va, vb, vc):\n",
    "             # Helper to project vectors onto frame\n",
    "             pa = torch.einsum('ij,ij->i', vec, va).unsqueeze(1)\n",
    "             pb = torch.einsum('ij,ij->i', vec, vb).unsqueeze(1)\n",
    "             pc = torch.einsum('ij,ij->i', vec, vc).unsqueeze(1)\n",
    "             return pa, pb, pc\n",
    "\n",
    "        # Senders projection\n",
    "        svt_a, svt_b, svt_c = project(senders_v_t, vector_a, vector_b, vector_c)\n",
    "        svtm1_a, svtm1_b, svtm1_c = project(senders_v_tm1, vector_a, vector_b, vector_c)\n",
    "        swt_a, swt_b, swt_c = project(senders_w_t, vector_a, vector_b, vector_c)\n",
    "\n",
    "        # Receivers projection (vectors are negated for receiver perspective relative to edge)\n",
    "        rvt_a, rvt_b, rvt_c = project(receivers_v_t, -vector_a, -vector_b, -vector_c)\n",
    "        rvtm1_a, rvtm1_b, rvtm1_c = project(receivers_v_tm1, -vector_a, -vector_b, -vector_c)\n",
    "        rwt_a, rwt_b, rwt_c = project(receivers_w_t, -vector_a, -vector_b, -vector_c)\n",
    "\n",
    "        edge_dx_a_s = edge_dx_.norm(dim=1, keepdim=True)\n",
    "        edge_dth_a_s = (receivers_th_t - senders_th_t).norm(dim=1, keepdim=True)\n",
    "\n",
    "        senders_features = torch.hstack((svt_a, svt_b, svt_c, svtm1_a, svtm1_b, svtm1_c, swt_a, swt_b, swt_c))\n",
    "        receivers_features = torch.hstack((rvt_a, rvt_b, rvt_c, rvtm1_a, rvtm1_b, rvtm1_c, rwt_a, rwt_b, rwt_c))\n",
    "        \n",
    "        edge_latent = self.edge_encoder(torch.hstack((edge_dx_a_s, edge_dth_a_s, edge_attr)))\n",
    "        senders_latent = self.edge_feat_encoder(senders_features)\n",
    "        receivers_latent = self.edge_feat_encoder(receivers_features)\n",
    "\n",
    "        concat_latent = torch.hstack((senders_latent + receivers_latent,\n",
    "                                      node_latent[senders] + node_latent[receivers],\n",
    "                                      edge_latent))\n",
    "        return self.interaction_encoder(concat_latent)\n",
    "\n",
    "class InteractionDecoder(nn.Module):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(InteractionDecoder, self).__init__()\n",
    "        self.i1_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=1, lay_norm=False)\n",
    "        self.i2_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=1, lay_norm=False)\n",
    "        self.node_weight_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "        self.f_scaler = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "\n",
    "    def forward(self, edge_index, vector_a, vector_b, vector_c, interaction_latent, senders_x_t, receivers_x_t,node_latent):\n",
    "        senders, receivers = edge_index\n",
    "        coeff_f = self.i1_decoder(interaction_latent)\n",
    "        coeff_t = self.i2_decoder(interaction_latent)\n",
    "\n",
    "        fij = (coeff_f[:, 0:1] * vector_a + coeff_f[:, 1:2] * vector_b + coeff_f[:, 2:] * vector_c)\n",
    "        tij = (coeff_t[:, 0:1] * vector_a + coeff_t[:, 1:2] * vector_b + coeff_t[:, 2:] * vector_c)        \n",
    "        node_weights = self.node_weight_decoder(node_latent)\n",
    "        r0ij = ((node_weights[senders]*senders_x_t[senders] + node_weights[receivers]*receivers_x_t[receivers])/(node_weights[senders] + node_weights[receivers]))\n",
    "        tauij = tij-torch.cross(receivers_x_t[receivers]-r0ij,fij)*self.f_scaler(interaction_latent)\n",
    "        return fij, tauij\n",
    "\n",
    "class Node_Internal_Dv_Decoder(nn.Module):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(Node_Internal_Dv_Decoder, self).__init__()\n",
    "        self.m_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "        self.i_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "        self.dv_ext_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "\n",
    "    def forward(self, edge_index, node_latent, fij, tij, device):\n",
    "        m_inv = self.m_inv_decoder(node_latent)\n",
    "        i_inv = self.i_inv_decoder(node_latent)\n",
    "        senders, receivers = edge_index   \n",
    "        \n",
    "        out_fij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_fij = out_fij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]), fij)\n",
    "        node_dv_int = m_inv * out_fij + self.dv_ext_decoder(node_latent)\n",
    "        \n",
    "        out_tij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_tij = out_tij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]), tij)\n",
    "        node_dw_int = i_inv * out_tij       \n",
    "\n",
    "        return node_dv_int, node_dw_int\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scaler, self).__init__()\n",
    "\n",
    "    def forward(self, senders_v_t, senders_v_tm1, receivers_v_t, receivers_v_tm1, edge_dx, train_stats):\n",
    "        stat_edge_dx, stat_node_v_t, _, _ = train_stats\n",
    "        \n",
    "        max_vel = stat_node_v_t[1].detach()\n",
    "        senders_v_t_ = senders_v_t / max_vel\n",
    "        senders_v_tm1_ = senders_v_tm1 / max_vel\n",
    "        receivers_v_t_ = receivers_v_t / max_vel\n",
    "        receivers_v_tm1_ = receivers_v_tm1 / max_vel\n",
    "        \n",
    "        norm_edge_dx = edge_dx.norm(dim=1, keepdim=True)\n",
    "        min_dx, max_dx = stat_edge_dx[0], stat_edge_dx[1]\n",
    "        \n",
    "        edge_dx_ = (((norm_edge_dx - min_dx) / (max_dx - min_dx)) * (edge_dx / norm_edge_dx)).detach()\n",
    "        return senders_v_t_, senders_v_tm1_, receivers_v_t_, receivers_v_tm1_, edge_dx_\n",
    "\n",
    "class Interaction_Block(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Interaction_Block, self).__init__()\n",
    "        self.interaction_encoder = InteractionEncoder(latent_size)\n",
    "        self.interaction_decoder = InteractionDecoder(latent_size)\n",
    "        self.internal_dv_decoder = Node_Internal_Dv_Decoder(latent_size)\n",
    "        self.layer_norm = nn.LayerNorm(latent_size)\n",
    "\n",
    "    def forward(self, edge_index, edge_dx_, edge_attr, vector_a, vector_b, vector_c, \n",
    "                senders_x_t,senders_v_t, senders_v_tm1, senders_w_t, senders_th_t,\n",
    "                receivers_x_t,receivers_v_t, receivers_v_tm1, receivers_w_t, receivers_th_t,\n",
    "                node_latent, residue=None, latent_history=False):\n",
    "        \n",
    "        interaction_latent = self.interaction_encoder(\n",
    "            edge_index, edge_dx_, edge_attr, vector_a, vector_b, vector_c,\n",
    "            senders_v_t, senders_v_tm1, senders_w_t, senders_th_t,\n",
    "            receivers_v_t, receivers_v_tm1, receivers_w_t, receivers_th_t,\n",
    "            node_latent\n",
    "        )\n",
    "\n",
    "        if latent_history:\n",
    "            interaction_latent = interaction_latent + residue\n",
    "            interaction_latent = self.layer_norm(interaction_latent)\n",
    "        \n",
    "        fij, tij = self.interaction_decoder(edge_index, vector_a, vector_b, vector_c, interaction_latent, senders_x_t, receivers_x_t,node_latent)\n",
    "        \n",
    "        device = fij.device\n",
    "        node_dv, node_dw = self.internal_dv_decoder(edge_index, node_latent, fij, tij, device)\n",
    "    \n",
    "        return node_dv, node_dw, interaction_latent\n",
    "\n",
    "class DynamicsSolver(nn.Module):\n",
    "    def __init__(self, sample_step, train_stats, num_jumps=1, num_msgs=1, latent_size=128):\n",
    "        super(DynamicsSolver, self).__init__()\n",
    "        self.refframecalc = RefFrameCalc()\n",
    "        self.scaler = Scaler()\n",
    "        self.node_encoder = NodeEncoder(latent_size)\n",
    "        self.interaction_proc_layer = Interaction_Block(latent_size)\n",
    "        self.interaction_init_layer = Interaction_Block(latent_size)\n",
    "        self.num_messages = num_msgs\n",
    "        self.sub_tstep = sample_step / num_msgs\n",
    "        self.train_stats = train_stats\n",
    "\n",
    "    def forward(self, graph):\n",
    "        device = graph.pos.device\n",
    "        \n",
    "        node_type = graph.node_type.float()\n",
    "        pos = graph.pos.float()\n",
    "        vel = graph.vel.float()\n",
    "        prev_vel = graph.prev_vel.float()\n",
    "        \n",
    "        edge_index = graph.edge_index.long()\n",
    "        senders, receivers = edge_index\n",
    "        edge_attr = graph.edge_attr.float()\n",
    "        \n",
    "        # Handle state variables\n",
    "        node_v_t = vel\n",
    "        node_w_t = getattr(graph, 'node_w_t', torch.zeros_like(node_v_t))\n",
    "        node_th_t = getattr(graph, 'node_th_t', torch.zeros_like(node_v_t))\n",
    "        \n",
    "        mask_reflected_node = (graph.node_type != 2).squeeze()\n",
    "        \n",
    "        # Init accumulators\n",
    "        sum_node_dv = torch.zeros_like(node_v_t)\n",
    "        sum_node_dx = torch.zeros_like(node_v_t)\n",
    "        node_vf = torch.zeros_like(node_v_t)\n",
    "        node_wf = torch.zeros_like(node_v_t)\n",
    "        \n",
    "        node_latent = self.node_encoder(torch.hstack((node_type, vel[:, 1:2])))\n",
    "\n",
    "        # Temp vars for loop\n",
    "        cur_pos = pos.clone()\n",
    "        cur_node_v_t = node_v_t.clone()\n",
    "        cur_node_v_tm1 = prev_vel.clone() # Initially prev_vel\n",
    "        cur_node_w_t = node_w_t.clone()\n",
    "        cur_node_th_t = node_th_t.clone()\n",
    "\n",
    "        for i in range(self.num_messages):\n",
    "            senders_pos = cur_pos[senders]\n",
    "            receivers_pos = cur_pos[receivers]\n",
    "            edge_dx = receivers_pos - senders_pos\n",
    "\n",
    "            senders_v_t = cur_node_v_t[senders]\n",
    "            receivers_v_t = cur_node_v_t[receivers]\n",
    "            senders_v_tm1 = cur_node_v_tm1[senders]\n",
    "            receivers_v_tm1 = cur_node_v_tm1[receivers]\n",
    "            \n",
    "            s_vt_s, s_vtm1_s, r_vt_s, r_vtm1_s, edge_dx_s = self.scaler(\n",
    "                senders_v_t, senders_v_tm1, receivers_v_t, receivers_v_tm1, edge_dx, self.train_stats\n",
    "            )\n",
    "\n",
    "            vector_a, vector_b, vector_c = self.refframecalc(\n",
    "                edge_index, senders_pos, receivers_pos, s_vt_s, r_vt_s, \n",
    "                cur_node_w_t[senders], cur_node_w_t[receivers]\n",
    "            )\n",
    "            \n",
    "            common_args = (\n",
    "                edge_index, edge_dx_s, edge_attr, vector_a, vector_b, vector_c,\n",
    "                cur_pos[senders], s_vt_s, s_vtm1_s, cur_node_w_t[senders], cur_node_th_t[senders],\n",
    "                cur_pos[receivers],r_vt_s, r_vtm1_s, cur_node_w_t[receivers], cur_node_th_t[receivers],\n",
    "                node_latent\n",
    "            )\n",
    "\n",
    "            if i == 0:\n",
    "                node_dv, node_dw, residue = self.interaction_init_layer(*common_args, latent_history=False)\n",
    "            else:\n",
    "                node_dv, node_dw, residue = self.interaction_proc_layer(*common_args, residue=residue, latent_history=True)\n",
    "\n",
    "            # Update State\n",
    "            sum_node_dv[mask_reflected_node] += node_dv[mask_reflected_node]\n",
    "            node_vf[mask_reflected_node] = cur_node_v_t[mask_reflected_node] + node_dv[mask_reflected_node]\n",
    "            node_wf[mask_reflected_node] = cur_node_w_t[mask_reflected_node] + node_dw[mask_reflected_node]\n",
    "\n",
    "            step_disp = (cur_node_v_t + node_vf) * 0.5 * self.sub_tstep\n",
    "            sum_node_dx[mask_reflected_node] += step_disp[mask_reflected_node]\n",
    "            \n",
    "            # Prepare next sub-step\n",
    "            cur_pos = cur_pos + step_disp\n",
    "            cur_node_th_t = cur_node_th_t + (node_wf + cur_node_w_t) * 0.5 * self.sub_tstep\n",
    "            \n",
    "            cur_node_v_tm1 = cur_node_v_t.clone()\n",
    "            cur_node_v_t = node_vf.clone()\n",
    "            cur_node_w_t = node_wf.clone()\n",
    "\n",
    "        return sum_node_dv, sum_node_dx, cur_node_v_tm1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55077fc9",
   "metadata": {},
   "source": [
    "# CODE OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18ebb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefFrameCalc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefFrameCalc, self).__init__()\n",
    "\n",
    "    def forward(self, edge_index,senders_pos,receivers_pos, senders_vel,receivers_vel, senders_omega, receivers_omega):\n",
    "        \n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        vector_a = (receivers_pos - senders_pos)/ torch.clamp((receivers_pos - senders_pos).norm(dim=1, keepdim=True), min=epsilon)\n",
    "\n",
    "        #prelimnary vectors\n",
    "        b_a = torch.cross(receivers_vel-senders_vel,vector_a,dim=1)\n",
    "        b_a = b_a / torch.clamp(b_a.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        b_c = (senders_vel + receivers_vel)\n",
    "        b_c = b_c / torch.clamp(b_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        \n",
    "        b_a_ = torch.cross(receivers_omega-senders_omega,vector_a,dim=1)\n",
    "        b_a_ = b_a_ / torch.clamp(b_a_.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        b_c_ = (senders_omega + receivers_omega)\n",
    "        b_c_ = b_c_ / torch.clamp(b_c_.norm(dim=1, keepdim=True), min=epsilon)\n",
    "\n",
    "        b = b_a + b_c + b_a_ + b_c_ \n",
    "\n",
    "        # Compute the parallel component of b\n",
    "        b_prl_dot = torch.einsum('ij,ij->i', b, vector_a).unsqueeze(1)\n",
    "        b_prl = b_prl_dot * vector_a\n",
    "\n",
    "        # Compute the perpendicular component of b\n",
    "        b_prp = b - b_prl\n",
    "\n",
    "        vector_b = torch.cross(b_prp, vector_a,dim=1) #perp to a and a new vector b_prp\n",
    "        vector_c = torch.cross(b_prl, vector_b,dim=1) #perp to a and b\n",
    "        \n",
    "        vector_b = vector_b / torch.clamp(vector_b.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        vector_c = vector_c / torch.clamp(vector_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "   \n",
    "        return vector_a, vector_b, vector_c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12582806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.node_encoder = build_mlp_d(2, latent_size, latent_size, num_layers=1, lay_norm=True)\n",
    "    def forward(self,node_scalar_feat):\n",
    "        node_latent = self.node_encoder(node_scalar_feat)  \n",
    "        return node_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "679e5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionEncoder(nn.Module):\n",
    "    \"\"\"Message passing.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size):\n",
    "        super(InteractionEncoder, self).__init__()\n",
    "        self.edge_feat_encoder = build_mlp_d(9, latent_size, latent_size, num_layers=1, lay_norm=True)\n",
    "        self.edge_encoder = build_mlp_d(3, latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "        self.interaction_encoder = build_mlp_d(3*latent_size,latent_size, latent_size, num_layers=1, lay_norm=True)\n",
    "    def forward(self, edge_index, edge_dx_,edge_attr, vector_a, vector_b, vector_c,\n",
    "                senders_v_t_, senders_v_tm1_, senders_w_t_,senders_th_t_,\n",
    "                receivers_v_t_, receivers_v_tm1_, receivers_w_t_,receivers_th_t_,\n",
    "                node_latent):\n",
    "\n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        node_v_t_senders_a = torch.einsum('ij,ij->i', senders_v_t_, vector_a).unsqueeze(1)\n",
    "        node_v_t_senders_b = torch.einsum('ij,ij->i', senders_v_t_, vector_b).unsqueeze(1)\n",
    "        node_v_t_senders_c = torch.einsum('ij,ij->i', senders_v_t_, vector_c).unsqueeze(1)\n",
    "\n",
    "        node_v_tm1_senders_a = torch.einsum('ij,ij->i', senders_v_tm1_, vector_a).unsqueeze(1)\n",
    "        node_v_tm1_senders_b = torch.einsum('ij,ij->i', senders_v_tm1_, vector_b).unsqueeze(1)\n",
    "        node_v_tm1_senders_c = torch.einsum('ij,ij->i', senders_v_tm1_, vector_c).unsqueeze(1)    \n",
    "\n",
    "        node_w_t_senders_a = torch.einsum('ij,ij->i', senders_w_t_, vector_a).unsqueeze(1)\n",
    "        node_w_t_senders_b = torch.einsum('ij,ij->i', senders_w_t_, vector_b).unsqueeze(1)\n",
    "        node_w_t_senders_c = torch.einsum('ij,ij->i', senders_w_t_, vector_c).unsqueeze(1)  \n",
    "        \n",
    "        node_v_t_receivers_a = torch.einsum('ij,ij->i', receivers_v_t_, -vector_a).unsqueeze(1)\n",
    "        node_v_t_receivers_b = torch.einsum('ij,ij->i', receivers_v_t_, -vector_b).unsqueeze(1)\n",
    "        node_v_t_receivers_c = torch.einsum('ij,ij->i', receivers_v_t_, -vector_c).unsqueeze(1)\n",
    "\n",
    "        node_v_tm1_receivers_a = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_a).unsqueeze(1)\n",
    "        node_v_tm1_receivers_b = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_b).unsqueeze(1)\n",
    "        node_v_tm1_receivers_c = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_c).unsqueeze(1)       \n",
    "        \n",
    "        node_w_t_receivers_a = torch.einsum('ij,ij->i', receivers_w_t_, -vector_a).unsqueeze(1)\n",
    "        node_w_t_receivers_b = torch.einsum('ij,ij->i', receivers_w_t_, -vector_b).unsqueeze(1)\n",
    "        node_w_t_receivers_c = torch.einsum('ij,ij->i', receivers_w_t_, -vector_c).unsqueeze(1)\n",
    "        \n",
    "        edge_dx_a_s = edge_dx_.norm(dim=1,keepdim=True)\n",
    "        edge_dth_a_s = (receivers_th_t_ - senders_th_t_).norm(dim=1,keepdim=True)\n",
    "\n",
    "        senders_features = torch.hstack((\n",
    "            node_v_t_senders_a, node_v_t_senders_b, node_v_t_senders_c,\n",
    "            node_v_tm1_senders_a, node_v_tm1_senders_b, node_v_tm1_senders_c,\n",
    "            node_w_t_senders_a, node_w_t_senders_b, node_w_t_senders_c\n",
    "        ))\n",
    "\n",
    "        receivers_features = torch.hstack((\n",
    "            node_v_t_receivers_a, node_v_t_receivers_b, node_v_t_receivers_c,\n",
    "            node_v_tm1_receivers_a, node_v_tm1_receivers_b, node_v_tm1_receivers_c,\n",
    "            node_w_t_receivers_a, node_w_t_receivers_b, node_w_t_receivers_c\n",
    "        ))\n",
    "        \n",
    "        edge_latent = self.edge_encoder(torch.hstack((edge_dx_a_s, edge_dth_a_s, edge_attr)))\n",
    "\n",
    "        senders_latent = self.edge_feat_encoder(senders_features)\n",
    "        receivers_latent = self.edge_feat_encoder(receivers_features)\n",
    "\n",
    "        interaction_latent = self.interaction_encoder(torch.hstack((senders_latent + receivers_latent,\n",
    "                                                                    node_latent[senders]+node_latent[receivers],\n",
    "                                                                    edge_latent)))\n",
    "\n",
    "        return interaction_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "368fce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(InteractionDecoder, self).__init__()\n",
    "        self.i1_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=1, lay_norm=False)\n",
    "        self.i2_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=1, lay_norm=False)\n",
    "\n",
    "    def forward(self, edge_index, senders_pos, receivers_pos, vector_a, vector_b, vector_c, interaction_latent, node_latent):\n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        coeff_f = self.i1_decoder(interaction_latent)\n",
    "        coeff_t = self.i2_decoder(interaction_latent)\n",
    "\n",
    "        fij = (coeff_f[:, 0:1] * vector_a + \n",
    "              coeff_f[:, 1:2] * vector_b + \n",
    "              coeff_f[:, 2:] * vector_c)\n",
    "        \n",
    "        \n",
    "        tij = (coeff_t[:, 0:1] * vector_a + \n",
    "              coeff_t[:, 1:2] * vector_b + \n",
    "              coeff_t[:, 2:] * vector_c)        \n",
    "        return fij, tij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a29b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_Internal_Dv_Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(Node_Internal_Dv_Decoder, self).__init__()\n",
    "        self.m_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "        self.i_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "        self.dv_ext_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "    def forward(self,edge_index,node_latent,fij,tij):\n",
    "        m_inv = self.m_inv_decoder(node_latent) # decode inverse of mass\n",
    "        i_inv = self.i_inv_decoder(node_latent) # decode inverse of inertia\n",
    "        senders,receivers = edge_index   \n",
    "        \n",
    "        out_fij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_fij = out_fij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]).to(device), fij.to(device))\n",
    "        node_dv_int = m_inv * (out_fij) + self.dv_ext_decoder(node_latent)\n",
    "        \n",
    "        out_tij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_tij = out_tij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]).to(device), tij.to(device))\n",
    "        node_dw_int = i_inv * out_tij       \n",
    "\n",
    "        return node_dv_int, node_dw_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70559816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scaler, self).__init__()\n",
    "        '''\n",
    "        Scales the velocity and angular velocity features by maximum magnitude of respective field in training data\n",
    "        Scales the magnitude of the edge_vector_dx using min-max scaling. (keeping the direction of edge_vector_dx same)\n",
    "        '''\n",
    "\n",
    "    def forward(self, senders_v_t, senders_v_tm1,receivers_v_t, receivers_v_tm1,edge_dx,train_stats):\n",
    "        stat_edge_dx, stat_node_v_t, _,_= train_stats\n",
    "        \n",
    "        senders_v_t_ = senders_v_t/stat_node_v_t[1].detach()\n",
    "        senders_v_tm1_ = senders_v_tm1/stat_node_v_t[1].detach()\n",
    "        receivers_v_t_ = receivers_v_t/stat_node_v_t[1].detach()\n",
    "        receivers_v_tm1_ = receivers_v_tm1/stat_node_v_t[1].detach()\n",
    "        norm_edge_dx = edge_dx.norm(dim=1, keepdim=True)\n",
    "        edge_dx_ = (((norm_edge_dx-stat_edge_dx[0])/(stat_edge_dx[1]-stat_edge_dx[0]))*(edge_dx/norm_edge_dx)).detach()\n",
    "        return senders_v_t_, senders_v_tm1_,receivers_v_t_, receivers_v_tm1_,edge_dx_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60303a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction_Block(torch.nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Interaction_Block, self).__init__()\n",
    "        self.interaction_encoder = InteractionEncoder(latent_size)\n",
    "        self.interaction_decoder = InteractionDecoder(latent_size)\n",
    "        self.internal_dv_decoder = Node_Internal_Dv_Decoder(latent_size)\n",
    "        self.layer_norm = nn.LayerNorm(latent_size)\n",
    "\n",
    "    def forward(self, edge_index, senders_pos, receivers_pos, edge_dx_, edge_attr,vector_a, vector_b, vector_c, \n",
    "                senders_v_t_, senders_v_tm1_, senders_w_t_,senders_th_t_,\n",
    "                receivers_v_t_, receivers_v_tm1_, receivers_w_t_,receivers_th_t_,\n",
    "                node_latent, residue=None, latent_history=False):\n",
    "            interaction_latent = self.interaction_encoder(edge_index, edge_dx_,edge_attr,\n",
    "                                                          vector_a, vector_b, vector_c,\n",
    "                                                          senders_v_t_, senders_v_tm1_, senders_w_t_,senders_th_t_,\n",
    "                                                          receivers_v_t_, receivers_v_tm1_, receivers_w_t_,receivers_th_t_,\n",
    "                                                          node_latent)\n",
    "\n",
    "            if latent_history:\n",
    "                interaction_latent = interaction_latent + residue\n",
    "                interaction_latent = self.layer_norm(interaction_latent)\n",
    "            \n",
    "            edge_interaction_force, edge_interaction_tau= self.interaction_decoder(\n",
    "                edge_index, senders_pos, receivers_pos, vector_a, vector_b, vector_c, interaction_latent, node_latent\n",
    "            )\n",
    "            node_dv_int_decoded, node_dw_int_decoded = self.internal_dv_decoder(\n",
    "                edge_index, node_latent, edge_interaction_force, edge_interaction_tau\n",
    "            )\n",
    "        \n",
    "            return node_dv_int_decoded, node_dw_int_decoded, interaction_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsSolver(torch.nn.Module):\n",
    "    def __init__(self, sample_step, train_stats, num_jumps=1, num_msgs=1, latent_size=128):\n",
    "        super(DynamicsSolver, self).__init__()\n",
    "        self.refframecalc = RefFrameCalc()\n",
    "        self.scaler = Scaler()\n",
    "        self.node_encoder = NodeEncoder(latent_size)\n",
    "        self.interaction_proc_layer = Interaction_Block(latent_size)\n",
    "        self.interaction_init_layer = Interaction_Block(latent_size)\n",
    "        self.num_messages = num_msgs\n",
    "        self.sub_tstep = sample_step / num_msgs\n",
    "        self.train_stats = train_stats\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # Initialize graph data for processing\n",
    "        device = graph.pos.device\n",
    "        graph = graph.to(device)\n",
    "        #node_type = torch.ones_like(graph.disp[:,0:1]).float()# get_node_type(graph).float()\n",
    "        node_type = graph.node_type.float()\n",
    "        pos = graph.pos.float()\n",
    "        vel = graph.vel.float()\n",
    "        prev_vel = graph.prev_vel.float()\n",
    "        \n",
    "        edge_index = graph.edge_index.long()\n",
    "        senders, receivers = edge_index\n",
    "        senders_pos = pos[senders]\n",
    "        receivers_pos = pos[receivers]\n",
    "        edge_dx = receivers_pos - senders_pos\n",
    "        edge_attr = graph.edge_attr.float()\n",
    "        \n",
    "        mask_reflected_node = (graph.node_type!=2).squeeze()\n",
    "\n",
    "        node_v_t = vel\n",
    "        node_w_t = getattr(graph, 'node_w_t',torch.zeros_like(node_v_t))\n",
    "        node_th_t = getattr(graph, 'node_th_t', torch.zeros_like(node_v_t))\n",
    "\n",
    "        senders_v_t = node_v_t[senders].float()\n",
    "        receivers_v_t = node_v_t[receivers].float()\n",
    "\n",
    "        senders_v_tm1 = prev_vel[senders].float()\n",
    "        receivers_v_tm1 = prev_vel[receivers].float()\n",
    "\n",
    "        senders_w_t = node_w_t[senders]\n",
    "        receivers_w_t = node_w_t[receivers]\n",
    "        \n",
    "        senders_th_t = node_th_t[senders]\n",
    "        receivers_th_t = node_th_t[receivers]        \n",
    "\n",
    "        node_disp = torch.zeros_like(node_v_t)\n",
    "        node_vf = torch.zeros_like(node_v_t)\n",
    "        node_wf = torch.zeros_like(node_v_t)\n",
    "\n",
    "        sum_node_dv = torch.zeros_like(node_v_t)\n",
    "        sum_node_dx = torch.zeros_like(node_v_t)\n",
    "        \n",
    "        node_latent = self.node_encoder(torch.hstack((node_type,vel[:,1:2])))\n",
    "\n",
    "\n",
    "        for i in range(self.num_messages):\n",
    "            (\n",
    "                senders_v_t_,\n",
    "                senders_v_tm1_,\n",
    "                receivers_v_t_,\n",
    "                receivers_v_tm1_,\n",
    "                edge_dx_,\n",
    "            ) = self.scaler(\n",
    "                senders_v_t,\n",
    "                senders_v_tm1,\n",
    "                receivers_v_t,\n",
    "                receivers_v_tm1,\n",
    "                edge_dx,\n",
    "                self.train_stats,\n",
    "            )\n",
    "\n",
    "\n",
    "            vector_a, vector_b, vector_c = self.refframecalc(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    senders_v_t_,\n",
    "                    receivers_v_t_,\n",
    "                    senders_w_t,\n",
    "                    receivers_w_t\n",
    "                    \n",
    "                )\n",
    "\n",
    "            if i == 0:\n",
    "                node_dv_int_decoded, node_dw_int_decoded,residue= self.interaction_init_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    senders_w_t,\n",
    "                    senders_th_t,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    receivers_w_t,\n",
    "                    receivers_th_t,\n",
    "                    node_latent,\n",
    "                    latent_history=False,\n",
    "                )\n",
    "            else:\n",
    "                node_dv_int_decoded, node_dw_int_decoded, residue = self.interaction_proc_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    senders_w_t,\n",
    "                    senders_th_t,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    receivers_w_t,\n",
    "                    receivers_th_t,\n",
    "                    node_latent,\n",
    "                    residue=residue,\n",
    "                    latent_history=True,\n",
    "                )\n",
    "\n",
    "            sum_node_dv [mask_reflected_node]= sum_node_dv [mask_reflected_node] + node_dv_int_decoded[mask_reflected_node]\n",
    "                \n",
    "            node_vf[mask_reflected_node]= node_v_t[mask_reflected_node] + node_dv_int_decoded[mask_reflected_node]\n",
    "            node_wf[mask_reflected_node]= node_w_t[mask_reflected_node] + node_dw_int_decoded[mask_reflected_node]\n",
    "\n",
    "            node_disp= (\n",
    "                (node_v_t + node_vf) * 0.5 * self.sub_tstep\n",
    "            )\n",
    "\n",
    "            node_th_t = node_th_t + (node_wf + node_w_t)* 0.5 * self.sub_tstep\n",
    "\n",
    "            sum_node_dx [mask_reflected_node]=sum_node_dx [mask_reflected_node]+ ((node_v_t + node_vf) * 0.5 * self.sub_tstep)[mask_reflected_node]\n",
    "\n",
    "            senders_disp = node_disp[senders]\n",
    "            receivers_disp = node_disp[receivers]\n",
    "\n",
    "            senders_pos = senders_disp + senders_pos\n",
    "            receivers_pos = receivers_disp + receivers_pos\n",
    "\n",
    "\n",
    "            node_v_tm1 = node_v_t.clone()\n",
    "\n",
    "            node_v_t = node_vf.clone()\n",
    "            node_w_t = node_wf.clone()\n",
    "\n",
    "            \n",
    "\n",
    "            senders_v_tm1 = senders_v_t.clone()\n",
    "            senders_v_t = node_v_t[senders].clone()\n",
    "            senders_w_t = node_w_t[senders].clone()\n",
    "            senders_th_t = node_th_t[senders].clone()\n",
    "\n",
    "            receivers_v_tm1 = receivers_v_t.clone()\n",
    "            receivers_v_t = node_v_t[receivers].clone()\n",
    "            receivers_w_t = node_w_t[receivers].clone()\n",
    "            receivers_th_t = node_th_t[receivers].clone()\n",
    "\n",
    "            edge_dx = receivers_pos - senders_pos\n",
    "        return sum_node_dv,sum_node_dx, node_v_tm1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde7671",
   "metadata": {},
   "source": [
    "# EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cd859dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model, device, mode='test', plot=False, frequency=1, model_name='PIGNN', experiment_name='Val'):\n",
    "    with torch.no_grad():\n",
    "        res = 0.\n",
    "        res_counter = 0\n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            for i in range(1):  # Iterates 10 times: 0 -> predicts 31, ..., 9 -> predicts 40\n",
    "                if i == 0:\n",
    "                    test_graph = test_batch.to(device)\n",
    "                    graph_t0 = test_graph\n",
    "                    # Clone to ensure ground truth remains unchanged\n",
    "                    end_pos = test_graph.end_pos.clone()  \n",
    "\n",
    "                node_dv,node_dx,_= model(graph_t0.detach())\n",
    "                new_vel =  graph_t0.vel + node_dv\n",
    "                new_pos =  graph_t0.pos + node_dx\n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = graph_t0.vel.clone()\n",
    "                graph_t0.pos = new_pos.clone()\n",
    "                graph_t0.vel = new_vel.clone()\n",
    "\n",
    "            loss = F.mse_loss(new_pos, end_pos)\n",
    "            batch_size = test_graph.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "\n",
    "        mean_pos_error = res / res_counter\n",
    "    return mean_pos_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc02010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device, train_stats):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.train_history = []\n",
    "        self.extr_test_history = []\n",
    "        self.gen_test_history = []\n",
    "        self.cur_dir = os.getcwd()\n",
    "        self.model_dir = os.path.join(self.cur_dir, 'saved_models')\n",
    "        self.gen_loss = 0.\n",
    "        self.train_stats = train_stats\n",
    "        self.mean_node_dv = self.train_stats[2][0]\n",
    "        self.mean_node_disp = self.train_stats[3][0]\n",
    "        self.std_node_dv = self.train_stats[2][1]\n",
    "        self.std_node_disp = self.train_stats[3][1]\n",
    "        # Initialize best test loss and best epoch.\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = None        \n",
    "\n",
    "    def train(self, train_graph):\n",
    "        self.model.train()\n",
    "        pred_node_dvel,pred_node_disp,_= self.model(train_graph.to(self.device))\n",
    "\n",
    "        actual_node_dvel = (train_graph.y_dv).float().to(self.device)\n",
    "        actual_node_disp = (train_graph.y_dx).float().to(self.device)\n",
    "\n",
    "        pred_node_dvel_ = (pred_node_dvel-self.mean_node_dv.detach())/self.std_node_dv.detach()\n",
    "        pred_node_disp_ = (pred_node_disp-self.mean_node_disp.detach())/self.std_node_disp.detach()\n",
    "        actual_node_dvel_ = (actual_node_dvel - self.mean_node_dv.detach())/self.std_node_dv.detach()\n",
    "        actual_node_disp_ = (actual_node_disp-self.mean_node_disp.detach())/self.std_node_disp.detach()\n",
    "\n",
    "        loss = F.mse_loss(pred_node_disp_,actual_node_disp_) #+ F.mse_loss(pred_node_dvel_,actual_node_dvel_)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.loss = loss.cpu().detach().numpy()\n",
    "        self.train_history.append(self.loss)\n",
    "    def test(self, test_loader, mode='val', epoch=None):\n",
    "        self.model.eval()\n",
    "        mean_pos_error = evaluate(test_loader,\n",
    "                            self.model,\n",
    "                            self.device)\n",
    "        \n",
    "        \n",
    "        if mode == 'val':\n",
    "            self.val_loss_pos = mean_pos_error\n",
    "            self.gen_test_history.append(self.val_loss_pos)\n",
    "            # Update best test loss and best epoch if the current loss is lower\n",
    "            if epoch is not None and self.val_loss_pos < self.best_val_loss:\n",
    "                self.best_val_loss = self.val_loss_pos\n",
    "                self.best_epoch = epoch\n",
    "                self.save_model(epoch)\n",
    "        if mode == 'test':\n",
    "            self.test_loss_pos = mean_pos_error\n",
    "\n",
    "    def save_model(self,iteration):\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        self.path = os.path.join(self.model_dir, \n",
    "                                 f'GenLoss_{self.val_loss_pos:.5f}mm_iter{iteration}.pth')\n",
    "        torch.save(self.model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3b6dbe9-823b-457f-b6b1-4653ebfe9036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE_TIME_STEP =  30.0\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if model_settings[\"finite_diff\"]:\n",
    "    SAMPLE_TIME_STEP = 30*model_settings[\"time_step\"]\n",
    "else:\n",
    "    SAMPLE_TIME_STEP = 30*model_settings[\"time_step_actual\"]\n",
    "print(f'SAMPLE_TIME_STEP = ',SAMPLE_TIME_STEP)\n",
    "N= 1\n",
    "MODEL = DynamicsSolver(SAMPLE_TIME_STEP, train_stats, num_jumps=N, num_msgs=4, latent_size=64)\n",
    "optimizer = torch.optim.Adam(MODEL.parameters(), lr = 5e-4)\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def ensure_model_on_device(model, device):\n",
    "    for module in model.modules():\n",
    "        # Check if any parameter of the module is not on the desired device\n",
    "        if any(p.device != device for p in module.parameters()):\n",
    "            # Move the entire module to the device\n",
    "            module.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensure_model_on_device(MODEL, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9070b11-5732-452a-a2bc-5cd6b4b95c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(MODEL, \n",
    "                  optimizer, \n",
    "                  device,\n",
    "                  train_stats\n",
    "                 )\n",
    "# Clear PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2762c-67d1-4e3c-aca2-7f84a38ff010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/650 [00:00<?, ?it/s]/var/folders/rw/1cgknnn12pl2pjhzv_hz7_zn3h5jy_/T/ipykernel_93208/1090409675.py:113: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1746257065223/work/aten/src/ATen/native/Cross.cpp:66.)\n",
      "  tauij = tij-torch.cross(receivers_x_t[receivers]-r0ij,fij)*self.f_scaler(interaction_latent)\n",
      "Training:   0%|          | 0/650 [00:01<?, ?it/s, TrainLoss (*1e3)=1.31738e+03, ValLoss=9.82505586e+00, BestVal=9.82506e+00(Ep1), TestLoss=9.49012403e+00, LR=5.00e-04]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wall_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# ——— Update the progress bar ———\u001b[39;00m\n\u001b[32m     28\u001b[39m pbar.set_postfix({\n\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mTrainLoss (*1e3)\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.loss\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m1e3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m     30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mValLoss\u001b[39m\u001b[33m'\u001b[39m:           \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.8e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLR\u001b[39m\u001b[33m'\u001b[39m:                \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_lr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m     34\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: wall time = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mwall_time\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s | cpu time = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m s\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'wall_time' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 650\n",
    "\n",
    "# initialize to large values\n",
    "val_loss = np.inf\n",
    "test_loss = np.inf\n",
    "\n",
    "# wrap the epoch loop in tqdm\n",
    "with tqdm(range(1, epochs+1), desc='Training') as pbar:\n",
    "    for epoch in pbar:\n",
    "        # ——— Training ——— \n",
    "        for train_batch in train_loader:\n",
    "            trainer.train(train_batch)\n",
    "\n",
    "        # ——— Validation & Test ———\n",
    "        # trainer.test now returns the mean positional error\n",
    "        if epoch%1==0:\n",
    "            trainer.test(val_loader,  mode='val',  epoch=epoch)\n",
    "            val_loss = trainer.val_loss_pos\n",
    "            trainer.test(test_loader, mode='test')\n",
    "            test_loss = trainer.test_loss_pos\n",
    "\n",
    "        # ——— Learning rate ———\n",
    "        current_lr = trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # ——— Update the progress bar ———\n",
    "        pbar.set_postfix({\n",
    "            'TrainLoss (*1e3)': f'{trainer.loss * 1e3:.5e}',\n",
    "            'ValLoss':           f'{val_loss:.8e}',\n",
    "            'BestVal':           f'{trainer.best_val_loss:.5e}(Ep{trainer.best_epoch})',\n",
    "            'TestLoss':          f'{test_loss:.8e}',\n",
    "            'LR':                f'{current_lr:.2e}',\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1330dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e0ef6",
   "metadata": {},
   "source": [
    "## Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f200b9b",
   "metadata": {},
   "source": [
    "This next cell has the functions that will, given a frame of the human, use the model to predict eventual position of the human after 30 time steps. It then also gives some sample animations of the predictions alongside the ground truth of the final position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1529bd0",
   "metadata": {},
   "source": [
    "### Main loop that runs the test rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f32a7e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best file: GenLoss_0.40743mm_iter79.pth\n",
      "  ↳ parsed loss:  0.40743\n",
      "  ↳ parsed epoch: 79\n",
      "Model loaded on CPU and set to eval().\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Regular expression pattern to extract loss and epoch\n",
    "pattern = r'GenLoss_(\\d+(?:\\.\\d+)?)mm_iter(\\d+)\\.pth'\n",
    "\n",
    "best_loss  = float('inf')\n",
    "best_epoch = None\n",
    "best_file  = None\n",
    "\n",
    "for fn in os.listdir(trainer.model_dir):\n",
    "    m = re.search(pattern, fn)\n",
    "    if m:\n",
    "        loss_val = float(m.group(1))\n",
    "        if loss_val < best_loss:\n",
    "            best_loss  = loss_val\n",
    "            best_epoch = int(m.group(2))\n",
    "            best_file  = fn\n",
    "\n",
    "if best_file is None:\n",
    "    raise RuntimeError(\"No checkpoint found matching pattern: GenLoss_<loss>mm_iter<epoch>.pth\")\n",
    "\n",
    "path = os.path.join(trainer.model_dir, best_file)\n",
    "\n",
    "# ----- load weights on CPU -----\n",
    "ckpt = torch.load(path, map_location=\"cpu\")\n",
    "\n",
    "# Unwrap common checkpoint containers\n",
    "if isinstance(ckpt, dict):\n",
    "    if \"state_dict\" in ckpt:\n",
    "        state = ckpt[\"state_dict\"]\n",
    "    elif \"model_state_dict\" in ckpt:\n",
    "        state = ckpt[\"model_state_dict\"]\n",
    "    elif \"model\" in ckpt and isinstance(ckpt[\"model\"], dict):\n",
    "        state = ckpt[\"model\"]\n",
    "    else:\n",
    "        # assume it's already a plain state_dict\n",
    "        state = ckpt\n",
    "else:\n",
    "    state = ckpt  # fallback\n",
    "\n",
    "# Strip \"module.\" prefix if saved from DataParallel\n",
    "if any(k.startswith(\"module.\") for k in state.keys()):\n",
    "    state = {k.replace(\"module.\", \"\", 1): v for k, v in state.items()}\n",
    "\n",
    "# Load into model (on CPU) and set eval mode\n",
    "trainer.model.load_state_dict(state, strict=True)\n",
    "trainer.model.to(\"cpu\").eval()\n",
    "\n",
    "print(f\"Best file: {best_file}\")\n",
    "print(f\"  ↳ parsed loss:  {best_loss}\")\n",
    "print(f\"  ↳ parsed epoch: {best_epoch}\")\n",
    "print(\"Model loaded on CPU and set to eval().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab4f95-c407-4430-88c5-0ff13b1285e6",
   "metadata": {},
   "source": [
    "# EVALUATION RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de2f23f8-8495-40d6-8166-ccd767407a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rollout(test_loader, model, device, nsteps=1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = 0.\n",
    "        res_counter = 0\n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            for i in range(nsteps):  # Iterates 10 times: 0 -> predicts 31, ..., 9 -> predicts 40\n",
    "                if i == 0:\n",
    "                    test_graph = test_batch.to(device)\n",
    "                    graph_t0 = test_graph\n",
    "                    # Clone to ensure ground truth remains unchanged\n",
    "                    end_pos = test_graph.end_pos.clone()  \n",
    "\n",
    "                node_dv,node_dx,vtm1= model(graph_t0.detach())\n",
    "                new_vel =  graph_t0.vel + node_dv\n",
    "                new_pos =  graph_t0.pos + node_dx\n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = graph_t0.vel.clone()\n",
    "                graph_t0.pos = new_pos.clone()\n",
    "                graph_t0.vel = new_vel.clone()\n",
    "\n",
    "            loss = F.mse_loss(new_pos, end_pos)\n",
    "            batch_size = test_graph.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "\n",
    "        mean_pos_error = res / res_counter\n",
    "    return mean_pos_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6af236bb-e575-4a07-9666-846ee1f07d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "===== Rollout Evaluation Timing =====\n",
      "Device:               cpu\n",
      "Batches processed:    1\n",
      "nsteps per batch:     1\n",
      "Total rollout steps:  1\n",
      "Total wall time:      0.004 s\n",
      "Total CPU time:       0.004 s  (CPU/Wall = 1.000)\n",
      "Latency/step:         3.805 ms\n",
      "Throughput:           262.84 steps/s\n",
      "Mean Position Error:  nan\n",
      "=====================================\n",
      "\n",
      "DGN Summary\n",
      "===========\n",
      "Total parameters:     108,050\n",
      "Trainable parameters: 108,050\n",
      "Non-trainable:        0\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "  Total nn.Modules (including container modules): 100\n",
      "  Immediate child modules:                        5\n",
      "  Parameter tensors:                              90\n",
      "\n",
      "loss for rollout 1 steps nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for nstep in [1]:\n",
    "    \n",
    "    dataset_eval = HumanDataset(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"],nsteps=nstep)\n",
    "    \n",
    "    dataloader_eval = create_dataloaders_from_raw(dataset_eval,1,shuffle=False)\n",
    "    \n",
    "    eval_error = evaluate_rollout(dataloader_eval, trainer.model, device, nsteps=nstep)\n",
    "    \n",
    "    print(f'loss for rollout {nstep} steps {eval_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870fd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c725db9-f613-4100-ae66-0707e5b44a46",
   "metadata": {},
   "source": [
    "# VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47a5eb-065c-4e2b-9476-09a227f6ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDatasetSeq(torch.utils.data.Dataset):\n",
    "    def __init__(self, partition='train', max_samples=600, data_dir='', nsteps=1):\n",
    "        self.partition = partition\n",
    "        self.data_dir = data_dir\n",
    "        self.nsteps = nsteps\n",
    "\n",
    "        # --- load raw data --------------------------------------\n",
    "        with open(os.path.join(data_dir, 'motion.pkl'), 'rb') as f:\n",
    "            edges, X = pkl.load(f)\n",
    "\n",
    "        # your smoothing / central_diff code here...\n",
    "        Ps, Vs, As = self.central_diff(X)\n",
    "\n",
    "        # trial IDs must match exactly\n",
    "        train_case_id = [20,1,17,13,14,9,4,2,7,5,16]\n",
    "        val_case_id   = [3,8,11,12,15,18]\n",
    "        test_case_id  = [6,19,21,0,22,10]\n",
    "\n",
    "        # --- load or create competitor splits (fixed for central_diff) ----------\n",
    "        split_path = os.path.join(data_dir, f'split_n{self.nsteps}.pkl')\n",
    "        try:\n",
    "            with open(split_path, 'rb') as f:\n",
    "                train_mapping, val_mapping, test_mapping = pkl.load(f)\n",
    "                print(\"Loaded competitor split!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Generating competitor split…\")\n",
    "\n",
    "            def make_map(case_ids):\n",
    "                mapping = {}\n",
    "                for i in case_ids:\n",
    "                    core_len = Ps[i].shape[0]                    # <<— use length after central_diff\n",
    "                    safe_max = core_len - self.nsteps*30 - 1\n",
    "                    if safe_max < 0:\n",
    "                        raise ValueError(f\"Trial {i} too short for look-ahead of {self.nsteps} steps.\")\n",
    "                    # competitor caps at 300\n",
    "                    itv = min(300, safe_max + 1)                # +1 because j in [0..safe_max]\n",
    "                    pool = np.arange(itv)                       # j ∈ [0..itv-1]\n",
    "                    mapping[i] = np.random.choice(pool, size=100, replace=False)\n",
    "                return mapping\n",
    "\n",
    "            train_mapping = make_map(train_case_id)\n",
    "            val_mapping   = make_map(val_case_id)\n",
    "            test_mapping  = make_map(test_case_id)\n",
    "\n",
    "            with open(split_path, 'wb') as f:\n",
    "                pkl.dump((train_mapping, val_mapping, test_mapping), f)\n",
    "            print(\"Saved competitor split!\")\n",
    "\n",
    "        # pick the mapping you need\n",
    "        if   partition == 'train': mapping = train_mapping\n",
    "        elif partition == 'val'  : mapping = val_mapping\n",
    "        elif partition == 'test' : mapping = test_mapping\n",
    "        else: raise ValueError(f\"Unknown partition {partition!r}\")\n",
    "\n",
    "        # now proceed exactly as before, using `mapping` instead of your make_mapping\n",
    "        each_len = max_samples // len(mapping)\n",
    "        in_graphs = []\n",
    "        for i, pool in mapping.items():\n",
    "            for j in pool[:each_len]:\n",
    "                # note: they use delta_frame; you have nsteps*30, so this is identical\n",
    "                cur_x_t   = Ps[i][j]\n",
    "                cur_v_t   = Vs[i][j]\n",
    "                cur_v_tm1 = Vs[i][j-1]\n",
    "                y_dv      = Vs[i][j + self.nsteps*30] - Vs[i][j]\n",
    "                y_dx      = Ps[i][j + self.nsteps*30] - Ps[i][j]\n",
    "                gt_seq = [ Ps[i][j + k*30] for k in range(self.nsteps+1) ]   # list of (31,3) arrays\n",
    "                y_pos_end = Ps[i][j + self.nsteps*30]\n",
    "                y_vel_end = Vs[i][j + self.nsteps*30]\n",
    "\n",
    "                in_graphs.append(self.create_in_graph(\n",
    "                    edges,\n",
    "                    x=(cur_x_t, cur_v_t, cur_v_tm1),\n",
    "                    y=(y_dv, y_dx, y_pos_end, y_vel_end),\n",
    "                    gt_seq = gt_seq\n",
    "                ))\n",
    "\n",
    "        self.in_graphs = in_graphs\n",
    "        print(f\"[HumanDataset:{partition}] built {len(in_graphs)} samples\")\n",
    "\n",
    "    def central_diff(self, Xs, dt: float = 1.0, window_length: int = 41):\n",
    "        Ps, Vs, As = [], [], []\n",
    "        for x in Xs:\n",
    "            v      = (x[2:] - x[:-2]) / (2*dt)\n",
    "            a      = (x[2:] - 2*x[1:-1] + x[:-2]) / (dt**2)\n",
    "            p      = x[1:-1]                      # align to v,a\n",
    "            Ps.append(p)\n",
    "            Vs.append(v)\n",
    "            As.append(a)\n",
    "        return Ps, Vs, As\n",
    "\n",
    "        \n",
    "    def get_foot_nodes(self, nodes):\n",
    "        foot_indices = np.argsort(nodes[:,1])[:6]\n",
    "        foot_pos = nodes[foot_indices]\n",
    "        return foot_pos, foot_indices\n",
    "    \n",
    "    def reflected_nodes(self, nodes, z0=0, epsilon=1e-3):\n",
    "        reflected = nodes.copy()\n",
    "        reflected[:,1] = 2*z0 - nodes[:,1] - epsilon\n",
    "        distances = reflected[:,1] - nodes[:,1]\n",
    "        return reflected, distances\n",
    "    \n",
    "    def find_min(self, nodes):\n",
    "        return np.min(nodes, axis=0)\n",
    "    \n",
    "\n",
    "    def create_edges(self, N, edges):\n",
    "        atom_edges = torch.zeros(N, N).int()\n",
    "        for edge in edges:\n",
    "            atom_edges[edge[0], edge[1]] = 1\n",
    "            atom_edges[edge[1], edge[0]] = 1\n",
    "\n",
    "        atom_edges2 = atom_edges @ atom_edges\n",
    "        self.atom_edge = atom_edges\n",
    "        self.atom_edge2 = atom_edges2\n",
    "        edge_attr = []\n",
    "        # Initialize edges and edge_attributes\n",
    "        rows, cols = [], []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    if atom_edges[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([1])\n",
    "                        assert not atom_edges2[i][j]\n",
    "                    if atom_edges2[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([2])\n",
    "                        assert not atom_edges[i][j]\n",
    "\n",
    "        edges = [rows, cols] \n",
    "        edge_attr = torch.Tensor(np.array(edge_attr))  # [edge, 3]\n",
    "        edge_idx =torch.tensor(edges, dtype=torch.long)  # [2, M]   \n",
    "        return edge_idx,edge_attr     \n",
    "    \n",
    "    \n",
    "    def create_in_graph(self, edges,x,y,gt_seq):\n",
    "        pos_t, vel_t, vel_tm1 = x\n",
    "        y_dv,y_dx,y_pos_end,y_vel_end = y\n",
    "\n",
    "        edge_idx,edge_attr = self.create_edges(pos_t.shape[0], edges)\n",
    "\n",
    "        # # Get the ground node\n",
    "        # z0_t = self.find_min(pos_t)[1]\n",
    "        # z0_end = self.find_min(y_end)[1]\n",
    "        # # Center the y-positions around z0 for input and target\n",
    "        # pos_t -= np.array([0, z0_t, 0]) \n",
    "        # y_end -= np.array([0, z0_end, 0])\n",
    "\n",
    "        # Get the foot node positions and indices\n",
    "        # foot_nodes_positions, foot_nodes_indices = self.get_foot_nodes(pos_t)\n",
    "        # foot_nodes_reflected, foot_distances = self.reflected_nodes(foot_nodes_positions,z0=0.0)\n",
    "        \n",
    "        # current_largest_node_index = pos_t.shape[0]\n",
    "        # reflected_nodes_indices = []\n",
    "        # for reflected_node in range(foot_nodes_indices.shape[0]):\n",
    "        #     reflected_node_index = current_largest_node_index\n",
    "        #     current_largest_node_index += 1\n",
    "        #     reflected_nodes_indices.append(reflected_node_index)\n",
    "        \n",
    "        \n",
    "        # # Set lists to torch tensors\n",
    "        # reflected_nodes_indices = torch.tensor(reflected_nodes_indices)\n",
    "        # foot_nodes_indices = torch.tensor(foot_nodes_indices)\n",
    "        pos_t = torch.tensor(pos_t)\n",
    "        vel_t = torch.tensor(vel_t)\n",
    "        vel_tm1 = torch.tensor(vel_tm1)\n",
    "\n",
    "        y_dv = torch.tensor(y_dv)\n",
    "        y_dx = torch.tensor(y_dx)\n",
    "        y_pos_end = torch.tensor(y_pos_end)\n",
    "        y_vel_end = torch.tensor(y_vel_end)\n",
    "        \n",
    "        \n",
    "        # foot_nodes_reflected = torch.tensor(foot_nodes_reflected)\n",
    "        \n",
    "        # Set the node type of feet to one\n",
    "        node_type = torch.ones(pos_t.shape[0],1)\n",
    "        # node_type[foot_nodes_indices] = 1\n",
    "        # # Make reflected nodes of type 2\n",
    "        # new_node_type = torch.vstack((node_type,2*torch.ones_like(reflected_nodes_indices).unsqueeze(1))) \n",
    "        \n",
    "        # New bi-dir edge indexes\n",
    "        # new_edges_ref = torch.hstack((foot_nodes_indices.unsqueeze(1), reflected_nodes_indices.unsqueeze(1))) # connect foot edges to their reflections\n",
    "        # new_edges_ref = new_edges_ref.t()  # now [2, M]\n",
    "        # rev_new_edges_ref = new_edges_ref.flip(0)  # reverse the order to match edge index format\n",
    "        # new_edges_bidir_ref = torch.cat((new_edges_ref, rev_new_edges_ref), dim=1)  # add reverse edges\n",
    "        # new_edge_index = torch.cat([edge_idx, new_edges_bidir_ref], dim=1) # add new edges to the graph edge index\n",
    "        # s,r = new_edge_index\n",
    "\n",
    "        # we add the 1 as edge attr for these edges as they are 1 hop\n",
    "        # new_edge_attr = torch.vstack((edge_attr, torch.ones((new_edges_bidir_ref.shape[1], 1))))  # add new edge attributes\n",
    "        # for differentiating reflected edges we use another features i.e. type_sender*type_receiver\n",
    "        # new_edge_attr = torch.hstack((new_edge_attr,\n",
    "        #                               new_node_type[s]*new_node_type[r]))\n",
    "        # new_pos_t = torch.vstack((pos_t, foot_nodes_reflected))\n",
    "        # new_vel_t = torch.vstack((vel_t,torch.zeros_like(foot_nodes_reflected)))\n",
    "        # new_vel_tm1 = torch.vstack((vel_tm1,torch.zeros_like(foot_nodes_reflected)))\n",
    "\n",
    "        \n",
    "        # in_graph = Data(x=new_pos_t,edge_index=new_edge_index,edge_attr=new_edge_attr)\n",
    "        # in_graph.node_vel_t = new_vel_t\n",
    "        # in_graph.node_vel_tm1 = new_vel_tm1\n",
    "        # in_graph.y_dv = y_dv\n",
    "        # in_graph.y_dx = y_dx\n",
    "        # in_graph.y_end = y_end\n",
    "        # in_graph.node_type = new_node_type\n",
    "\n",
    "        in_graph = Data(edge_index=edge_idx, edge_attr=edge_attr)\n",
    "        in_graph.pos = pos_t\n",
    "        in_graph.vel = vel_t\n",
    "        in_graph.prev_vel = vel_tm1\n",
    "        in_graph.y_dv = y_dv\n",
    "        in_graph.y_dx = y_dx\n",
    "        in_graph.end_pos = y_pos_end\n",
    "        in_graph.end_vel = y_vel_end\n",
    "        in_graph.node_type = node_type\n",
    "        in_graph.gt_seq = gt_seq\n",
    "        \n",
    "        return in_graph     \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.in_graphs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.in_graphs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aaccb3-2c30-44b3-92d3-6cfbd52c2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def visualize_multi_step(\n",
    "    test_loader,\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    steps=(1,2,3,4),\n",
    "    num_graphs=10,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    For each of `num_graphs` random graphs from test_loader:\n",
    "      • Plot initial_vs_gt.png once (vs GT at step=1)\n",
    "      • Then do a single graph rollout, saving pred_vs_gt_step{n}.png\n",
    "        for each n in `steps`, comparing to GT at that same step.\n",
    "    Uses absolute coordinates (no centering).\n",
    "    Assumes each Data has `gt_seq` as a list of length T+1,\n",
    "    each element shape (n_nodes,3).\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    model.eval()\n",
    "\n",
    "    # flatten loader\n",
    "    all_graphs = []\n",
    "    for batch in test_loader:\n",
    "        all_graphs.extend(batch.to_data_list())\n",
    "    if not all_graphs:\n",
    "        raise RuntimeError(\"No graphs in loader\")\n",
    "\n",
    "    # sample indices\n",
    "    chosen = random.sample(range(len(all_graphs)), min(num_graphs, len(all_graphs)))\n",
    "\n",
    "    # skeleton edges for the 31-body joints\n",
    "    skeleton31 = [\n",
    "      [1,0],[2,1],[3,2],[4,3],[5,4],\n",
    "      [6,0],[7,6],[8,7],[9,8],[10,9],\n",
    "      [11,0],[12,11],[13,12],[14,13],[15,14],\n",
    "      [16,15],[17,13],[18,17],[19,18],[20,19],\n",
    "      [21,20],[22,21],[23,20],[24,13],[25,24],\n",
    "      [26,25],[27,26],[28,27],[29,28],[30,27]\n",
    "    ]\n",
    "\n",
    "    for idx in chosen:\n",
    "        data = all_graphs[idx].to(device)\n",
    "        base = f\"./RESULTS/TrajectoryPlotsRefNodes/graph_{idx}\"\n",
    "        os.makedirs(base, exist_ok=True)\n",
    "\n",
    "        # — stack gt_seq list → np array [T+1,31,3] —\n",
    "        seq_list = data.gt_seq\n",
    "        seq_np = []\n",
    "        for arr in seq_list:\n",
    "            if torch.is_tensor(arr):\n",
    "                seq_np.append(arr.cpu().numpy())\n",
    "            else:\n",
    "                seq_np.append(np.array(arr))\n",
    "        gt_seq = np.stack(seq_np, axis=0)[:, :31, :]  # shape [T+1,31,3]\n",
    "\n",
    "        # initial pose (absolute)\n",
    "        init31 = data.pos[:31].cpu().numpy()\n",
    "\n",
    "        # compute axis limits from initial + all selected GT steps\n",
    "        all_pts = np.vstack([init31] + [gt_seq[k] for k in steps])\n",
    "        pad = 2.0\n",
    "        x_min, x_max = all_pts[:,2].min() - pad, all_pts[:,2].max() + pad\n",
    "        y_min, y_max = all_pts[:,0].min() - pad, all_pts[:,0].max() + pad\n",
    "        z_min, z_max = all_pts[:,1].min() - pad, all_pts[:,1].max() + pad\n",
    "\n",
    "        # — Plot initial_vs_gt.png (vs GT at step=1) —\n",
    "        gt1 = gt_seq[0]\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax  = fig.add_subplot(111, projection='3d')\n",
    "        xx, yy = np.meshgrid([x_min,x_max], [y_min,y_max])\n",
    "        ax.plot_surface(xx, yy, np.zeros_like(xx), color='gray', alpha=0.2, linewidth=0)\n",
    "        ax.scatter(init31[:,2], init31[:,0], init31[:,1],\n",
    "                   c='red', s=30, edgecolors='k', alpha=0.5, label='Initial')\n",
    "        for a,b in skeleton31:\n",
    "            ax.plot([gt1[a,2], gt1[b,2]],\n",
    "                    [gt1[a,0], gt1[b,0]],\n",
    "                    [gt1[a,1], gt1[b,1]],\n",
    "                    c='red', alpha=0.6, linestyle='-', linewidth=2)\n",
    "\n",
    "        mid_x = (x_min + x_max)/2\n",
    "        mid_y = (y_min + y_max)/2\n",
    "        mid_z = (z_min + z_max)/2\n",
    "        \n",
    "        ax.set_xlim(mid_x-20, mid_x+20)\n",
    "        ax.set_ylim(mid_y-20, mid_y+20)\n",
    "        ax.set_zlim(mid_z-20, mid_z+20)\n",
    "        ax.set_box_aspect((1,1,1))\n",
    "        ax.set_xlabel(\"X\",fontsize = 16); ax.set_ylabel(\"Y\",fontsize = 16); ax.set_zlabel(\"Z\",fontsize = 16)\n",
    "        ax.set_title(\"Initial\",fontsize = 18)\n",
    "        ax.legend(loc='upper left',fontsize = 18)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base, 'initial_vs_gt.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # — rollout & per-step plotting —\n",
    "        graph = data.clone().to(device)\n",
    "        for step in steps:\n",
    "            dv, dx,prev_vel = model(graph.detach())\n",
    "            graph.prev_vel = prev_vel\n",
    "            graph.vel      = graph.vel + dv\n",
    "            graph.pos      = graph.pos + dx\n",
    "\n",
    "            pred31 = graph.pos[:31].detach().cpu().numpy()\n",
    "            gt_k   = gt_seq[step]\n",
    "\n",
    "            fig = plt.figure(figsize=(6,6))\n",
    "            ax  = fig.add_subplot(111, projection='3d')\n",
    "            # optional ground plane: uncomment if desired\n",
    "            ax.plot_surface(xx, yy, np.zeros_like(xx), color='gray', alpha=0.2, linewidth=0)\n",
    "\n",
    "            ax.scatter(pred31[:,2], pred31[:,0], pred31[:,1],\n",
    "                       c='blue', s=30, edgecolors='k', alpha=0.5,\n",
    "                       label=f'Pred (step={step})')\n",
    "            ax.scatter(gt_k[:,2], gt_k[:,0], gt_k[:,1],\n",
    "                       c='red',  s=30, edgecolors='k', alpha=0.5,\n",
    "                       label=f'GT (step={step})')\n",
    "            for a,b in skeleton31:\n",
    "                ax.plot([pred31[a,2], pred31[b,2]],\n",
    "                        [pred31[a,0], pred31[b,0]],\n",
    "                        [pred31[a,1], pred31[b,1]],\n",
    "                        c='blue', alpha=0.6, linewidth=2)\n",
    "                ax.plot([gt_k[a,2], gt_k[b,2]],\n",
    "                        [gt_k[a,0], gt_k[b,0]],\n",
    "                        [gt_k[a,1], gt_k[b,1]],\n",
    "                        c='red', alpha=0.6, linestyle='-',linewidth=2)\n",
    "\n",
    "            ax.set_xlim(mid_x-20, mid_x+20)\n",
    "            ax.set_ylim(mid_y-20, mid_y+20)\n",
    "            ax.set_zlim(mid_z-20, mid_z+20)\n",
    "            ax.set_box_aspect((1,1,1))\n",
    "            ax.set_xlabel(\"X\",fontsize = 16); ax.set_ylabel(\"Y\",fontsize = 16); ax.set_zlabel(\"Z\",fontsize = 16)\n",
    "            ax.set_title(f\"Prediction vs GT — {step} steps\",fontsize = 18)\n",
    "            ax.legend(loc='upper left',fontsize = 18)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(base, f'pred_vs_gt_step{step}.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "        # optional: compute final MSE in absolute frame\n",
    "        mask_cuda = (data.node_type[:31] != 2).squeeze()\n",
    "        mask_cpu  = mask_cuda.cpu().numpy()\n",
    "        final_pred = graph.pos[:31][mask_cuda]\n",
    "        final_gt_np = gt_seq[steps[-1]][mask_cpu]\n",
    "        final_gt    = torch.from_numpy(final_gt_np).to(device)\n",
    "        mse = F.mse_loss(final_pred, final_gt).item()\n",
    "        print(f\"Graph {idx}, final step={steps[-1]}, MSE={mse:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81124734-e9b8-48a9-8cc9-9d9adf182d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_eval = HumanDatasetSeq(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"],nsteps=4)\n",
    "\n",
    "loader = create_dataloaders_from_raw(dataset_eval,200,shuffle=False)\n",
    "\n",
    "visualize_multi_step(\n",
    "    loader,\n",
    "    trainer.model,\n",
    "    device,\n",
    "    steps=[1,2,3,4],\n",
    "    num_graphs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933f41f-06b5-4d90-8c4e-dee0613c00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "\n",
    "# Base directory containing graph folders\n",
    "base_dir = './RESULTS/TrajectoryPlotsRefNodes'\n",
    "\n",
    "# Iterate through each subfolder (graph_*)\n",
    "for graph_folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, graph_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    # Collect all PNG files in sorted order\n",
    "    png_files = sorted(glob.glob(os.path.join(folder_path, '*.png')))\n",
    "    if not png_files:\n",
    "        continue\n",
    "    \n",
    "    # Read each image\n",
    "    images = []\n",
    "    for png in png_files:\n",
    "        try:\n",
    "            img = imageio.imread(png)\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not read {png}: {e}\")\n",
    "    \n",
    "    # Save as infinite-loop GIF\n",
    "    gif_path = os.path.join(folder_path, 'rollout.gif')\n",
    "    imageio.mimsave(gif_path, images, duration=0.7, loop=0)\n",
    "    print(f\"Created {gif_path} with {len(images)} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18200e-7984-4132-b233-ee9c893d2615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e08202-4ea4-4a11-83da-6338bf9a3611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_control_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
