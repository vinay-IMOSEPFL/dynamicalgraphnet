{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c9b56",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for the running of DYNAMICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7edd8365-2fe3-4354-b373-38befe37df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# # -----------------------------\n",
    "# # 1. Limit to a single CPU thread\n",
    "# # -----------------------------\n",
    "# os.environ[\"OMP_NUM_THREADS\"]        = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"]        = \"1\"\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"]    = \"1\"\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"]   = \"1\"\n",
    "# os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "\n",
    "# # -----------------------------\n",
    "# # 2. Fix GPU selection (if needed)\n",
    "# # -----------------------------\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # or \"0\" if you want the first GPU\n",
    "\n",
    "# # -----------------------------\n",
    "# # 3. Fix all random seeds *before* importing torch\n",
    "# # -----------------------------\n",
    "# seed = 90\n",
    "# os.environ[\"PYTHONHASHSEED\"]          = str(seed)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# def set_seed(seed=42):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     torch.use_deterministic_algorithms(True)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set_seed(seed)\n",
    "\n",
    "# # -----------------------------\n",
    "# # 4. Confirm settings\n",
    "# # -----------------------------\n",
    "# import torch\n",
    "# print(\"CPU threads:\", torch.get_num_threads())\n",
    "# print(\"CUDA visible devices:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"None\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75740aa6",
   "metadata": {},
   "source": [
    "All of the code that is present in this notebook are present in the other python files of this Dynamical folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ee16f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62c0afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import pickle as pkl\n",
    "import random\n",
    "import shutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4be8b",
   "metadata": {},
   "source": [
    "### Dictionary that sets the model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b91ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings ={\n",
    "        \"batch_size\": 100,\n",
    "        \"epochs\": 600,\n",
    "        \"lr\": 5e-4,\n",
    "        \"nf\": 64,\n",
    "        \"model\": \"gmn\",\n",
    "        \"attention\": 0,\n",
    "        \"n_layers\": 4,\n",
    "        \"max_testing_samples\": 600,\n",
    "        \"max_training_samples\": 200,\n",
    "        \"data_dir\": \"/Users/visharma/Desktop/Repositories/Dynamical_codebase/HUMAN_WALK/data\",\n",
    "        \"norm_diff\": False,\n",
    "        \"weight_decay\": 1e-10,\n",
    "        \"tanh\": False,\n",
    "        \"learnable\": False,\n",
    "        \"finite_diff\":True,\n",
    "        \"time_step\":1.0,\n",
    "        \"end_time_step\": 30.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c81ab9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76d86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, partition='train', max_samples=600, data_dir='', nsteps=1):\n",
    "        self.partition = partition\n",
    "        self.data_dir = data_dir\n",
    "        self.nsteps = nsteps\n",
    "\n",
    "        # --- load raw data --------------------------------------\n",
    "        with open(os.path.join(data_dir, 'motion.pkl'), 'rb') as f:\n",
    "            edges, X = pkl.load(f)\n",
    "\n",
    "        # your smoothing / central_diff code here...\n",
    "        Ps, Vs, As = self.central_diff(X)\n",
    "\n",
    "        # trial IDs must match exactly\n",
    "        train_case_id = [20,1,17,13,14,9,4,2,7,5,16]\n",
    "        val_case_id   = [3,8,11,12,15,18]\n",
    "        test_case_id  = [6,19,21,0,22,10]\n",
    "\n",
    "        # --- load or create competitor splits (fixed for central_diff) ----------\n",
    "        split_path = os.path.join(data_dir, f'split_n{self.nsteps}.pkl')\n",
    "        try:\n",
    "            with open(split_path, 'rb') as f:\n",
    "                train_mapping, val_mapping, test_mapping = pkl.load(f)\n",
    "                print(\"Loaded competitor split!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Generating competitor split…\")\n",
    "\n",
    "            def make_map(case_ids):\n",
    "                mapping = {}\n",
    "                for i in case_ids:\n",
    "                    core_len = Ps[i].shape[0]                    # <<— use length after central_diff\n",
    "                    safe_max = core_len - self.nsteps*30 - 1\n",
    "                    if safe_max < 0:\n",
    "                        raise ValueError(f\"Trial {i} too short for look-ahead of {self.nsteps} steps.\")\n",
    "                    # competitor caps at 300\n",
    "                    itv = min(300, safe_max + 1)                # +1 because j in [0..safe_max]\n",
    "                    pool = np.arange(itv)                       # j ∈ [0..itv-1]\n",
    "                    mapping[i] = np.random.choice(pool, size=100, replace=False)\n",
    "                return mapping\n",
    "\n",
    "            train_mapping = make_map(train_case_id)\n",
    "            val_mapping   = make_map(val_case_id)\n",
    "            test_mapping  = make_map(test_case_id)\n",
    "\n",
    "            with open(split_path, 'wb') as f:\n",
    "                pkl.dump((train_mapping, val_mapping, test_mapping), f)\n",
    "            print(\"Saved competitor split!\")\n",
    "\n",
    "        # pick the mapping you need\n",
    "        if   partition == 'train': mapping = train_mapping\n",
    "        elif partition == 'val'  : mapping = val_mapping\n",
    "        elif partition == 'test' : mapping = test_mapping\n",
    "        else: raise ValueError(f\"Unknown partition {partition!r}\")\n",
    "\n",
    "        # now proceed exactly as before, using `mapping` instead of your make_mapping\n",
    "        each_len = max_samples // len(mapping)\n",
    "        in_graphs = []\n",
    "        for i, pool in mapping.items():\n",
    "            for j in pool[:each_len]:\n",
    "                # note: they use delta_frame; you have nsteps*30, so this is identical\n",
    "                cur_x_t   = Ps[i][j]\n",
    "                cur_v_t   = Vs[i][j]\n",
    "                cur_v_tm1 = Vs[i][j-1]\n",
    "                y_dv      = Vs[i][j + self.nsteps*30] - Vs[i][j]\n",
    "                y_dx      = Ps[i][j + self.nsteps*30] - Ps[i][j]\n",
    "                y_pos_end = Ps[i][j + self.nsteps*30]\n",
    "                y_vel_end = Vs[i][j + self.nsteps*30]\n",
    "\n",
    "                in_graphs.append(self.create_in_graph(\n",
    "                    edges,\n",
    "                    x=(cur_x_t, cur_v_t, cur_v_tm1),\n",
    "                    y=(y_dv, y_dx, y_pos_end, y_vel_end)\n",
    "                ))\n",
    "\n",
    "        self.in_graphs = in_graphs\n",
    "        print(f\"[HumanDataset:{partition}] built {len(in_graphs)} samples\")\n",
    "\n",
    "    def central_diff(self, Xs, dt: float = 1.0, window_length: int = 41):\n",
    "        Ps, Vs, As = [], [], []\n",
    "        for x in Xs:\n",
    "            v      = (x[2:] - x[:-2]) / (2*dt)\n",
    "            a      = (x[2:] - 2*x[1:-1] + x[:-2]) / (dt**2)\n",
    "            p      = x[1:-1]                      # align to v,a\n",
    "            Ps.append(p)\n",
    "            Vs.append(v)\n",
    "            As.append(a)\n",
    "        return Ps, Vs, As\n",
    "\n",
    "        \n",
    "    def get_foot_nodes(self, nodes):\n",
    "        foot_indices = np.argsort(nodes[:,1])[:6]\n",
    "        foot_pos = nodes[foot_indices]\n",
    "        return foot_pos, foot_indices\n",
    "    \n",
    "    def reflected_nodes(self, nodes, z0=0, epsilon=1e-3):\n",
    "        reflected = nodes.copy()\n",
    "        reflected[:,1] = 2*z0 - nodes[:,1] - epsilon\n",
    "        distances = reflected[:,1] - nodes[:,1]\n",
    "        return reflected, distances\n",
    "    \n",
    "    def find_min(self, nodes):\n",
    "        return np.min(nodes, axis=0)\n",
    "    \n",
    "\n",
    "    def create_edges(self, N, edges):\n",
    "        atom_edges = torch.zeros(N, N).int()\n",
    "        for edge in edges:\n",
    "            atom_edges[edge[0], edge[1]] = 1\n",
    "            atom_edges[edge[1], edge[0]] = 1\n",
    "\n",
    "        atom_edges2 = atom_edges @ atom_edges\n",
    "        self.atom_edge = atom_edges\n",
    "        self.atom_edge2 = atom_edges2\n",
    "        edge_attr = []\n",
    "        # Initialize edges and edge_attributes\n",
    "        rows, cols = [], []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    if atom_edges[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([1])\n",
    "                        assert not atom_edges2[i][j]\n",
    "                    if atom_edges2[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([2])\n",
    "                        assert not atom_edges[i][j]\n",
    "\n",
    "        edges = [rows, cols] \n",
    "        edge_attr = torch.Tensor(np.array(edge_attr))  # [edge, 3]\n",
    "        edge_idx =torch.tensor(edges, dtype=torch.long)  # [2, M]   \n",
    "        return edge_idx,edge_attr     \n",
    "    \n",
    "    \n",
    "    def create_in_graph(self, edges,x,y):\n",
    "        pos_t, vel_t, vel_tm1 = x\n",
    "        y_dv,y_dx,y_pos_end,y_vel_end = y\n",
    "\n",
    "        edge_idx,edge_attr = self.create_edges(pos_t.shape[0], edges)\n",
    "\n",
    "        # # Get the ground node\n",
    "        # z0_t = self.find_min(pos_t)[1]\n",
    "        # z0_end = self.find_min(y_end)[1]\n",
    "        # # Center the y-positions around z0 for input and target\n",
    "        # pos_t -= np.array([0, z0_t, 0]) \n",
    "        # y_end -= np.array([0, z0_end, 0])\n",
    "\n",
    "        # Get the foot node positions and indices\n",
    "        # foot_nodes_positions, foot_nodes_indices = self.get_foot_nodes(pos_t)\n",
    "        # foot_nodes_reflected, foot_distances = self.reflected_nodes(foot_nodes_positions,z0=0.0)\n",
    "        \n",
    "        # current_largest_node_index = pos_t.shape[0]\n",
    "        # reflected_nodes_indices = []\n",
    "        # for reflected_node in range(foot_nodes_indices.shape[0]):\n",
    "        #     reflected_node_index = current_largest_node_index\n",
    "        #     current_largest_node_index += 1\n",
    "        #     reflected_nodes_indices.append(reflected_node_index)\n",
    "        \n",
    "        \n",
    "        # # Set lists to torch tensors\n",
    "        # reflected_nodes_indices = torch.tensor(reflected_nodes_indices)\n",
    "        # foot_nodes_indices = torch.tensor(foot_nodes_indices)\n",
    "        pos_t = torch.tensor(pos_t)\n",
    "        vel_t = torch.tensor(vel_t)\n",
    "        vel_tm1 = torch.tensor(vel_tm1)\n",
    "\n",
    "        y_dv = torch.tensor(y_dv)\n",
    "        y_dx = torch.tensor(y_dx)\n",
    "        y_pos_end = torch.tensor(y_pos_end)\n",
    "        y_vel_end = torch.tensor(y_vel_end)\n",
    "        \n",
    "        \n",
    "        # foot_nodes_reflected = torch.tensor(foot_nodes_reflected)\n",
    "        \n",
    "        # Set the node type of feet to one\n",
    "        node_type = torch.ones(pos_t.shape[0],1)\n",
    "        # node_type[foot_nodes_indices] = 1\n",
    "        # # Make reflected nodes of type 2\n",
    "        # new_node_type = torch.vstack((node_type,2*torch.ones_like(reflected_nodes_indices).unsqueeze(1))) \n",
    "        \n",
    "        # New bi-dir edge indexes\n",
    "        # new_edges_ref = torch.hstack((foot_nodes_indices.unsqueeze(1), reflected_nodes_indices.unsqueeze(1))) # connect foot edges to their reflections\n",
    "        # new_edges_ref = new_edges_ref.t()  # now [2, M]\n",
    "        # rev_new_edges_ref = new_edges_ref.flip(0)  # reverse the order to match edge index format\n",
    "        # new_edges_bidir_ref = torch.cat((new_edges_ref, rev_new_edges_ref), dim=1)  # add reverse edges\n",
    "        # new_edge_index = torch.cat([edge_idx, new_edges_bidir_ref], dim=1) # add new edges to the graph edge index\n",
    "        # s,r = new_edge_index\n",
    "\n",
    "        # we add the 1 as edge attr for these edges as they are 1 hop\n",
    "        # new_edge_attr = torch.vstack((edge_attr, torch.ones((new_edges_bidir_ref.shape[1], 1))))  # add new edge attributes\n",
    "        # for differentiating reflected edges we use another features i.e. type_sender*type_receiver\n",
    "        # new_edge_attr = torch.hstack((new_edge_attr,\n",
    "        #                               new_node_type[s]*new_node_type[r]))\n",
    "        # new_pos_t = torch.vstack((pos_t, foot_nodes_reflected))\n",
    "        # new_vel_t = torch.vstack((vel_t,torch.zeros_like(foot_nodes_reflected)))\n",
    "        # new_vel_tm1 = torch.vstack((vel_tm1,torch.zeros_like(foot_nodes_reflected)))\n",
    "\n",
    "        \n",
    "        # in_graph = Data(x=new_pos_t,edge_index=new_edge_index,edge_attr=new_edge_attr)\n",
    "        # in_graph.node_vel_t = new_vel_t\n",
    "        # in_graph.node_vel_tm1 = new_vel_tm1\n",
    "        # in_graph.y_dv = y_dv\n",
    "        # in_graph.y_dx = y_dx\n",
    "        # in_graph.y_end = y_end\n",
    "        # in_graph.node_type = new_node_type\n",
    "\n",
    "        in_graph = Data(edge_index=edge_idx, edge_attr=edge_attr)\n",
    "        in_graph.pos = pos_t\n",
    "        in_graph.vel = vel_t\n",
    "        in_graph.prev_vel = vel_tm1\n",
    "        in_graph.y_dv = y_dv\n",
    "        in_graph.y_dx = y_dx\n",
    "        in_graph.end_pos = y_pos_end\n",
    "        in_graph.end_vel = y_vel_end\n",
    "        in_graph.node_type = node_type\n",
    "        \n",
    "        return in_graph     \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.in_graphs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.in_graphs[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2be11086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded competitor split!\n",
      "[HumanDataset:train] built 198 samples\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "Loaded competitor split!\n",
      "[HumanDataset:val] built 600 samples\n"
     ]
    }
   ],
   "source": [
    "dataset_train = HumanDataset(partition='train', max_samples=model_settings[\"max_training_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)\n",
    "dataset_test = HumanDataset(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)\n",
    "dataset_val = HumanDataset(partition='val', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c270d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All edges are bidirectional.\n"
     ]
    }
   ],
   "source": [
    "gt = next(iter(dataset_val))\n",
    "# Convert to set of tuples\n",
    "edges = set((i.item(), j.item()) for i, j in zip(gt.edge_index[0], gt.edge_index[1]))\n",
    "\n",
    "# Check for missing reverse edges\n",
    "missing = [(j, i) for (i, j) in edges if (j, i) not in edges]\n",
    "\n",
    "if len(missing) == 0:\n",
    "    print(\" All edges are bidirectional.\")\n",
    "else:\n",
    "    print(f\"{len(missing)} edge(s) are not bidirectional:\")\n",
    "    print(missing[:10])  # print a few missing edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cc051",
   "metadata": {},
   "source": [
    "The next cell creates the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7f784c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(data):\n",
    "    graph_current = data\n",
    "    \n",
    "    return graph_current\n",
    "\n",
    "class GraphFromRawDataset(Dataset):\n",
    "    def __init__(self, raw_dataset):\n",
    "        self.raw_dataset = raw_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.raw_dataset[idx]\n",
    "        return create_graph_data(data)\n",
    "\n",
    "def create_dataloaders_from_raw(dataset, M, shuffle=True):\n",
    "    \"\"\"\n",
    "    M: number of graphs to batch in training (use 1 for test/val)\n",
    "    \"\"\"\n",
    "    # Wrap raw datasets into PyTorch Dataset\n",
    "    dataset = GraphFromRawDataset(dataset)\n",
    "\n",
    "    # Create dataloaders\n",
    "    loader = DataLoader(dataset, batch_size=M, shuffle=shuffle, collate_fn=Batch.from_data_list)\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28af6d2c-14cb-4f14-b0cf-b6a14387d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloaders_from_raw(dataset_train,M=model_settings[\"batch_size\"])\n",
    "val_loader = create_dataloaders_from_raw(dataset_val,M=model_settings[\"batch_size\"], shuffle=False)\n",
    "test_loader = create_dataloaders_from_raw(dataset_test,M=model_settings[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f481e76-50e2-42df-96f9-372eadad2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_max_edge(train_loader):\n",
    "    \"\"\"\n",
    "    Calculate min/max statistics for graph properties using the ConsecutiveGraphDataset.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    timestep_dict : dict\n",
    "        Dictionary containing graphs with keys as timesteps.\n",
    "    time_step_increment : int, optional\n",
    "        Time step increment to use (default: 1).\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    tuple:\n",
    "        Min/max statistics for various physical properties.\n",
    "    \"\"\"\n",
    "    # Initialize lists to collect data\n",
    "    all_edge_dx = []\n",
    "    all_node_v_t = []\n",
    "    all_node_v_tm1 = []\n",
    "    all_node_dv = []\n",
    "    all_node_dx = []\n",
    "    \n",
    "    # Process all valid timesteps\n",
    "    for batch in train_loader:\n",
    "        batched_graph = batch\n",
    "        \n",
    "        senders,receivers = batched_graph.edge_index\n",
    "\n",
    "        edge_dx = batched_graph.pos[receivers] - batched_graph.pos[senders]\n",
    "        \n",
    "        # Extract positions and velocities\n",
    "        node_vel_t = batched_graph.vel.float()\n",
    "        node_vel_tm1 = batched_graph.prev_vel.float()\n",
    "        \n",
    "        # Calculate displacements and acceleration changes\n",
    "        mask_body = (batched_graph.node_type!=2).squeeze()\n",
    "        node_dv = (batched_graph.y_dv).float()\n",
    "        node_dx = (batched_graph.y_dx).float()\n",
    "        \n",
    "        # Collect data\n",
    "        all_edge_dx.append(edge_dx)\n",
    "        all_node_v_t.append(node_vel_t)\n",
    "        all_node_v_tm1.append(node_vel_tm1)\n",
    "        all_node_dv.append(node_dv)\n",
    "        all_node_dx.append(node_dx)\n",
    "    \n",
    "    # Concatenate all collected data\n",
    "    all_edge_dx = torch.cat(all_edge_dx, dim=0)\n",
    "    all_node_v_t = torch.cat(all_node_v_t, dim=0)\n",
    "    all_node_v_tm1 = torch.cat(all_node_v_tm1, dim=0)\n",
    "    all_node_dv= torch.cat(all_node_dv, dim=0)\n",
    "    all_node_dx = torch.cat(all_node_dx,dim=0)\n",
    "    \n",
    "    # Compute norms\n",
    "    norm_edge_dx = all_edge_dx.norm(dim=1)\n",
    "    norm_node_v_t = all_node_v_t.norm(dim=1)\n",
    "    norm_node_v_tm1 = all_node_v_tm1.norm(dim=1)\n",
    "    norm_node_dv = all_node_dv.norm(dim=1)\n",
    "    norm_node_dx = all_node_dx.norm(dim=1)\n",
    "    \n",
    "    # Compute min and max values of the norms\n",
    "    min_edge_dx = norm_edge_dx.min()\n",
    "    max_edge_dx = norm_edge_dx.max()\n",
    "\n",
    "    min_node_v_t = norm_node_v_t.min()\n",
    "    max_node_v_t = norm_node_v_t.max()\n",
    "\n",
    "    min_node_v_tm1 = norm_node_v_tm1.min()\n",
    "    max_node_v_tm1 = norm_node_v_tm1.max()\n",
    "\n",
    "    mean_node_dv = norm_node_dv.mean()\n",
    "    std_node_dv = norm_node_dv.std()\n",
    "\n",
    "    mean_node_dx = norm_node_dx.mean()\n",
    "    std_node_dx = norm_node_dx.std()\n",
    "\n",
    "    # Collect statistics in tuples\n",
    "    stat_edge_dx = (min_edge_dx, max_edge_dx)\n",
    "    stat_node_v_t = (min_node_v_t, max_node_v_t)\n",
    "    stat_node_v_tm1 = (min_node_v_tm1, max_node_v_tm1)\n",
    "    stat_node_dv = (mean_node_dv, std_node_dv)\n",
    "    stat_node_dx = (mean_node_dx, std_node_dx)\n",
    "    \n",
    "    return stat_edge_dx, stat_node_v_t, stat_node_dv, stat_node_dx\n",
    "\n",
    "def move_train_stats_to_device(train_stats, device):\n",
    "    def move_to_device(stat):\n",
    "        if len(stat)==2:\n",
    "            min_val, max_val = stat\n",
    "            return min_val.to(device), max_val.to(device)\n",
    "        else: \n",
    "            max_val = stat\n",
    "            return max_val.to(device)\n",
    "        \n",
    "\n",
    "    return tuple(move_to_device(stat) for stat in train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13699dea",
   "metadata": {},
   "source": [
    "## Dynamical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4da637d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = calculate_min_max_edge(train_loader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_stats = move_train_stats_to_device(stats, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b93cc",
   "metadata": {},
   "source": [
    "### Components for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ebc83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_d(in_size, hidden_size, out_size, num_layers=1, lay_norm=True, use_sigmoid=False, use_softmax=False):\n",
    "    \"\"\"\n",
    "    Builds a multi-layer perceptron (MLP) with configurable depth and optional layer normalization and sigmoid or softmax activation.\n",
    "\n",
    "    Args:\n",
    "        in_size (int): The size of the input feature vector.\n",
    "        hidden_size (int): The size of the hidden layers.\n",
    "        out_size (int): The size of the output layer.\n",
    "        num_layers (int): The number of layers in the MLP.\n",
    "        lay_norm (bool): Flag to add layer normalization after the last linear layer.\n",
    "        use_sigmoid (bool): Flag to add a sigmoid activation layer at the output.\n",
    "        use_softmax (bool): Flag to add a softmax activation layer at the output.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The constructed MLP model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If both use_sigmoid and use_softmax are True.\n",
    "    \"\"\"\n",
    "    if use_sigmoid and use_softmax:\n",
    "        raise ValueError(\"Only one of use_sigmoid or use_softmax can be true.\")\n",
    "    layers = [nn.Linear(in_size, hidden_size), nn.ReLU()]\n",
    "    \n",
    "    # Add intermediate layers\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "    # Add the output layer\n",
    "    layers.append(nn.Linear(hidden_size, out_size))\n",
    "\n",
    "    # Create the MLP module\n",
    "    module = nn.Sequential(*layers)\n",
    "\n",
    "    # Optionally add layer normalization\n",
    "    if lay_norm:\n",
    "        module = nn.Sequential(module, nn.LayerNorm(normalized_shape=out_size))\n",
    "\n",
    "    # Optionally add sigmoid activation\n",
    "    if use_sigmoid:\n",
    "        module = nn.Sequential(module, nn.Sigmoid())\n",
    "\n",
    "    # Optionally add softmax activation\n",
    "    if use_softmax:\n",
    "        module = nn.Sequential(module, nn.Softmax(dim=-1))\n",
    "\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18ebb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefFrameCalc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefFrameCalc, self).__init__()\n",
    "\n",
    "    def forward(self, edge_index,senders_pos,receivers_pos, senders_vel,receivers_vel, senders_omega, receivers_omega):\n",
    "        \n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        vector_a = (receivers_pos - senders_pos)/ torch.clamp((receivers_pos - senders_pos).norm(dim=1, keepdim=True), min=epsilon)\n",
    "\n",
    "        #prelimnary vectors\n",
    "        b_a = torch.cross(receivers_vel-senders_vel,vector_a,dim=1)\n",
    "        b_a = b_a / torch.clamp(b_a.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        b_c = (senders_vel + receivers_vel)\n",
    "        b_c = b_c / torch.clamp(b_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        \n",
    "        b_a_ = torch.cross(receivers_omega-senders_omega,vector_a,dim=1)\n",
    "        b_a_ = b_a_ / torch.clamp(b_a_.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        b_c_ = (senders_omega + receivers_omega)\n",
    "        b_c_ = b_c_ / torch.clamp(b_c_.norm(dim=1, keepdim=True), min=epsilon)\n",
    "\n",
    "        b = b_a + b_c + b_a_ + b_c_ \n",
    "\n",
    "        # Compute the parallel component of b\n",
    "        b_prl_dot = torch.einsum('ij,ij->i', b, vector_a).unsqueeze(1)\n",
    "        b_prl = b_prl_dot * vector_a\n",
    "\n",
    "        # Compute the perpendicular component of b\n",
    "        b_prp = b - b_prl\n",
    "\n",
    "        vector_b = torch.cross(b_prp, vector_a,dim=1) #perp to a and a new vector b_prp\n",
    "        vector_c = torch.cross(b_prl, vector_b,dim=1) #perp to a and b\n",
    "        \n",
    "        vector_b = vector_b / torch.clamp(vector_b.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        vector_c = vector_c / torch.clamp(vector_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "   \n",
    "        return vector_a, vector_b, vector_c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101908f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'module' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70165da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComDecoder(nn.Module):\n",
    "    def __init__(self, num_nodes,latent_size):\n",
    "        super(ComDecoder, self).__init__()\n",
    "        self.weight_decoder = build_mlp_d(1,latent_size,)\n",
    "    def forward(self, pos_markers):\n",
    "          weights = self.weight_decoder(pos_markers)\n",
    "        return node_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12582806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.node_encoder = build_mlp_d(2, latent_size, latent_size, num_layers=1, lay_norm=True)\n",
    "    def forward(self,node_scalar_feat):\n",
    "        node_latent = self.node_encoder(node_scalar_feat)  \n",
    "        return node_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "679e5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionEncoder(nn.Module):\n",
    "    \"\"\"Message passing.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size):\n",
    "        super(InteractionEncoder, self).__init__()\n",
    "        self.edge_feat_encoder = build_mlp_d(9, latent_size, latent_size, num_layers=1, lay_norm=True)\n",
    "        self.edge_encoder = build_mlp_d(3, latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "        self.interaction_encoder = build_mlp_d(3*latent_size,latent_size, latent_size, num_layers=1, lay_norm=True)\n",
    "    def forward(self, edge_index, edge_dx_,edge_attr, vector_a, vector_b, vector_c,\n",
    "                senders_v_t_, senders_v_tm1_, senders_w_t_,senders_th_t_,\n",
    "                receivers_v_t_, receivers_v_tm1_, receivers_w_t_,receivers_th_t_,\n",
    "                node_latent):\n",
    "\n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        node_v_t_senders_a = torch.einsum('ij,ij->i', senders_v_t_, vector_a).unsqueeze(1)\n",
    "        node_v_t_senders_b = torch.einsum('ij,ij->i', senders_v_t_, vector_b).unsqueeze(1)\n",
    "        node_v_t_senders_c = torch.einsum('ij,ij->i', senders_v_t_, vector_c).unsqueeze(1)\n",
    "\n",
    "        node_v_tm1_senders_a = torch.einsum('ij,ij->i', senders_v_tm1_, vector_a).unsqueeze(1)\n",
    "        node_v_tm1_senders_b = torch.einsum('ij,ij->i', senders_v_tm1_, vector_b).unsqueeze(1)\n",
    "        node_v_tm1_senders_c = torch.einsum('ij,ij->i', senders_v_tm1_, vector_c).unsqueeze(1)    \n",
    "\n",
    "        node_w_t_senders_a = torch.einsum('ij,ij->i', senders_w_t_, vector_a).unsqueeze(1)\n",
    "        node_w_t_senders_b = torch.einsum('ij,ij->i', senders_w_t_, vector_b).unsqueeze(1)\n",
    "        node_w_t_senders_c = torch.einsum('ij,ij->i', senders_w_t_, vector_c).unsqueeze(1)  \n",
    "        \n",
    "        node_v_t_receivers_a = torch.einsum('ij,ij->i', receivers_v_t_, -vector_a).unsqueeze(1)\n",
    "        node_v_t_receivers_b = torch.einsum('ij,ij->i', receivers_v_t_, -vector_b).unsqueeze(1)\n",
    "        node_v_t_receivers_c = torch.einsum('ij,ij->i', receivers_v_t_, -vector_c).unsqueeze(1)\n",
    "\n",
    "        node_v_tm1_receivers_a = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_a).unsqueeze(1)\n",
    "        node_v_tm1_receivers_b = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_b).unsqueeze(1)\n",
    "        node_v_tm1_receivers_c = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_c).unsqueeze(1)       \n",
    "        \n",
    "        node_w_t_receivers_a = torch.einsum('ij,ij->i', receivers_w_t_, -vector_a).unsqueeze(1)\n",
    "        node_w_t_receivers_b = torch.einsum('ij,ij->i', receivers_w_t_, -vector_b).unsqueeze(1)\n",
    "        node_w_t_receivers_c = torch.einsum('ij,ij->i', receivers_w_t_, -vector_c).unsqueeze(1)\n",
    "        \n",
    "        edge_dx_a_s = edge_dx_.norm(dim=1,keepdim=True)\n",
    "        edge_dth_a_s = (receivers_th_t_ - senders_th_t_).norm(dim=1,keepdim=True)\n",
    "\n",
    "        senders_features = torch.hstack((\n",
    "            node_v_t_senders_a, node_v_t_senders_b, node_v_t_senders_c,\n",
    "            node_v_tm1_senders_a, node_v_tm1_senders_b, node_v_tm1_senders_c,\n",
    "            node_w_t_senders_a, node_w_t_senders_b, node_w_t_senders_c\n",
    "        ))\n",
    "\n",
    "        receivers_features = torch.hstack((\n",
    "            node_v_t_receivers_a, node_v_t_receivers_b, node_v_t_receivers_c,\n",
    "            node_v_tm1_receivers_a, node_v_tm1_receivers_b, node_v_tm1_receivers_c,\n",
    "            node_w_t_receivers_a, node_w_t_receivers_b, node_w_t_receivers_c\n",
    "        ))\n",
    "        \n",
    "        edge_latent = self.edge_encoder(torch.hstack((edge_dx_a_s, edge_dth_a_s, edge_attr)))\n",
    "\n",
    "        senders_latent = self.edge_feat_encoder(senders_features)\n",
    "        receivers_latent = self.edge_feat_encoder(receivers_features)\n",
    "\n",
    "        interaction_latent = self.interaction_encoder(torch.hstack((senders_latent + receivers_latent,\n",
    "                                                                    node_latent[senders]+node_latent[receivers],\n",
    "                                                                    edge_latent)))\n",
    "\n",
    "        return interaction_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "368fce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(InteractionDecoder, self).__init__()\n",
    "        self.i1_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=1, lay_norm=False)\n",
    "        self.i2_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=1, lay_norm=False)\n",
    "\n",
    "    def forward(self, edge_index, senders_pos, receivers_pos, vector_a, vector_b, vector_c, interaction_latent, node_latent):\n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        coeff_f = self.i1_decoder(interaction_latent)\n",
    "        coeff_t = self.i2_decoder(interaction_latent)\n",
    "\n",
    "        fij = (coeff_f[:, 0:1] * vector_a + \n",
    "              coeff_f[:, 1:2] * vector_b + \n",
    "              coeff_f[:, 2:] * vector_c)\n",
    "        \n",
    "        \n",
    "        tij = (coeff_t[:, 0:1] * vector_a + \n",
    "              coeff_t[:, 1:2] * vector_b + \n",
    "              coeff_t[:, 2:] * vector_c)        \n",
    "        return fij, tij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a29b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_Internal_Dv_Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(Node_Internal_Dv_Decoder, self).__init__()\n",
    "        self.m_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "        self.i_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "        self.dv_ext_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=1, lay_norm=False)\n",
    "    def forward(self,edge_index,node_latent,fij,tij):\n",
    "        m_inv = self.m_inv_decoder(node_latent) # decode inverse of mass\n",
    "        i_inv = self.i_inv_decoder(node_latent) # decode inverse of inertia\n",
    "        senders,receivers = edge_index   \n",
    "        \n",
    "        out_fij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_fij = out_fij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]).to(device), fij.to(device))\n",
    "        node_dv_int = m_inv * (out_fij) + self.dv_ext_decoder(node_latent)\n",
    "        \n",
    "        out_tij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_tij = out_tij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]).to(device), tij.to(device))\n",
    "        node_dw_int = i_inv * out_tij       \n",
    "\n",
    "        return node_dv_int, node_dw_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70559816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scaler, self).__init__()\n",
    "        '''\n",
    "        Scales the velocity and angular velocity features by maximum magnitude of respective field in training data\n",
    "        Scales the magnitude of the edge_vector_dx using min-max scaling. (keeping the direction of edge_vector_dx same)\n",
    "        '''\n",
    "\n",
    "    def forward(self, senders_v_t, senders_v_tm1,receivers_v_t, receivers_v_tm1,edge_dx,train_stats):\n",
    "        stat_edge_dx, stat_node_v_t, _,_= train_stats\n",
    "        \n",
    "        senders_v_t_ = senders_v_t/stat_node_v_t[1].detach()\n",
    "        senders_v_tm1_ = senders_v_tm1/stat_node_v_t[1].detach()\n",
    "        receivers_v_t_ = receivers_v_t/stat_node_v_t[1].detach()\n",
    "        receivers_v_tm1_ = receivers_v_tm1/stat_node_v_t[1].detach()\n",
    "        norm_edge_dx = edge_dx.norm(dim=1, keepdim=True)\n",
    "        edge_dx_ = (((norm_edge_dx-stat_edge_dx[0])/(stat_edge_dx[1]-stat_edge_dx[0]))*(edge_dx/norm_edge_dx)).detach()\n",
    "        return senders_v_t_, senders_v_tm1_,receivers_v_t_, receivers_v_tm1_,edge_dx_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60303a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction_Block(torch.nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Interaction_Block, self).__init__()\n",
    "        self.interaction_encoder = InteractionEncoder(latent_size)\n",
    "        self.interaction_decoder = InteractionDecoder(latent_size)\n",
    "        self.internal_dv_decoder = Node_Internal_Dv_Decoder(latent_size)\n",
    "        self.layer_norm = nn.LayerNorm(latent_size)\n",
    "\n",
    "    def forward(self, edge_index, senders_pos, receivers_pos, edge_dx_, edge_attr,vector_a, vector_b, vector_c, \n",
    "                senders_v_t_, senders_v_tm1_, senders_w_t_,senders_th_t_,\n",
    "                receivers_v_t_, receivers_v_tm1_, receivers_w_t_,receivers_th_t_,\n",
    "                node_latent, residue=None, latent_history=False):\n",
    "            interaction_latent = self.interaction_encoder(edge_index, edge_dx_,edge_attr,\n",
    "                                                          vector_a, vector_b, vector_c,\n",
    "                                                          senders_v_t_, senders_v_tm1_, senders_w_t_,senders_th_t_,\n",
    "                                                          receivers_v_t_, receivers_v_tm1_, receivers_w_t_,receivers_th_t_,\n",
    "                                                          node_latent)\n",
    "\n",
    "            if latent_history:\n",
    "                interaction_latent = interaction_latent + residue\n",
    "                interaction_latent = self.layer_norm(interaction_latent)\n",
    "            \n",
    "            edge_interaction_force, edge_interaction_tau= self.interaction_decoder(\n",
    "                edge_index, senders_pos, receivers_pos, vector_a, vector_b, vector_c, interaction_latent, node_latent\n",
    "            )\n",
    "            node_dv_int_decoded, node_dw_int_decoded = self.internal_dv_decoder(\n",
    "                edge_index, node_latent, edge_interaction_force, edge_interaction_tau\n",
    "            )\n",
    "        \n",
    "            return node_dv_int_decoded, node_dw_int_decoded, interaction_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f536404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsSolver(torch.nn.Module):\n",
    "    def __init__(self, sample_step, train_stats, num_jumps=1, num_msgs=1, latent_size=128):\n",
    "        super(DynamicsSolver, self).__init__()\n",
    "        self.refframecalc = RefFrameCalc()\n",
    "        self.scaler = Scaler()\n",
    "        self.node_encoder = NodeEncoder(latent_size)\n",
    "        self.interaction_proc_layer = Interaction_Block(latent_size)\n",
    "        self.interaction_init_layer = Interaction_Block(latent_size)\n",
    "        self.num_messages = num_msgs\n",
    "        self.sub_tstep = sample_step / num_msgs\n",
    "        self.train_stats = train_stats\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # Initialize graph data for processing\n",
    "        device = graph.pos.device\n",
    "        graph = graph.to(device)\n",
    "        #node_type = torch.ones_like(graph.disp[:,0:1]).float()# get_node_type(graph).float()\n",
    "        node_type = graph.node_type.float()\n",
    "        pos = graph.pos.float()\n",
    "        vel = graph.vel.float()\n",
    "        prev_vel = graph.prev_vel.float()\n",
    "        \n",
    "        edge_index = graph.edge_index.long()\n",
    "        senders, receivers = edge_index\n",
    "        senders_pos = pos[senders]\n",
    "        receivers_pos = pos[receivers]\n",
    "        edge_dx = receivers_pos - senders_pos\n",
    "        edge_attr = graph.edge_attr.float()\n",
    "        \n",
    "        mask_reflected_node = (graph.node_type!=2).squeeze()\n",
    "\n",
    "        node_v_t = vel\n",
    "        node_w_t = getattr(graph, 'node_w_t',torch.zeros_like(node_v_t))\n",
    "        node_th_t = getattr(graph, 'node_th_t', torch.zeros_like(node_v_t))\n",
    "\n",
    "        senders_v_t = node_v_t[senders].float()\n",
    "        receivers_v_t = node_v_t[receivers].float()\n",
    "\n",
    "        senders_v_tm1 = prev_vel[senders].float()\n",
    "        receivers_v_tm1 = prev_vel[receivers].float()\n",
    "\n",
    "        senders_w_t = node_w_t[senders]\n",
    "        receivers_w_t = node_w_t[receivers]\n",
    "        \n",
    "        senders_th_t = node_th_t[senders]\n",
    "        receivers_th_t = node_th_t[receivers]        \n",
    "\n",
    "        node_disp = torch.zeros_like(node_v_t)\n",
    "        node_vf = torch.zeros_like(node_v_t)\n",
    "        node_wf = torch.zeros_like(node_v_t)\n",
    "\n",
    "        sum_node_dv = torch.zeros_like(node_v_t)\n",
    "        sum_node_dx = torch.zeros_like(node_v_t)\n",
    "        \n",
    "        node_latent = self.node_encoder(torch.hstack((node_type,vel[:,1:2])))\n",
    "\n",
    "\n",
    "        for i in range(self.num_messages):\n",
    "            (\n",
    "                senders_v_t_,\n",
    "                senders_v_tm1_,\n",
    "                receivers_v_t_,\n",
    "                receivers_v_tm1_,\n",
    "                edge_dx_,\n",
    "            ) = self.scaler(\n",
    "                senders_v_t,\n",
    "                senders_v_tm1,\n",
    "                receivers_v_t,\n",
    "                receivers_v_tm1,\n",
    "                edge_dx,\n",
    "                self.train_stats,\n",
    "            )\n",
    "\n",
    "\n",
    "            vector_a, vector_b, vector_c = self.refframecalc(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    senders_v_t_,\n",
    "                    receivers_v_t_,\n",
    "                    senders_w_t,\n",
    "                    receivers_w_t\n",
    "                    \n",
    "                )\n",
    "\n",
    "            if i == 0:\n",
    "                node_dv_int_decoded, node_dw_int_decoded,residue= self.interaction_init_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    senders_w_t,\n",
    "                    senders_th_t,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    receivers_w_t,\n",
    "                    receivers_th_t,\n",
    "                    node_latent,\n",
    "                    latent_history=False,\n",
    "                )\n",
    "            else:\n",
    "                node_dv_int_decoded, node_dw_int_decoded, residue = self.interaction_proc_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    senders_w_t,\n",
    "                    senders_th_t,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    receivers_w_t,\n",
    "                    receivers_th_t,\n",
    "                    node_latent,\n",
    "                    residue=residue,\n",
    "                    latent_history=True,\n",
    "                )\n",
    "\n",
    "            sum_node_dv [mask_reflected_node]= sum_node_dv [mask_reflected_node] + node_dv_int_decoded[mask_reflected_node]\n",
    "                \n",
    "            node_vf[mask_reflected_node]= node_v_t[mask_reflected_node] + node_dv_int_decoded[mask_reflected_node]\n",
    "            node_wf[mask_reflected_node]= node_w_t[mask_reflected_node] + node_dw_int_decoded[mask_reflected_node]\n",
    "\n",
    "            node_disp= (\n",
    "                (node_v_t + node_vf) * 0.5 * self.sub_tstep\n",
    "            )\n",
    "\n",
    "            node_th_t = node_th_t + (node_wf + node_w_t)* 0.5 * self.sub_tstep\n",
    "\n",
    "            sum_node_dx [mask_reflected_node]=sum_node_dx [mask_reflected_node]+ ((node_v_t + node_vf) * 0.5 * self.sub_tstep)[mask_reflected_node]\n",
    "\n",
    "            senders_disp = node_disp[senders]\n",
    "            receivers_disp = node_disp[receivers]\n",
    "\n",
    "            senders_pos = senders_disp + senders_pos\n",
    "            receivers_pos = receivers_disp + receivers_pos\n",
    "\n",
    "\n",
    "            node_v_tm1 = node_v_t.clone()\n",
    "\n",
    "            node_v_t = node_vf.clone()\n",
    "            node_w_t = node_wf.clone()\n",
    "\n",
    "            \n",
    "\n",
    "            senders_v_tm1 = senders_v_t.clone()\n",
    "            senders_v_t = node_v_t[senders].clone()\n",
    "            senders_w_t = node_w_t[senders].clone()\n",
    "            senders_th_t = node_th_t[senders].clone()\n",
    "\n",
    "            receivers_v_tm1 = receivers_v_t.clone()\n",
    "            receivers_v_t = node_v_t[receivers].clone()\n",
    "            receivers_w_t = node_w_t[receivers].clone()\n",
    "            receivers_th_t = node_th_t[receivers].clone()\n",
    "\n",
    "            edge_dx = receivers_pos - senders_pos\n",
    "        return sum_node_dv,sum_node_dx, node_v_tm1\n",
    "\n",
    "    def forward_infer(self, graph):\n",
    "        edge_index = graph.edge_index.long()\n",
    "        senders, receivers = edge_index\n",
    "        edge_attr = graph.edge_attr.to(torch.float32, copy=False).contiguous()\n",
    "    \n",
    "        node_type = graph.node_type.to(torch.float32, copy=False).contiguous()\n",
    "        mask_reflected_node = (graph.node_type != 2).squeeze()\n",
    "        ref_idx = torch.nonzero(mask_reflected_node, as_tuple=False).squeeze(1)\n",
    "    \n",
    "        pos      = graph.pos.to(torch.float32, copy=False).contiguous()\n",
    "        vel      = graph.vel.to(torch.float32, copy=False).contiguous()\n",
    "        prev_vel = graph.prev_vel.to(torch.float32, copy=False).contiguous()\n",
    "    \n",
    "        node_w_t  = getattr(graph, 'node_w_t',\n",
    "                            torch.zeros_like(vel, dtype=vel.dtype, device=vel.device)).contiguous()\n",
    "        node_th_t = getattr(graph, 'node_th_t',\n",
    "                            torch.zeros_like(vel, dtype=vel.dtype, device=vel.device)).contiguous()\n",
    "    \n",
    "        senders_pos   = pos.index_select(0, senders)\n",
    "        receivers_pos = pos.index_select(0, receivers)\n",
    "        edge_dx       = receivers_pos.sub(senders_pos)\n",
    "    \n",
    "        node_v_t   = vel.clone()\n",
    "        node_v_tm1 = prev_vel.clone()\n",
    "    \n",
    "        senders_v_t    = node_v_t.index_select(0, senders).contiguous()\n",
    "        receivers_v_t  = node_v_t.index_select(0, receivers).contiguous()\n",
    "        senders_v_tm1  = node_v_tm1.index_select(0, senders).contiguous()\n",
    "        receivers_v_tm1= node_v_tm1.index_select(0, receivers).contiguous()\n",
    "    \n",
    "        senders_w_t   = node_w_t.index_select(0, senders).contiguous()\n",
    "        receivers_w_t = node_w_t.index_select(0, receivers).contiguous()\n",
    "    \n",
    "        node_disp = torch.zeros_like(node_v_t)\n",
    "        node_vf   = torch.zeros_like(node_v_t)\n",
    "        node_wf   = torch.zeros_like(node_v_t)\n",
    "    \n",
    "        sum_node_dv = torch.zeros_like(node_v_t)\n",
    "        sum_node_dx = torch.zeros_like(node_v_t)\n",
    "    \n",
    "        senders_disp   = torch.empty_like(senders_pos)\n",
    "        receivers_disp = torch.empty_like(receivers_pos)\n",
    "    \n",
    "        node_latent = self.node_encoder(torch.hstack((node_type, vel[:, 1:2])))\n",
    "    \n",
    "        half_dt      = 0.5 * self.sub_tstep\n",
    "        num_messages = self.num_messages\n",
    "        scaler       = self.scaler\n",
    "        refframecalc = self.refframecalc\n",
    "        init_layer   = self.interaction_init_layer\n",
    "        proc_layer   = self.interaction_proc_layer\n",
    "        train_stats  = self.train_stats\n",
    "    \n",
    "        residue = None\n",
    "        for i in range(num_messages):\n",
    "            (senders_v_t_, senders_v_tm1_,\n",
    "             receivers_v_t_, receivers_v_tm1_,\n",
    "             edge_dx_) = scaler(\n",
    "                senders_v_t, senders_v_tm1,\n",
    "                receivers_v_t, receivers_v_tm1,\n",
    "                edge_dx, train_stats\n",
    "            )\n",
    "    \n",
    "            vector_a, vector_b, vector_c = refframecalc(\n",
    "                edge_index, senders_pos, receivers_pos,\n",
    "                senders_v_t_, receivers_v_t_,\n",
    "                senders_w_t,  receivers_w_t\n",
    "            )\n",
    "    \n",
    "            if i == 0:\n",
    "                node_dv_int_decoded, node_dw_int_decoded, residue = init_layer(\n",
    "                    edge_index, senders_pos, receivers_pos, edge_dx_, edge_attr,\n",
    "                    vector_a, vector_b, vector_c,\n",
    "                    senders_v_t_, senders_v_tm1_, senders_w_t, node_th_t.index_select(0, senders),\n",
    "                    receivers_v_t_, receivers_v_tm1_, receivers_w_t, node_th_t.index_select(0, receivers),\n",
    "                    node_latent, latent_history=False\n",
    "                )\n",
    "            else:\n",
    "                node_dv_int_decoded, node_dw_int_decoded, residue = proc_layer(\n",
    "                    edge_index, senders_pos, receivers_pos, edge_dx_, edge_attr,\n",
    "                    vector_a, vector_b, vector_c,\n",
    "                    senders_v_t_, senders_v_tm1_, senders_w_t, node_th_t.index_select(0, senders),\n",
    "                    receivers_v_t_, receivers_v_tm1_, receivers_w_t, node_th_t.index_select(0, receivers),\n",
    "                    node_latent, residue=residue, latent_history=True\n",
    "                )\n",
    "    \n",
    "            sum_node_dv.index_add_(0, ref_idx,\n",
    "                node_dv_int_decoded.index_select(0, ref_idx))\n",
    "    \n",
    "            node_vf.index_copy_(\n",
    "                0, ref_idx,\n",
    "                node_v_t.index_select(0, ref_idx)\n",
    "                        .add(node_dv_int_decoded.index_select(0, ref_idx))\n",
    "            )\n",
    "            node_wf.index_copy_(\n",
    "                0, ref_idx,\n",
    "                node_w_t.index_select(0, ref_idx)\n",
    "                        .add(node_dw_int_decoded.index_select(0, ref_idx))\n",
    "            )\n",
    "    \n",
    "            node_disp.copy_(node_v_t).add_(node_vf).mul_(half_dt)\n",
    "    \n",
    "            # FIXED: equivalent, side-effect-free update\n",
    "            tmp_w = node_wf + node_w_t\n",
    "            node_th_t.add_(tmp_w, alpha=half_dt)\n",
    "    \n",
    "            sum_node_dx.index_add_(0, ref_idx, node_disp.index_select(0, ref_idx))\n",
    "    \n",
    "            senders_disp.copy_(node_disp.index_select(0, senders))\n",
    "            receivers_disp.copy_(node_disp.index_select(0, receivers))\n",
    "            senders_pos.add_(senders_disp)\n",
    "            receivers_pos.add_(receivers_disp)\n",
    "    \n",
    "            node_v_tm1 = node_v_t.clone()\n",
    "            node_v_t.copy_(node_vf)\n",
    "            node_w_t.copy_(node_wf)\n",
    "    \n",
    "            senders_v_tm1.copy_(senders_v_t)\n",
    "            senders_v_t.copy_(node_v_t.index_select(0, senders))\n",
    "            receivers_v_tm1.copy_(receivers_v_t)\n",
    "            receivers_v_t.copy_(node_v_t.index_select(0, receivers))\n",
    "    \n",
    "            senders_w_t.copy_(node_w_t.index_select(0, senders))\n",
    "            receivers_w_t.copy_(node_w_t.index_select(0, receivers))\n",
    "    \n",
    "            # FIXED: keep *unscaled* delta-x for next iteration\n",
    "            edge_dx.copy_(receivers_pos).sub_(senders_pos)\n",
    "    \n",
    "        return sum_node_dv, sum_node_dx, node_v_tm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cd859dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model, device, mode='test', plot=False, frequency=1, model_name='PIGNN', experiment_name='Val'):\n",
    "    with torch.no_grad():\n",
    "        res = 0.\n",
    "        res_counter = 0\n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            for i in range(1):  # Iterates 10 times: 0 -> predicts 31, ..., 9 -> predicts 40\n",
    "                if i == 0:\n",
    "                    test_graph = test_batch.to(device)\n",
    "                    graph_t0 = test_graph\n",
    "                    # Clone to ensure ground truth remains unchanged\n",
    "                    end_pos = test_graph.end_pos.clone()  \n",
    "\n",
    "                node_dv,node_dx,_= model(graph_t0.detach())\n",
    "                new_vel =  graph_t0.vel + node_dv\n",
    "                new_pos =  graph_t0.pos + node_dx\n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = graph_t0.vel.clone()\n",
    "                graph_t0.pos = new_pos.clone()\n",
    "                graph_t0.vel = new_vel.clone()\n",
    "\n",
    "            loss = F.mse_loss(new_pos, end_pos)\n",
    "            batch_size = test_graph.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "\n",
    "        mean_pos_error = res / res_counter\n",
    "    return mean_pos_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc02010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device, train_stats):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.train_history = []\n",
    "        self.extr_test_history = []\n",
    "        self.gen_test_history = []\n",
    "        self.cur_dir = os.getcwd()\n",
    "        self.model_dir = os.path.join(self.cur_dir, 'saved_models')\n",
    "        self.gen_loss = 0.\n",
    "        self.train_stats = train_stats\n",
    "        self.mean_node_dv = self.train_stats[2][0]\n",
    "        self.mean_node_disp = self.train_stats[3][0]\n",
    "        self.std_node_dv = self.train_stats[2][1]\n",
    "        self.std_node_disp = self.train_stats[3][1]\n",
    "        # Initialize best test loss and best epoch.\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = None        \n",
    "\n",
    "    def train(self, train_graph):\n",
    "        self.model.train()\n",
    "        pred_node_dvel,pred_node_disp,_= self.model(train_graph.to(self.device))\n",
    "\n",
    "        actual_node_dvel = (train_graph.y_dv).float().to(self.device)\n",
    "        actual_node_disp = (train_graph.y_dx).float().to(self.device)\n",
    "\n",
    "        pred_node_dvel_ = (pred_node_dvel-self.mean_node_dv.detach())/self.std_node_dv.detach()\n",
    "        pred_node_disp_ = (pred_node_disp-self.mean_node_disp.detach())/self.std_node_disp.detach()\n",
    "        actual_node_dvel_ = (actual_node_dvel - self.mean_node_dv.detach())/self.std_node_dv.detach()\n",
    "        actual_node_disp_ = (actual_node_disp-self.mean_node_disp.detach())/self.std_node_disp.detach()\n",
    "\n",
    "        loss = F.mse_loss(pred_node_disp_,actual_node_disp_) #+ F.mse_loss(pred_node_dvel_,actual_node_dvel_)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.loss = loss.cpu().detach().numpy()\n",
    "        self.train_history.append(self.loss)\n",
    "    def test(self, test_loader, mode='val', epoch=None):\n",
    "        self.model.eval()\n",
    "        mean_pos_error = evaluate(test_loader,\n",
    "                            self.model,\n",
    "                            self.device)\n",
    "        \n",
    "        \n",
    "        if mode == 'val':\n",
    "            self.val_loss_pos = mean_pos_error\n",
    "            self.gen_test_history.append(self.val_loss_pos)\n",
    "            # Update best test loss and best epoch if the current loss is lower\n",
    "            if epoch is not None and self.val_loss_pos < self.best_val_loss:\n",
    "                self.best_val_loss = self.val_loss_pos\n",
    "                self.best_epoch = epoch\n",
    "                self.save_model(epoch)\n",
    "        if mode == 'test':\n",
    "            self.test_loss_pos = mean_pos_error\n",
    "\n",
    "    def save_model(self,iteration):\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        self.path = os.path.join(self.model_dir, \n",
    "                                 f'GenLoss_{self.val_loss_pos:.5f}mm_iter{iteration}.pth')\n",
    "        torch.save(self.model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3b6dbe9-823b-457f-b6b1-4653ebfe9036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE_TIME_STEP =  30.0\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if model_settings[\"finite_diff\"]:\n",
    "    SAMPLE_TIME_STEP = 30*model_settings[\"time_step\"]\n",
    "else:\n",
    "    SAMPLE_TIME_STEP = 30*model_settings[\"time_step_actual\"]\n",
    "print(f'SAMPLE_TIME_STEP = ',SAMPLE_TIME_STEP)\n",
    "N= 1\n",
    "MODEL = DynamicsSolver(SAMPLE_TIME_STEP, train_stats, num_jumps=N, num_msgs=4, latent_size=64)\n",
    "optimizer = torch.optim.Adam(MODEL.parameters(), lr = 5e-4)\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def ensure_model_on_device(model, device):\n",
    "    for module in model.modules():\n",
    "        # Check if any parameter of the module is not on the desired device\n",
    "        if any(p.device != device for p in module.parameters()):\n",
    "            # Move the entire module to the device\n",
    "            module.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensure_model_on_device(MODEL, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9070b11-5732-452a-a2bc-5cd6b4b95c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(MODEL, \n",
    "                  optimizer, \n",
    "                  device,\n",
    "                  train_stats\n",
    "                 )\n",
    "# Clear PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b83cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5fc2762c-67d1-4e3c-aca2-7f84a38ff010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/650 [00:01<20:43,  1.92s/it, TrainLoss (*1e3)=9.28940e+02, ValLoss=4.71438442e+00, BestVal=4.71438e+00(Ep1), TestLoss=4.47530481e+00, LR=5.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: wall time = 0.554 s | cpu time = 0.554 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/650 [00:03<20:33,  1.90s/it, TrainLoss (*1e3)=4.00743e+02, ValLoss=3.79091947e+00, BestVal=3.79092e+00(Ep2), TestLoss=3.57325913e+00, LR=5.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: wall time = 0.565 s | cpu time = 0.565 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/650 [00:05<20:41,  1.92s/it, TrainLoss (*1e3)=3.74397e+02, ValLoss=4.89486206e+00, BestVal=3.79092e+00(Ep2), TestLoss=4.55180381e+00, LR=5.00e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: wall time = 0.560 s | cpu time = 0.559 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/650 [00:06<22:40,  2.10s/it, TrainLoss (*1e3)=3.74397e+02, ValLoss=4.89486206e+00, BestVal=3.79092e+00(Ep2), TestLoss=4.55180381e+00, LR=5.00e-04]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m start_cpu  = time.process_time()       \n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m train_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m wall_time = time.perf_counter() - start_wall\n\u001b[32m     18\u001b[39m cpu_time  = time.process_time() - start_cpu\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_graph)\u001b[39m\n\u001b[32m     33\u001b[39m loss = F.mse_loss(pred_node_disp_,actual_node_disp_) \u001b[38;5;66;03m#+ F.mse_loss(pred_node_dvel_,actual_node_dvel_)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nn_control_env/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nn_control_env/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nn_control_env/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 650\n",
    "\n",
    "# initialize to large values\n",
    "val_loss = np.inf\n",
    "test_loss = np.inf\n",
    "\n",
    "# wrap the epoch loop in tqdm\n",
    "with tqdm(range(1, epochs+1), desc='Training') as pbar:\n",
    "    for epoch in pbar:\n",
    "        # ——— Training ———\n",
    "        start_wall = time.perf_counter()\n",
    "        start_cpu  = time.process_time()       \n",
    "        for train_batch in train_loader:\n",
    "            trainer.train(train_batch)\n",
    "        wall_time = time.perf_counter() - start_wall\n",
    "        cpu_time  = time.process_time() - start_cpu\n",
    "        # ——— Validation & Test ———\n",
    "        # trainer.test now returns the mean positional error\n",
    "        if epoch%1==0:\n",
    "            trainer.test(val_loader,  mode='val',  epoch=epoch)\n",
    "            val_loss = trainer.val_loss_pos\n",
    "            trainer.test(test_loader, mode='test')\n",
    "            test_loss = trainer.test_loss_pos\n",
    "\n",
    "        # ——— Learning rate ———\n",
    "        current_lr = trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # ——— Update the progress bar ———\n",
    "        pbar.set_postfix({\n",
    "            'TrainLoss (*1e3)': f'{trainer.loss * 1e3:.5e}',\n",
    "            'ValLoss':           f'{val_loss:.8e}',\n",
    "            'BestVal':           f'{trainer.best_val_loss:.5e}(Ep{trainer.best_epoch})',\n",
    "            'TestLoss':          f'{test_loss:.8e}',\n",
    "            'LR':                f'{current_lr:.2e}',\n",
    "        })\n",
    "        print(f\"Epoch {epoch}: wall time = {wall_time:.3f} s | cpu time = {cpu_time:.3f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029910e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m.plot(trainer.train_history)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(trainer.train_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1330dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e0ef6",
   "metadata": {},
   "source": [
    "## Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f200b9b",
   "metadata": {},
   "source": [
    "This next cell has the functions that will, given a frame of the human, use the model to predict eventual position of the human after 30 time steps. It then also gives some sample animations of the predictions alongside the ground truth of the final position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1529bd0",
   "metadata": {},
   "source": [
    "### Main loop that runs the test rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f32a7e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best file: GenLoss_0.40743mm_iter79.pth\n",
      "  ↳ parsed loss:  0.40743\n",
      "  ↳ parsed epoch: 79\n",
      "Model loaded on CPU and set to eval().\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Regular expression pattern to extract loss and epoch\n",
    "pattern = r'GenLoss_(\\d+(?:\\.\\d+)?)mm_iter(\\d+)\\.pth'\n",
    "\n",
    "best_loss  = float('inf')\n",
    "best_epoch = None\n",
    "best_file  = None\n",
    "\n",
    "for fn in os.listdir(trainer.model_dir):\n",
    "    m = re.search(pattern, fn)\n",
    "    if m:\n",
    "        loss_val = float(m.group(1))\n",
    "        if loss_val < best_loss:\n",
    "            best_loss  = loss_val\n",
    "            best_epoch = int(m.group(2))\n",
    "            best_file  = fn\n",
    "\n",
    "if best_file is None:\n",
    "    raise RuntimeError(\"No checkpoint found matching pattern: GenLoss_<loss>mm_iter<epoch>.pth\")\n",
    "\n",
    "path = os.path.join(trainer.model_dir, best_file)\n",
    "\n",
    "# ----- load weights on CPU -----\n",
    "ckpt = torch.load(path, map_location=\"cpu\")\n",
    "\n",
    "# Unwrap common checkpoint containers\n",
    "if isinstance(ckpt, dict):\n",
    "    if \"state_dict\" in ckpt:\n",
    "        state = ckpt[\"state_dict\"]\n",
    "    elif \"model_state_dict\" in ckpt:\n",
    "        state = ckpt[\"model_state_dict\"]\n",
    "    elif \"model\" in ckpt and isinstance(ckpt[\"model\"], dict):\n",
    "        state = ckpt[\"model\"]\n",
    "    else:\n",
    "        # assume it's already a plain state_dict\n",
    "        state = ckpt\n",
    "else:\n",
    "    state = ckpt  # fallback\n",
    "\n",
    "# Strip \"module.\" prefix if saved from DataParallel\n",
    "if any(k.startswith(\"module.\") for k in state.keys()):\n",
    "    state = {k.replace(\"module.\", \"\", 1): v for k, v in state.items()}\n",
    "\n",
    "# Load into model (on CPU) and set eval mode\n",
    "trainer.model.load_state_dict(state, strict=True)\n",
    "trainer.model.to(\"cpu\").eval()\n",
    "\n",
    "print(f\"Best file: {best_file}\")\n",
    "print(f\"  ↳ parsed loss:  {best_loss}\")\n",
    "print(f\"  ↳ parsed epoch: {best_epoch}\")\n",
    "print(\"Model loaded on CPU and set to eval().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab4f95-c407-4430-88c5-0ff13b1285e6",
   "metadata": {},
   "source": [
    "# EVALUATION RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a73d106-a728-4eb9-bbe6-cd5d604b1cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40742951562512364\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "trainer.test(val_loader, mode='val')\n",
    "print(trainer.val_loss_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2b3f5fe-984f-4a1f-8c4b-c67ff7f2b100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34161011619788384\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "trainer.test(test_loader, mode='val')\n",
    "print(trainer.val_loss_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de2f23f8-8495-40d6-8166-ccd767407a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rollout(test_loader, model, device, nsteps=1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = 0.\n",
    "        res_counter = 0\n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            for i in range(nsteps):  # Iterates 10 times: 0 -> predicts 31, ..., 9 -> predicts 40\n",
    "                if i == 0:\n",
    "                    test_graph = test_batch.to(device)\n",
    "                    graph_t0 = test_graph\n",
    "                    # Clone to ensure ground truth remains unchanged\n",
    "                    end_pos = test_graph.end_pos.clone()  \n",
    "\n",
    "                node_dv,node_dx,vtm1= model(graph_t0.detach())\n",
    "                new_vel =  graph_t0.vel + node_dv\n",
    "                new_pos =  graph_t0.pos + node_dx\n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = graph_t0.vel.clone()\n",
    "                graph_t0.pos = new_pos.clone()\n",
    "                graph_t0.vel = new_vel.clone()\n",
    "\n",
    "            loss = F.mse_loss(new_pos, end_pos)\n",
    "            batch_size = test_graph.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "\n",
    "        mean_pos_error = res / res_counter\n",
    "    return mean_pos_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e849850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model: torch.nn.Module, name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Print number of parameters and structural metrics for a PyTorch model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{name} Summary\")\n",
    "    print(\"=\" * (len(name) + 8))\n",
    "    \n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    layer_info = []\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "        numel = p.numel()\n",
    "        total_params += numel\n",
    "        if p.requires_grad:\n",
    "            trainable_params += numel\n",
    "        layer_info.append((n, tuple(p.shape), numel, p.requires_grad))\n",
    "\n",
    "    print(f\"Total parameters:     {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Non-trainable:        {total_params - trainable_params:,}\\n\")\n",
    "\n",
    "    # Extra structural metrics\n",
    "    num_layers = sum(1 for _ in model.modules())\n",
    "    num_unique_layers = sum(1 for _ in model.children())\n",
    "    print(\"\\nOther metrics:\")\n",
    "    print(f\"  Total nn.Modules (including container modules): {num_layers}\")\n",
    "    print(f\"  Immediate child modules:                        {num_unique_layers}\")\n",
    "    print(f\"  Parameter tensors:                              {len(layer_info)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5fc6e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_rollout(test_loader, model, device, nsteps=1):\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        res = 0.0\n",
    "        res_counter = 0\n",
    "        batches = 0\n",
    "\n",
    "        t_wall0 = time.perf_counter()\n",
    "        t_cpu0  = time.process_time()        \n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            batches += 1\n",
    "            for i in range(nsteps):\n",
    "                if i == 0:\n",
    "                    test_graph = test_batch.to(device)\n",
    "                    graph_t0 = test_graph\n",
    "                    end_pos = test_graph.end_pos.clone()  # keep GT unchanged\n",
    "\n",
    "                node_dv, node_dx, vtm1 = model(graph_t0.detach())\n",
    "                new_vel = graph_t0.vel + node_dv\n",
    "                new_pos = graph_t0.pos + node_dx\n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = vtm1.clone()\n",
    "                graph_t0.pos = new_pos.clone()\n",
    "                graph_t0.vel = new_vel.clone()\n",
    "            break\n",
    "            loss = F.mse_loss(new_pos, end_pos)\n",
    "            batch_size = test_graph.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "            \n",
    "\n",
    "        mean_pos_error = res / res_counter if res_counter > 0 else float(\"nan\")\n",
    "\n",
    "    # timing + summary\n",
    "    t_wall = time.perf_counter() - t_wall0\n",
    "    t_cpu  = time.process_time() - t_cpu0\n",
    "    total_steps = batches * nsteps\n",
    "\n",
    "    print(\"===== Rollout Evaluation Timing =====\")\n",
    "    print(f\"Device:               {device}\")\n",
    "    print(f\"Batches processed:    {batches}\")\n",
    "    print(f\"nsteps per batch:     {nsteps}\")\n",
    "    print(f\"Total rollout steps:  {total_steps}\")\n",
    "    print(f\"Total wall time:      {t_wall:.3f} s\")\n",
    "    print(f\"Total CPU time:       {t_cpu:.3f} s  (CPU/Wall = {t_cpu/t_wall:.3f})\")\n",
    "    if total_steps > 0 and t_wall > 0:\n",
    "        print(f\"Latency/step:         {t_wall/total_steps*1000:.3f} ms\")\n",
    "        print(f\"Throughput:           {total_steps/t_wall:.2f} steps/s\")\n",
    "    print(f\"Mean Position Error:  {mean_pos_error:.6e}\")\n",
    "    print(\"=====================================\")\n",
    "\n",
    "    model_summary(model,'DGN')\n",
    "\n",
    "    return mean_pos_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6af236bb-e575-4a07-9666-846ee1f07d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "===== Rollout Evaluation Timing =====\n",
      "Device:               cpu\n",
      "Batches processed:    1\n",
      "nsteps per batch:     1\n",
      "Total rollout steps:  1\n",
      "Total wall time:      0.004 s\n",
      "Total CPU time:       0.004 s  (CPU/Wall = 1.000)\n",
      "Latency/step:         3.805 ms\n",
      "Throughput:           262.84 steps/s\n",
      "Mean Position Error:  nan\n",
      "=====================================\n",
      "\n",
      "DGN Summary\n",
      "===========\n",
      "Total parameters:     108,050\n",
      "Trainable parameters: 108,050\n",
      "Non-trainable:        0\n",
      "\n",
      "\n",
      "Other metrics:\n",
      "  Total nn.Modules (including container modules): 100\n",
      "  Immediate child modules:                        5\n",
      "  Parameter tensors:                              90\n",
      "\n",
      "loss for rollout 1 steps nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for nstep in [1]:\n",
    "    \n",
    "    dataset_eval = HumanDataset(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"],nsteps=nstep)\n",
    "    \n",
    "    dataloader_eval = create_dataloaders_from_raw(dataset_eval,1,shuffle=False)\n",
    "    \n",
    "    eval_error = evaluate_rollout(dataloader_eval, trainer.model, device, nsteps=nstep)\n",
    "    \n",
    "    print(f'loss for rollout {nstep} steps {eval_error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870fd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c725db9-f613-4100-ae66-0707e5b44a46",
   "metadata": {},
   "source": [
    "# VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47a5eb-065c-4e2b-9476-09a227f6ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDatasetSeq(torch.utils.data.Dataset):\n",
    "    def __init__(self, partition='train', max_samples=600, data_dir='', nsteps=1):\n",
    "        self.partition = partition\n",
    "        self.data_dir = data_dir\n",
    "        self.nsteps = nsteps\n",
    "\n",
    "        # --- load raw data --------------------------------------\n",
    "        with open(os.path.join(data_dir, 'motion.pkl'), 'rb') as f:\n",
    "            edges, X = pkl.load(f)\n",
    "\n",
    "        # your smoothing / central_diff code here...\n",
    "        Ps, Vs, As = self.central_diff(X)\n",
    "\n",
    "        # trial IDs must match exactly\n",
    "        train_case_id = [20,1,17,13,14,9,4,2,7,5,16]\n",
    "        val_case_id   = [3,8,11,12,15,18]\n",
    "        test_case_id  = [6,19,21,0,22,10]\n",
    "\n",
    "        # --- load or create competitor splits (fixed for central_diff) ----------\n",
    "        split_path = os.path.join(data_dir, f'split_n{self.nsteps}.pkl')\n",
    "        try:\n",
    "            with open(split_path, 'rb') as f:\n",
    "                train_mapping, val_mapping, test_mapping = pkl.load(f)\n",
    "                print(\"Loaded competitor split!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Generating competitor split…\")\n",
    "\n",
    "            def make_map(case_ids):\n",
    "                mapping = {}\n",
    "                for i in case_ids:\n",
    "                    core_len = Ps[i].shape[0]                    # <<— use length after central_diff\n",
    "                    safe_max = core_len - self.nsteps*30 - 1\n",
    "                    if safe_max < 0:\n",
    "                        raise ValueError(f\"Trial {i} too short for look-ahead of {self.nsteps} steps.\")\n",
    "                    # competitor caps at 300\n",
    "                    itv = min(300, safe_max + 1)                # +1 because j in [0..safe_max]\n",
    "                    pool = np.arange(itv)                       # j ∈ [0..itv-1]\n",
    "                    mapping[i] = np.random.choice(pool, size=100, replace=False)\n",
    "                return mapping\n",
    "\n",
    "            train_mapping = make_map(train_case_id)\n",
    "            val_mapping   = make_map(val_case_id)\n",
    "            test_mapping  = make_map(test_case_id)\n",
    "\n",
    "            with open(split_path, 'wb') as f:\n",
    "                pkl.dump((train_mapping, val_mapping, test_mapping), f)\n",
    "            print(\"Saved competitor split!\")\n",
    "\n",
    "        # pick the mapping you need\n",
    "        if   partition == 'train': mapping = train_mapping\n",
    "        elif partition == 'val'  : mapping = val_mapping\n",
    "        elif partition == 'test' : mapping = test_mapping\n",
    "        else: raise ValueError(f\"Unknown partition {partition!r}\")\n",
    "\n",
    "        # now proceed exactly as before, using `mapping` instead of your make_mapping\n",
    "        each_len = max_samples // len(mapping)\n",
    "        in_graphs = []\n",
    "        for i, pool in mapping.items():\n",
    "            for j in pool[:each_len]:\n",
    "                # note: they use delta_frame; you have nsteps*30, so this is identical\n",
    "                cur_x_t   = Ps[i][j]\n",
    "                cur_v_t   = Vs[i][j]\n",
    "                cur_v_tm1 = Vs[i][j-1]\n",
    "                y_dv      = Vs[i][j + self.nsteps*30] - Vs[i][j]\n",
    "                y_dx      = Ps[i][j + self.nsteps*30] - Ps[i][j]\n",
    "                gt_seq = [ Ps[i][j + k*30] for k in range(self.nsteps+1) ]   # list of (31,3) arrays\n",
    "                y_pos_end = Ps[i][j + self.nsteps*30]\n",
    "                y_vel_end = Vs[i][j + self.nsteps*30]\n",
    "\n",
    "                in_graphs.append(self.create_in_graph(\n",
    "                    edges,\n",
    "                    x=(cur_x_t, cur_v_t, cur_v_tm1),\n",
    "                    y=(y_dv, y_dx, y_pos_end, y_vel_end),\n",
    "                    gt_seq = gt_seq\n",
    "                ))\n",
    "\n",
    "        self.in_graphs = in_graphs\n",
    "        print(f\"[HumanDataset:{partition}] built {len(in_graphs)} samples\")\n",
    "\n",
    "    def central_diff(self, Xs, dt: float = 1.0, window_length: int = 41):\n",
    "        Ps, Vs, As = [], [], []\n",
    "        for x in Xs:\n",
    "            v      = (x[2:] - x[:-2]) / (2*dt)\n",
    "            a      = (x[2:] - 2*x[1:-1] + x[:-2]) / (dt**2)\n",
    "            p      = x[1:-1]                      # align to v,a\n",
    "            Ps.append(p)\n",
    "            Vs.append(v)\n",
    "            As.append(a)\n",
    "        return Ps, Vs, As\n",
    "\n",
    "        \n",
    "    def get_foot_nodes(self, nodes):\n",
    "        foot_indices = np.argsort(nodes[:,1])[:6]\n",
    "        foot_pos = nodes[foot_indices]\n",
    "        return foot_pos, foot_indices\n",
    "    \n",
    "    def reflected_nodes(self, nodes, z0=0, epsilon=1e-3):\n",
    "        reflected = nodes.copy()\n",
    "        reflected[:,1] = 2*z0 - nodes[:,1] - epsilon\n",
    "        distances = reflected[:,1] - nodes[:,1]\n",
    "        return reflected, distances\n",
    "    \n",
    "    def find_min(self, nodes):\n",
    "        return np.min(nodes, axis=0)\n",
    "    \n",
    "\n",
    "    def create_edges(self, N, edges):\n",
    "        atom_edges = torch.zeros(N, N).int()\n",
    "        for edge in edges:\n",
    "            atom_edges[edge[0], edge[1]] = 1\n",
    "            atom_edges[edge[1], edge[0]] = 1\n",
    "\n",
    "        atom_edges2 = atom_edges @ atom_edges\n",
    "        self.atom_edge = atom_edges\n",
    "        self.atom_edge2 = atom_edges2\n",
    "        edge_attr = []\n",
    "        # Initialize edges and edge_attributes\n",
    "        rows, cols = [], []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    if atom_edges[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([1])\n",
    "                        assert not atom_edges2[i][j]\n",
    "                    if atom_edges2[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([2])\n",
    "                        assert not atom_edges[i][j]\n",
    "\n",
    "        edges = [rows, cols] \n",
    "        edge_attr = torch.Tensor(np.array(edge_attr))  # [edge, 3]\n",
    "        edge_idx =torch.tensor(edges, dtype=torch.long)  # [2, M]   \n",
    "        return edge_idx,edge_attr     \n",
    "    \n",
    "    \n",
    "    def create_in_graph(self, edges,x,y,gt_seq):\n",
    "        pos_t, vel_t, vel_tm1 = x\n",
    "        y_dv,y_dx,y_pos_end,y_vel_end = y\n",
    "\n",
    "        edge_idx,edge_attr = self.create_edges(pos_t.shape[0], edges)\n",
    "\n",
    "        # # Get the ground node\n",
    "        # z0_t = self.find_min(pos_t)[1]\n",
    "        # z0_end = self.find_min(y_end)[1]\n",
    "        # # Center the y-positions around z0 for input and target\n",
    "        # pos_t -= np.array([0, z0_t, 0]) \n",
    "        # y_end -= np.array([0, z0_end, 0])\n",
    "\n",
    "        # Get the foot node positions and indices\n",
    "        # foot_nodes_positions, foot_nodes_indices = self.get_foot_nodes(pos_t)\n",
    "        # foot_nodes_reflected, foot_distances = self.reflected_nodes(foot_nodes_positions,z0=0.0)\n",
    "        \n",
    "        # current_largest_node_index = pos_t.shape[0]\n",
    "        # reflected_nodes_indices = []\n",
    "        # for reflected_node in range(foot_nodes_indices.shape[0]):\n",
    "        #     reflected_node_index = current_largest_node_index\n",
    "        #     current_largest_node_index += 1\n",
    "        #     reflected_nodes_indices.append(reflected_node_index)\n",
    "        \n",
    "        \n",
    "        # # Set lists to torch tensors\n",
    "        # reflected_nodes_indices = torch.tensor(reflected_nodes_indices)\n",
    "        # foot_nodes_indices = torch.tensor(foot_nodes_indices)\n",
    "        pos_t = torch.tensor(pos_t)\n",
    "        vel_t = torch.tensor(vel_t)\n",
    "        vel_tm1 = torch.tensor(vel_tm1)\n",
    "\n",
    "        y_dv = torch.tensor(y_dv)\n",
    "        y_dx = torch.tensor(y_dx)\n",
    "        y_pos_end = torch.tensor(y_pos_end)\n",
    "        y_vel_end = torch.tensor(y_vel_end)\n",
    "        \n",
    "        \n",
    "        # foot_nodes_reflected = torch.tensor(foot_nodes_reflected)\n",
    "        \n",
    "        # Set the node type of feet to one\n",
    "        node_type = torch.ones(pos_t.shape[0],1)\n",
    "        # node_type[foot_nodes_indices] = 1\n",
    "        # # Make reflected nodes of type 2\n",
    "        # new_node_type = torch.vstack((node_type,2*torch.ones_like(reflected_nodes_indices).unsqueeze(1))) \n",
    "        \n",
    "        # New bi-dir edge indexes\n",
    "        # new_edges_ref = torch.hstack((foot_nodes_indices.unsqueeze(1), reflected_nodes_indices.unsqueeze(1))) # connect foot edges to their reflections\n",
    "        # new_edges_ref = new_edges_ref.t()  # now [2, M]\n",
    "        # rev_new_edges_ref = new_edges_ref.flip(0)  # reverse the order to match edge index format\n",
    "        # new_edges_bidir_ref = torch.cat((new_edges_ref, rev_new_edges_ref), dim=1)  # add reverse edges\n",
    "        # new_edge_index = torch.cat([edge_idx, new_edges_bidir_ref], dim=1) # add new edges to the graph edge index\n",
    "        # s,r = new_edge_index\n",
    "\n",
    "        # we add the 1 as edge attr for these edges as they are 1 hop\n",
    "        # new_edge_attr = torch.vstack((edge_attr, torch.ones((new_edges_bidir_ref.shape[1], 1))))  # add new edge attributes\n",
    "        # for differentiating reflected edges we use another features i.e. type_sender*type_receiver\n",
    "        # new_edge_attr = torch.hstack((new_edge_attr,\n",
    "        #                               new_node_type[s]*new_node_type[r]))\n",
    "        # new_pos_t = torch.vstack((pos_t, foot_nodes_reflected))\n",
    "        # new_vel_t = torch.vstack((vel_t,torch.zeros_like(foot_nodes_reflected)))\n",
    "        # new_vel_tm1 = torch.vstack((vel_tm1,torch.zeros_like(foot_nodes_reflected)))\n",
    "\n",
    "        \n",
    "        # in_graph = Data(x=new_pos_t,edge_index=new_edge_index,edge_attr=new_edge_attr)\n",
    "        # in_graph.node_vel_t = new_vel_t\n",
    "        # in_graph.node_vel_tm1 = new_vel_tm1\n",
    "        # in_graph.y_dv = y_dv\n",
    "        # in_graph.y_dx = y_dx\n",
    "        # in_graph.y_end = y_end\n",
    "        # in_graph.node_type = new_node_type\n",
    "\n",
    "        in_graph = Data(edge_index=edge_idx, edge_attr=edge_attr)\n",
    "        in_graph.pos = pos_t\n",
    "        in_graph.vel = vel_t\n",
    "        in_graph.prev_vel = vel_tm1\n",
    "        in_graph.y_dv = y_dv\n",
    "        in_graph.y_dx = y_dx\n",
    "        in_graph.end_pos = y_pos_end\n",
    "        in_graph.end_vel = y_vel_end\n",
    "        in_graph.node_type = node_type\n",
    "        in_graph.gt_seq = gt_seq\n",
    "        \n",
    "        return in_graph     \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.in_graphs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.in_graphs[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aaccb3-2c30-44b3-92d3-6cfbd52c2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def visualize_multi_step(\n",
    "    test_loader,\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    steps=(1,2,3,4),\n",
    "    num_graphs=10,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    For each of `num_graphs` random graphs from test_loader:\n",
    "      • Plot initial_vs_gt.png once (vs GT at step=1)\n",
    "      • Then do a single graph rollout, saving pred_vs_gt_step{n}.png\n",
    "        for each n in `steps`, comparing to GT at that same step.\n",
    "    Uses absolute coordinates (no centering).\n",
    "    Assumes each Data has `gt_seq` as a list of length T+1,\n",
    "    each element shape (n_nodes,3).\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    model.eval()\n",
    "\n",
    "    # flatten loader\n",
    "    all_graphs = []\n",
    "    for batch in test_loader:\n",
    "        all_graphs.extend(batch.to_data_list())\n",
    "    if not all_graphs:\n",
    "        raise RuntimeError(\"No graphs in loader\")\n",
    "\n",
    "    # sample indices\n",
    "    chosen = random.sample(range(len(all_graphs)), min(num_graphs, len(all_graphs)))\n",
    "\n",
    "    # skeleton edges for the 31-body joints\n",
    "    skeleton31 = [\n",
    "      [1,0],[2,1],[3,2],[4,3],[5,4],\n",
    "      [6,0],[7,6],[8,7],[9,8],[10,9],\n",
    "      [11,0],[12,11],[13,12],[14,13],[15,14],\n",
    "      [16,15],[17,13],[18,17],[19,18],[20,19],\n",
    "      [21,20],[22,21],[23,20],[24,13],[25,24],\n",
    "      [26,25],[27,26],[28,27],[29,28],[30,27]\n",
    "    ]\n",
    "\n",
    "    for idx in chosen:\n",
    "        data = all_graphs[idx].to(device)\n",
    "        base = f\"./RESULTS/TrajectoryPlotsRefNodes/graph_{idx}\"\n",
    "        os.makedirs(base, exist_ok=True)\n",
    "\n",
    "        # — stack gt_seq list → np array [T+1,31,3] —\n",
    "        seq_list = data.gt_seq\n",
    "        seq_np = []\n",
    "        for arr in seq_list:\n",
    "            if torch.is_tensor(arr):\n",
    "                seq_np.append(arr.cpu().numpy())\n",
    "            else:\n",
    "                seq_np.append(np.array(arr))\n",
    "        gt_seq = np.stack(seq_np, axis=0)[:, :31, :]  # shape [T+1,31,3]\n",
    "\n",
    "        # initial pose (absolute)\n",
    "        init31 = data.pos[:31].cpu().numpy()\n",
    "\n",
    "        # compute axis limits from initial + all selected GT steps\n",
    "        all_pts = np.vstack([init31] + [gt_seq[k] for k in steps])\n",
    "        pad = 2.0\n",
    "        x_min, x_max = all_pts[:,2].min() - pad, all_pts[:,2].max() + pad\n",
    "        y_min, y_max = all_pts[:,0].min() - pad, all_pts[:,0].max() + pad\n",
    "        z_min, z_max = all_pts[:,1].min() - pad, all_pts[:,1].max() + pad\n",
    "\n",
    "        # — Plot initial_vs_gt.png (vs GT at step=1) —\n",
    "        gt1 = gt_seq[0]\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax  = fig.add_subplot(111, projection='3d')\n",
    "        xx, yy = np.meshgrid([x_min,x_max], [y_min,y_max])\n",
    "        ax.plot_surface(xx, yy, np.zeros_like(xx), color='gray', alpha=0.2, linewidth=0)\n",
    "        ax.scatter(init31[:,2], init31[:,0], init31[:,1],\n",
    "                   c='red', s=30, edgecolors='k', alpha=0.5, label='Initial')\n",
    "        for a,b in skeleton31:\n",
    "            ax.plot([gt1[a,2], gt1[b,2]],\n",
    "                    [gt1[a,0], gt1[b,0]],\n",
    "                    [gt1[a,1], gt1[b,1]],\n",
    "                    c='red', alpha=0.6, linestyle='-', linewidth=2)\n",
    "\n",
    "        mid_x = (x_min + x_max)/2\n",
    "        mid_y = (y_min + y_max)/2\n",
    "        mid_z = (z_min + z_max)/2\n",
    "        \n",
    "        ax.set_xlim(mid_x-20, mid_x+20)\n",
    "        ax.set_ylim(mid_y-20, mid_y+20)\n",
    "        ax.set_zlim(mid_z-20, mid_z+20)\n",
    "        ax.set_box_aspect((1,1,1))\n",
    "        ax.set_xlabel(\"X\",fontsize = 16); ax.set_ylabel(\"Y\",fontsize = 16); ax.set_zlabel(\"Z\",fontsize = 16)\n",
    "        ax.set_title(\"Initial\",fontsize = 18)\n",
    "        ax.legend(loc='upper left',fontsize = 18)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base, 'initial_vs_gt.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # — rollout & per-step plotting —\n",
    "        graph = data.clone().to(device)\n",
    "        for step in steps:\n",
    "            dv, dx,prev_vel = model(graph.detach())\n",
    "            graph.prev_vel = prev_vel\n",
    "            graph.vel      = graph.vel + dv\n",
    "            graph.pos      = graph.pos + dx\n",
    "\n",
    "            pred31 = graph.pos[:31].detach().cpu().numpy()\n",
    "            gt_k   = gt_seq[step]\n",
    "\n",
    "            fig = plt.figure(figsize=(6,6))\n",
    "            ax  = fig.add_subplot(111, projection='3d')\n",
    "            # optional ground plane: uncomment if desired\n",
    "            ax.plot_surface(xx, yy, np.zeros_like(xx), color='gray', alpha=0.2, linewidth=0)\n",
    "\n",
    "            ax.scatter(pred31[:,2], pred31[:,0], pred31[:,1],\n",
    "                       c='blue', s=30, edgecolors='k', alpha=0.5,\n",
    "                       label=f'Pred (step={step})')\n",
    "            ax.scatter(gt_k[:,2], gt_k[:,0], gt_k[:,1],\n",
    "                       c='red',  s=30, edgecolors='k', alpha=0.5,\n",
    "                       label=f'GT (step={step})')\n",
    "            for a,b in skeleton31:\n",
    "                ax.plot([pred31[a,2], pred31[b,2]],\n",
    "                        [pred31[a,0], pred31[b,0]],\n",
    "                        [pred31[a,1], pred31[b,1]],\n",
    "                        c='blue', alpha=0.6, linewidth=2)\n",
    "                ax.plot([gt_k[a,2], gt_k[b,2]],\n",
    "                        [gt_k[a,0], gt_k[b,0]],\n",
    "                        [gt_k[a,1], gt_k[b,1]],\n",
    "                        c='red', alpha=0.6, linestyle='-',linewidth=2)\n",
    "\n",
    "            ax.set_xlim(mid_x-20, mid_x+20)\n",
    "            ax.set_ylim(mid_y-20, mid_y+20)\n",
    "            ax.set_zlim(mid_z-20, mid_z+20)\n",
    "            ax.set_box_aspect((1,1,1))\n",
    "            ax.set_xlabel(\"X\",fontsize = 16); ax.set_ylabel(\"Y\",fontsize = 16); ax.set_zlabel(\"Z\",fontsize = 16)\n",
    "            ax.set_title(f\"Prediction vs GT — {step} steps\",fontsize = 18)\n",
    "            ax.legend(loc='upper left',fontsize = 18)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(base, f'pred_vs_gt_step{step}.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "        # optional: compute final MSE in absolute frame\n",
    "        mask_cuda = (data.node_type[:31] != 2).squeeze()\n",
    "        mask_cpu  = mask_cuda.cpu().numpy()\n",
    "        final_pred = graph.pos[:31][mask_cuda]\n",
    "        final_gt_np = gt_seq[steps[-1]][mask_cpu]\n",
    "        final_gt    = torch.from_numpy(final_gt_np).to(device)\n",
    "        mse = F.mse_loss(final_pred, final_gt).item()\n",
    "        print(f\"Graph {idx}, final step={steps[-1]}, MSE={mse:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81124734-e9b8-48a9-8cc9-9d9adf182d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_eval = HumanDatasetSeq(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"],nsteps=4)\n",
    "\n",
    "loader = create_dataloaders_from_raw(dataset_eval,200,shuffle=False)\n",
    "\n",
    "visualize_multi_step(\n",
    "    loader,\n",
    "    trainer.model,\n",
    "    device,\n",
    "    steps=[1,2,3,4],\n",
    "    num_graphs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933f41f-06b5-4d90-8c4e-dee0613c00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "\n",
    "# Base directory containing graph folders\n",
    "base_dir = './RESULTS/TrajectoryPlotsRefNodes'\n",
    "\n",
    "# Iterate through each subfolder (graph_*)\n",
    "for graph_folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, graph_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    # Collect all PNG files in sorted order\n",
    "    png_files = sorted(glob.glob(os.path.join(folder_path, '*.png')))\n",
    "    if not png_files:\n",
    "        continue\n",
    "    \n",
    "    # Read each image\n",
    "    images = []\n",
    "    for png in png_files:\n",
    "        try:\n",
    "            img = imageio.imread(png)\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not read {png}: {e}\")\n",
    "    \n",
    "    # Save as infinite-loop GIF\n",
    "    gif_path = os.path.join(folder_path, 'rollout.gif')\n",
    "    imageio.mimsave(gif_path, images, duration=0.7, loop=0)\n",
    "    print(f\"Created {gif_path} with {len(images)} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18200e-7984-4132-b233-ee9c893d2615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e08202-4ea4-4a11-83da-6338bf9a3611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn_control_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
