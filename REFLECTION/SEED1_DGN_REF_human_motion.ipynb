{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9c9b56",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for the running of DYNAMICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7edd8365-2fe3-4354-b373-38befe37df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use GPU 1 for the operation\n",
    "\n",
    "\n",
    "seed = 100\n",
    "\n",
    "# 1) MUST do this before importing torch:\n",
    "os.environ[\"PYTHONHASHSEED\"]         = str(seed)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75740aa6",
   "metadata": {},
   "source": [
    "All of the code that is present in this notebook are present in the other python files of this Dynamical folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ee16f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c0afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import math\n",
    "import pickle as pkl\n",
    "import random\n",
    "import shutil\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4be8b",
   "metadata": {},
   "source": [
    "### Dictionary that sets the model settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b91ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings ={\n",
    "        \"batch_size\": 100,\n",
    "        \"epochs\": 600,\n",
    "        \"lr\": 3e-4,\n",
    "        \"nf\": 64,\n",
    "        \"model\": \"gmn\",\n",
    "        \"attention\": 0,\n",
    "        \"n_layers\": 4,\n",
    "        \"max_testing_samples\": 600,\n",
    "        \"max_training_samples\": 200,\n",
    "        \"data_dir\": \"/home/visharma/gnn/WORKING_DIR/2025_MASTER_STUDENTS/HUMAN_MOTION/data\",\n",
    "        \"norm_diff\": False,\n",
    "        \"weight_decay\": 1e-10,\n",
    "        \"tanh\": False,\n",
    "        \"learnable\": False,\n",
    "        \"finite_diff\":True,\n",
    "        \"time_step\":1.0,\n",
    "        \"end_time_step\": 30.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c81ab9",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4a41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d86b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, partition='train', max_samples=600, data_dir='', nsteps=1):\n",
    "        self.partition = partition\n",
    "        self.data_dir = data_dir\n",
    "        self.nsteps = nsteps\n",
    "\n",
    "        # --- load raw data --------------------------------------\n",
    "        with open(os.path.join(data_dir, 'motion.pkl'), 'rb') as f:\n",
    "            edges, X = pkl.load(f)\n",
    "\n",
    "        # your smoothing / central_diff code here...\n",
    "        Ps, Vs, As = self.central_diff(X)\n",
    "\n",
    "        # trial IDs must match exactly\n",
    "        train_case_id = [20,1,17,13,14,9,4,2,7,5,16]\n",
    "        val_case_id   = [3,8,11,12,15,18]\n",
    "        test_case_id  = [6,19,21,0,22,10]\n",
    "\n",
    "        # --- load or create competitor splits (fixed for central_diff) ----------\n",
    "        split_path = os.path.join(data_dir, f'split_n{self.nsteps}.pkl')\n",
    "        try:\n",
    "            with open(split_path, 'rb') as f:\n",
    "                train_mapping, val_mapping, test_mapping = pkl.load(f)\n",
    "                print(\"Loaded competitor split!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Generating competitor split…\")\n",
    "\n",
    "            def make_map(case_ids):\n",
    "                mapping = {}\n",
    "                for i in case_ids:\n",
    "                    core_len = Ps[i].shape[0]                    # <<— use length after central_diff\n",
    "                    safe_max = core_len - self.nsteps*30 - 1\n",
    "                    if safe_max < 0:\n",
    "                        raise ValueError(f\"Trial {i} too short for look-ahead of {self.nsteps} steps.\")\n",
    "                    # competitor caps at 300\n",
    "                    itv = min(300, safe_max + 1)                # +1 because j in [0..safe_max]\n",
    "                    pool = np.arange(itv)                       # j ∈ [0..itv-1]\n",
    "                    mapping[i] = np.random.choice(pool, size=100, replace=False)\n",
    "                return mapping\n",
    "\n",
    "            train_mapping = make_map(train_case_id)\n",
    "            val_mapping   = make_map(val_case_id)\n",
    "            test_mapping  = make_map(test_case_id)\n",
    "\n",
    "            with open(split_path, 'wb') as f:\n",
    "                pkl.dump((train_mapping, val_mapping, test_mapping), f)\n",
    "            print(\"Saved competitor split!\")\n",
    "\n",
    "        # pick the mapping you need\n",
    "        if   partition == 'train': mapping = train_mapping\n",
    "        elif partition == 'val'  : mapping = val_mapping\n",
    "        elif partition == 'test' : mapping = test_mapping\n",
    "        else: raise ValueError(f\"Unknown partition {partition!r}\")\n",
    "\n",
    "        # now proceed exactly as before, using `mapping` instead of your make_mapping\n",
    "        each_len = max_samples // len(mapping)\n",
    "        in_graphs = []\n",
    "        for i, pool in mapping.items():\n",
    "            for j in pool[:each_len]:\n",
    "                # note: they use delta_frame; you have nsteps*30, so this is identical\n",
    "                cur_x_t   = Ps[i][j]\n",
    "                cur_v_t   = Vs[i][j]\n",
    "                cur_v_tm1 = Vs[i][j-1]\n",
    "                y_dv      = Vs[i][j + self.nsteps*30] - Vs[i][j]\n",
    "                y_dx      = Ps[i][j + self.nsteps*30] - Ps[i][j]\n",
    "                y_pos_end = Ps[i][j + self.nsteps*30]\n",
    "                y_vel_end = Vs[i][j + self.nsteps*30]\n",
    "\n",
    "                in_graphs.append(self.create_in_graph(\n",
    "                    edges,\n",
    "                    x=(cur_x_t, cur_v_t, cur_v_tm1),\n",
    "                    y=(y_dv, y_dx, y_pos_end, y_vel_end)\n",
    "                ))\n",
    "\n",
    "        self.in_graphs = in_graphs\n",
    "        print(f\"[HumanDataset:{partition}] built {len(in_graphs)} samples\")\n",
    "\n",
    "    def central_diff(self, Xs, dt: float = 1.0, window_length: int = 41):\n",
    "        Ps, Vs, As = [], [], []\n",
    "        for x in Xs:\n",
    "            v      = (x[2:] - x[:-2]) / (2*dt)\n",
    "            a      = (x[2:] - 2*x[1:-1] + x[:-2]) / (dt**2)\n",
    "            p      = x[1:-1]                      # align to v,a\n",
    "            Ps.append(p)\n",
    "            Vs.append(v)\n",
    "            As.append(a)\n",
    "        return Ps, Vs, As\n",
    "\n",
    "        \n",
    "    def get_foot_nodes(self, nodes):\n",
    "        foot_indices = np.argsort(nodes[:,1])[:6]\n",
    "        foot_pos = nodes[foot_indices]\n",
    "        return foot_pos, foot_indices\n",
    "    \n",
    "    def reflected_nodes(self, nodes, z0=0, epsilon=1e-3):\n",
    "        reflected = nodes.copy()\n",
    "        reflected[:,1] = 2*z0 - nodes[:,1] - epsilon\n",
    "        distances = reflected[:,1] - nodes[:,1]\n",
    "        return reflected, distances\n",
    "    \n",
    "    def find_min(self, nodes):\n",
    "        return np.min(nodes, axis=0)\n",
    "    \n",
    "\n",
    "    def create_edges(self, N, edges):\n",
    "        atom_edges = torch.zeros(N, N).int()\n",
    "        for edge in edges:\n",
    "            atom_edges[edge[0], edge[1]] = 1\n",
    "            atom_edges[edge[1], edge[0]] = 1\n",
    "\n",
    "        atom_edges2 = atom_edges @ atom_edges\n",
    "        self.atom_edge = atom_edges\n",
    "        self.atom_edge2 = atom_edges2\n",
    "        edge_attr = []\n",
    "        # Initialize edges and edge_attributes\n",
    "        rows, cols = [], []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    if atom_edges[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([0])\n",
    "                        assert not atom_edges2[i][j]\n",
    "                    if atom_edges2[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([1])\n",
    "                        assert not atom_edges[i][j]\n",
    "\n",
    "        edges = [rows, cols] \n",
    "        edge_attr = torch.Tensor(np.array(edge_attr))  # [edge, 3]\n",
    "        edge_idx =torch.tensor(edges, dtype=torch.long)  # [2, M]   \n",
    "        return edge_idx,edge_attr     \n",
    "    \n",
    "    \n",
    "    def create_in_graph(self, edges,x,y):\n",
    "        pos_t, vel_t, vel_tm1 = x\n",
    "        y_dv,y_dx,y_pos_end,y_vel_end = y\n",
    "\n",
    "        edge_idx,edge_attr = self.create_edges(pos_t.shape[0], edges)\n",
    "\n",
    "        # Get the ground node\n",
    "        z0_t = self.find_min(pos_t)[1]\n",
    "\n",
    "        # Get the foot node positions and indices\n",
    "        foot_nodes_positions, foot_nodes_indices = self.get_foot_nodes(pos_t)\n",
    "        foot_nodes_reflected, foot_distances = self.reflected_nodes(foot_nodes_positions,z0=z0_t)\n",
    "        \n",
    "        # Build reflected node indices\n",
    "        current_largest_node_index = pos_t.shape[0]\n",
    "        reflected_nodes_indices = list(range(current_largest_node_index,\n",
    "                                             current_largest_node_index + len(foot_nodes_indices)))\n",
    "        \n",
    "        # Convert arrays to torch\n",
    "        pos_t       = torch.tensor(pos_t, dtype=torch.float32)\n",
    "        vel_t       = torch.tensor(vel_t, dtype=torch.float32)\n",
    "        vel_tm1     = torch.tensor(vel_tm1, dtype=torch.float32)\n",
    "        y_dv        = torch.tensor(y_dv, dtype=torch.float32)\n",
    "        y_dx        = torch.tensor(y_dx, dtype=torch.float32)\n",
    "        y_pos_end   = torch.tensor(y_pos_end, dtype=torch.float32)\n",
    "        y_vel_end   = torch.tensor(y_vel_end, dtype=torch.float32)\n",
    "        foot_nodes_reflected = torch.tensor(foot_nodes_reflected, dtype=torch.float32)\n",
    "                \n",
    "        # Node types: feet=1, reflections=2, others=0\n",
    "        node_type = torch.zeros(pos_t.shape[0], 1)\n",
    "        node_type[foot_nodes_indices] = 1\n",
    "        new_node_type = torch.cat([\n",
    "            node_type,\n",
    "            2 * torch.ones(len(reflected_nodes_indices), 1)\n",
    "        ], dim=0)\n",
    "        \n",
    "        # New bidirectional edges connecting each foot to its reflection\n",
    "        # shape before transpose: [num_foot, 2]\n",
    "        new_edges = torch.stack([\n",
    "            torch.tensor(foot_nodes_indices,            dtype=torch.long),\n",
    "            torch.tensor(reflected_nodes_indices,       dtype=torch.long),\n",
    "        ], dim=0)  # [2, num_foot]\n",
    "        rev_new_edges = new_edges.flip(0)\n",
    "        new_edges_bidir = torch.cat([new_edges, rev_new_edges], dim=1)  # [2, 2*num_foot]\n",
    "\n",
    "        # Combine with original edges\n",
    "        full_edge_idx = torch.cat([edge_idx, new_edges_bidir], dim=1)\n",
    "    \n",
    "        # New edge attributes: all ones for these 1-hop reflections\n",
    "        refl_attr = torch.zeros((new_edges_bidir.shape[1], 1))\n",
    "        full_edge_attr = torch.cat([edge_attr, refl_attr], dim=0)\n",
    "\n",
    "        # new edge attr for differentiating reflected nodes\n",
    "        non_refl_edges = torch.zeros_like(edge_attr)\n",
    "        refl_edges = torch.ones_like(refl_attr)\n",
    "        new_edge_attr  = torch.cat([non_refl_edges, refl_edges], dim=0)\n",
    "\n",
    "        full_edge_attr = torch.hstack((full_edge_attr, new_edge_attr))\n",
    "        \n",
    "        # Extend node features to include reflected nodes\n",
    "        full_pos_t   = torch.cat([pos_t, foot_nodes_reflected], dim=0)\n",
    "        full_vel_t   = torch.cat([vel_t, torch.zeros_like(foot_nodes_reflected)], dim=0)\n",
    "        full_vel_tm1 = torch.cat([vel_tm1, torch.zeros_like(foot_nodes_reflected)], dim=0)\n",
    "    \n",
    "        in_graph = Data(edge_index=full_edge_idx, edge_attr=full_edge_attr)\n",
    "        in_graph.pos       = full_pos_t\n",
    "        in_graph.vel       = full_vel_t\n",
    "        in_graph.prev_vel  = full_vel_tm1\n",
    "        in_graph.y_dv       = y_dv\n",
    "        in_graph.y_dx       = y_dx\n",
    "        in_graph.end_pos   = y_pos_end\n",
    "        in_graph.end_vel   = y_vel_end\n",
    "        in_graph.node_type = new_node_type\n",
    "    \n",
    "        return in_graph   \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.in_graphs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.in_graphs[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be11086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded competitor split!\n",
      "[HumanDataset:train] built 198 samples\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "Loaded competitor split!\n",
      "[HumanDataset:val] built 600 samples\n"
     ]
    }
   ],
   "source": [
    "dataset_train = HumanDataset(partition='train', max_samples=model_settings[\"max_training_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)\n",
    "dataset_test = HumanDataset(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)\n",
    "dataset_val = HumanDataset(partition='val', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"], nsteps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c270d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All edges are bidirectional.\n"
     ]
    }
   ],
   "source": [
    "gt = next(iter(dataset_val))\n",
    "# Convert to set of tuples\n",
    "edges = set((i.item(), j.item()) for i, j in zip(gt.edge_index[0], gt.edge_index[1]))\n",
    "\n",
    "# Check for missing reverse edges\n",
    "missing = [(j, i) for (i, j) in edges if (j, i) not in edges]\n",
    "\n",
    "if len(missing) == 0:\n",
    "    print(\" All edges are bidirectional.\")\n",
    "else:\n",
    "    print(f\"{len(missing)} edge(s) are not bidirectional:\")\n",
    "    print(missing[:10])  # print a few missing edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cc051",
   "metadata": {},
   "source": [
    "The next cell creates the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7f784c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_data(data):\n",
    "    graph_current = data\n",
    "    \n",
    "    return graph_current\n",
    "\n",
    "class GraphFromRawDataset(Dataset):\n",
    "    def __init__(self, raw_dataset):\n",
    "        self.raw_dataset = raw_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.raw_dataset[idx]\n",
    "        return create_graph_data(data)\n",
    "\n",
    "def create_dataloaders_from_raw(dataset, M, shuffle=True):\n",
    "    \"\"\"\n",
    "    M: number of graphs to batch in training (use 1 for test/val)\n",
    "    \"\"\"\n",
    "    # Wrap raw datasets into PyTorch Dataset\n",
    "    dataset = GraphFromRawDataset(dataset)\n",
    "\n",
    "    # Create dataloaders\n",
    "    loader = DataLoader(dataset, batch_size=M, shuffle=shuffle, collate_fn=Batch.from_data_list)\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28af6d2c-14cb-4f14-b0cf-b6a14387d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloaders_from_raw(dataset_train,M=model_settings[\"batch_size\"])\n",
    "val_loader = create_dataloaders_from_raw(dataset_val,M=model_settings[\"batch_size\"], shuffle=False)\n",
    "test_loader = create_dataloaders_from_raw(dataset_test,M=model_settings[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f481e76-50e2-42df-96f9-372eadad2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_max_edge(train_loader):\n",
    "    \"\"\"\n",
    "    Calculate min/max statistics for graph properties using the ConsecutiveGraphDataset.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    timestep_dict : dict\n",
    "        Dictionary containing graphs with keys as timesteps.\n",
    "    time_step_increment : int, optional\n",
    "        Time step increment to use (default: 1).\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    tuple:\n",
    "        Min/max statistics for various physical properties.\n",
    "    \"\"\"\n",
    "    # Initialize lists to collect data\n",
    "    all_edge_dx = []\n",
    "    all_node_v_t = []\n",
    "    all_node_v_tm1 = []\n",
    "    all_node_dv = []\n",
    "    all_node_dx = []\n",
    "    \n",
    "    # Process all valid timesteps\n",
    "    for batch in train_loader:\n",
    "        batched_graph = batch\n",
    "        \n",
    "        senders,receivers = batched_graph.edge_index\n",
    "\n",
    "        edge_dx = batched_graph.pos[receivers] - batched_graph.pos[senders]\n",
    "        \n",
    "        # Extract positions and velocities\n",
    "        node_vel_t = batched_graph.vel.float()\n",
    "        node_vel_tm1 = batched_graph.prev_vel.float()\n",
    "        \n",
    "        # Calculate displacements and acceleration changes\n",
    "        node_dv = (batched_graph.y_dv).float()\n",
    "        node_dx = (batched_graph.y_dx).float()\n",
    "        \n",
    "        # Collect data\n",
    "        all_edge_dx.append(edge_dx)\n",
    "        all_node_v_t.append(node_vel_t)\n",
    "        all_node_v_tm1.append(node_vel_tm1)\n",
    "        all_node_dv.append(node_dv)\n",
    "        all_node_dx.append(node_dx)\n",
    "    \n",
    "    # Concatenate all collected data\n",
    "    all_edge_dx = torch.cat(all_edge_dx, dim=0)\n",
    "    all_node_v_t = torch.cat(all_node_v_t, dim=0)\n",
    "    all_node_v_tm1 = torch.cat(all_node_v_tm1, dim=0)\n",
    "    all_node_dv= torch.cat(all_node_dv, dim=0)\n",
    "    all_node_dx = torch.cat(all_node_dx,dim=0)\n",
    "    \n",
    "    # Compute norms\n",
    "    norm_edge_dx = all_edge_dx.norm(dim=1)\n",
    "    norm_node_v_t = all_node_v_t.norm(dim=1)\n",
    "    norm_node_v_tm1 = all_node_v_tm1.norm(dim=1)\n",
    "    norm_node_dv = all_node_dv.norm(dim=1)\n",
    "    norm_node_dx = all_node_dx.norm(dim=1)\n",
    "    \n",
    "    # Compute min and max values of the norms\n",
    "    min_edge_dx = norm_edge_dx.min()\n",
    "    max_edge_dx = norm_edge_dx.max()\n",
    "\n",
    "    min_node_v_t = norm_node_v_t.min()\n",
    "    max_node_v_t = norm_node_v_t.max()\n",
    "\n",
    "    min_node_v_tm1 = norm_node_v_tm1.min()\n",
    "    max_node_v_tm1 = norm_node_v_tm1.max()\n",
    "\n",
    "    mean_node_dv = norm_node_dv.mean()\n",
    "    std_node_dv = norm_node_dv.std()\n",
    "\n",
    "    mean_node_dx = norm_node_dx.mean()\n",
    "    std_node_dx = norm_node_dx.std()\n",
    "\n",
    "    # Collect statistics in tuples\n",
    "    stat_edge_dx = (min_edge_dx, max_edge_dx)\n",
    "    stat_node_v_t = (min_node_v_t, max_node_v_t)\n",
    "    stat_node_v_tm1 = (min_node_v_tm1, max_node_v_tm1)\n",
    "    stat_node_dv = (mean_node_dv, std_node_dv)\n",
    "    stat_node_dx = (mean_node_dx, std_node_dx)\n",
    "    \n",
    "    return stat_edge_dx, stat_node_v_t, stat_node_dv, stat_node_dx\n",
    "\n",
    "def move_train_stats_to_device(train_stats, device):\n",
    "    def move_to_device(stat):\n",
    "        if len(stat)==2:\n",
    "            min_val, max_val = stat\n",
    "            return min_val.to(device), max_val.to(device)\n",
    "        else: \n",
    "            max_val = stat\n",
    "            return max_val.to(device)\n",
    "        \n",
    "\n",
    "    return tuple(move_to_device(stat) for stat in train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13699dea",
   "metadata": {},
   "source": [
    "## Dynamical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4da637d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = calculate_min_max_edge(train_loader)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_stats = move_train_stats_to_device(stats, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b93cc",
   "metadata": {},
   "source": [
    "### Components for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ebc83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_d(in_size, hidden_size, out_size, num_layers=1, lay_norm=True, use_sigmoid=False, use_softmax=False):\n",
    "    \"\"\"\n",
    "    Builds a multi-layer perceptron (MLP) with configurable depth and optional layer normalization and sigmoid or softmax activation.\n",
    "\n",
    "    Args:\n",
    "        in_size (int): The size of the input feature vector.\n",
    "        hidden_size (int): The size of the hidden layers.\n",
    "        out_size (int): The size of the output layer.\n",
    "        num_layers (int): The number of layers in the MLP.\n",
    "        lay_norm (bool): Flag to add layer normalization after the last linear layer.\n",
    "        use_sigmoid (bool): Flag to add a sigmoid activation layer at the output.\n",
    "        use_softmax (bool): Flag to add a softmax activation layer at the output.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The constructed MLP model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If both use_sigmoid and use_softmax are True.\n",
    "    \"\"\"\n",
    "    if use_sigmoid and use_softmax:\n",
    "        raise ValueError(\"Only one of use_sigmoid or use_softmax can be true.\")\n",
    "    layers = [nn.Linear(in_size, hidden_size), nn.ReLU()]\n",
    "    \n",
    "    # Add intermediate layers\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "    # Add the output layer\n",
    "    layers.append(nn.Linear(hidden_size, out_size))\n",
    "\n",
    "    # Create the MLP module\n",
    "    module = nn.Sequential(*layers)\n",
    "\n",
    "    # Optionally add layer normalization\n",
    "    if lay_norm:\n",
    "        module = nn.Sequential(module, nn.LayerNorm(normalized_shape=out_size))\n",
    "\n",
    "    # Optionally add sigmoid activation\n",
    "    if use_sigmoid:\n",
    "        module = nn.Sequential(module, nn.Sigmoid())\n",
    "\n",
    "    # Optionally add softmax activation\n",
    "    if use_softmax:\n",
    "        module = nn.Sequential(module, nn.Softmax(dim=-1))\n",
    "\n",
    "    return module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18ebb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefFrameCalc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefFrameCalc, self).__init__()\n",
    "\n",
    "    def forward(self, edge_index,senders_pos,receivers_pos, senders_vel,receivers_vel, senders_omega, receivers_omega):\n",
    "        \n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        epsilon = 1e-8\n",
    "\n",
    "        vector_a = (receivers_pos - senders_pos)/ torch.clamp((receivers_pos - senders_pos).norm(dim=1, keepdim=True), min=epsilon)\n",
    "\n",
    "        #prelimnary vectors\n",
    "        b_a = torch.cross(receivers_vel-senders_vel,vector_a)\n",
    "        b_a = b_a / torch.clamp(b_a.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        b_c = (senders_vel + receivers_vel)\n",
    "        b_c = b_c / torch.clamp(b_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        \n",
    "        b_a_ = torch.cross(receivers_omega-senders_omega,vector_a)\n",
    "        b_a_ = b_a_ / torch.clamp(b_a_.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        b_c_ = (senders_omega + receivers_omega)\n",
    "        b_c_ = b_c_ / torch.clamp(b_c_.norm(dim=1, keepdim=True), min=epsilon)\n",
    "\n",
    "        b = b_a + b_c + b_a_ + b_c_ \n",
    "\n",
    "        # Compute the parallel component of b\n",
    "        b_prl_dot = torch.einsum('ij,ij->i', b, vector_a).unsqueeze(1)\n",
    "        b_prl = b_prl_dot * vector_a\n",
    "\n",
    "        # Compute the perpendicular component of b\n",
    "        b_prp = b - b_prl\n",
    "\n",
    "        vector_b = torch.cross(b_prp, vector_a) #perp to a and a new vector b_prp\n",
    "        vector_c = torch.cross(b_prl, vector_b) #perp to a and b\n",
    "        \n",
    "        vector_b = vector_b / torch.clamp(vector_b.norm(dim=1, keepdim=True), min=epsilon)\n",
    "        vector_c = vector_c / torch.clamp(vector_c.norm(dim=1, keepdim=True), min=epsilon)\n",
    "   \n",
    "        return vector_a, vector_b, vector_c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12582806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.node_encoder = build_mlp_d(2, latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "    def forward(self,node_scalar_feat):\n",
    "        node_latent = self.node_encoder(node_scalar_feat)  \n",
    "        return node_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "679e5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionEncoder(nn.Module):\n",
    "    \"\"\"Message passing.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size):\n",
    "        super(InteractionEncoder, self).__init__()\n",
    "        self.edge_feat_encoder = build_mlp_d(12, latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "        self.edge_encoder = build_mlp_d(3, latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "        self.interaction_encoder = build_mlp_d(3*latent_size,latent_size, latent_size, num_layers=2, lay_norm=True)\n",
    "    def forward(self, edge_index, edge_dx_,edge_attr, vector_a, vector_b, vector_c,\n",
    "                senders_v_t_, senders_v_tm1_, senders_w_t_,senders_th_t_,\n",
    "                receivers_v_t_, receivers_v_tm1_, receivers_w_t_,receivers_th_t_,\n",
    "                node_latent):\n",
    "\n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        node_v_t_senders_a = torch.einsum('ij,ij->i', senders_v_t_, vector_a).unsqueeze(1)\n",
    "        node_v_t_senders_b = torch.einsum('ij,ij->i', senders_v_t_, vector_b).unsqueeze(1)\n",
    "        node_v_t_senders_c = torch.einsum('ij,ij->i', senders_v_t_, vector_c).unsqueeze(1)\n",
    "\n",
    "        node_v_tm1_senders_a = torch.einsum('ij,ij->i', senders_v_tm1_, vector_a).unsqueeze(1)\n",
    "        node_v_tm1_senders_b = torch.einsum('ij,ij->i', senders_v_tm1_, vector_b).unsqueeze(1)\n",
    "        node_v_tm1_senders_c = torch.einsum('ij,ij->i', senders_v_tm1_, vector_c).unsqueeze(1)    \n",
    "\n",
    "        node_w_t_senders_a = torch.einsum('ij,ij->i', senders_w_t_, vector_a).unsqueeze(1)\n",
    "        node_w_t_senders_b = torch.einsum('ij,ij->i', senders_w_t_, vector_b).unsqueeze(1)\n",
    "        node_w_t_senders_c = torch.einsum('ij,ij->i', senders_w_t_, vector_c).unsqueeze(1)\n",
    "        \n",
    "        node_th_t_senders_a = torch.einsum('ij,ij->i', senders_th_t_, vector_a).unsqueeze(1)\n",
    "        node_th_t_senders_b = torch.einsum('ij,ij->i', senders_th_t_, vector_b).unsqueeze(1)\n",
    "        node_th_t_senders_c = torch.einsum('ij,ij->i', senders_th_t_, vector_c).unsqueeze(1)   \n",
    "        \n",
    "        node_v_t_receivers_a = torch.einsum('ij,ij->i', receivers_v_t_, -vector_a).unsqueeze(1)\n",
    "        node_v_t_receivers_b = torch.einsum('ij,ij->i', receivers_v_t_, -vector_b).unsqueeze(1)\n",
    "        node_v_t_receivers_c = torch.einsum('ij,ij->i', receivers_v_t_, -vector_c).unsqueeze(1)\n",
    "\n",
    "        node_v_tm1_receivers_a = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_a).unsqueeze(1)\n",
    "        node_v_tm1_receivers_b = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_b).unsqueeze(1)\n",
    "        node_v_tm1_receivers_c = torch.einsum('ij,ij->i',receivers_v_tm1_, -vector_c).unsqueeze(1)       \n",
    "        \n",
    "        node_w_t_receivers_a = torch.einsum('ij,ij->i', receivers_w_t_, -vector_a).unsqueeze(1)\n",
    "        node_w_t_receivers_b = torch.einsum('ij,ij->i', receivers_w_t_, -vector_b).unsqueeze(1)\n",
    "        node_w_t_receivers_c = torch.einsum('ij,ij->i', receivers_w_t_, -vector_c).unsqueeze(1)\n",
    "\n",
    "        node_th_t_receivers_a = torch.einsum('ij,ij->i', receivers_th_t_, -vector_a).unsqueeze(1)\n",
    "        node_th_t_receivers_b = torch.einsum('ij,ij->i', receivers_th_t_, -vector_b).unsqueeze(1)\n",
    "        node_th_t_receivers_c = torch.einsum('ij,ij->i', receivers_th_t_, -vector_c).unsqueeze(1)\n",
    "        \n",
    "        edge_dx_a_s = edge_dx_.norm(dim=1,keepdim=True)\n",
    "\n",
    "        senders_features = torch.hstack((\n",
    "            node_v_t_senders_a, node_v_t_senders_b, node_v_t_senders_c,\n",
    "            node_v_tm1_senders_a, node_v_tm1_senders_b, node_v_tm1_senders_c,\n",
    "            node_w_t_senders_a, node_w_t_senders_b, node_w_t_senders_c,\n",
    "            node_th_t_senders_a, node_th_t_senders_b, node_th_t_senders_c\n",
    "        ))\n",
    "\n",
    "        receivers_features = torch.hstack((\n",
    "            node_v_t_receivers_a, node_v_t_receivers_b, node_v_t_receivers_c,\n",
    "            node_v_tm1_receivers_a, node_v_tm1_receivers_b, node_v_tm1_receivers_c,\n",
    "            node_w_t_receivers_a, node_w_t_receivers_b, node_w_t_receivers_c,\n",
    "            node_th_t_receivers_a, node_th_t_receivers_b, node_th_t_receivers_c\n",
    "        ))\n",
    "        \n",
    "        edge_latent = self.edge_encoder(torch.hstack((edge_dx_a_s, edge_attr)))\n",
    "\n",
    "        senders_latent = self.edge_feat_encoder(senders_features)\n",
    "        receivers_latent = self.edge_feat_encoder(receivers_features)\n",
    "\n",
    "        interaction_latent = self.interaction_encoder(torch.hstack((senders_latent + receivers_latent,\n",
    "                                                                    node_latent[senders]+node_latent[receivers],\n",
    "                                                                    edge_latent)))\n",
    "\n",
    "        return interaction_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "368fce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionDecoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(InteractionDecoder, self).__init__()\n",
    "        self.i1_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=2, lay_norm=False)\n",
    "        self.i2_decoder = build_mlp_d(latent_size, latent_size, 3, num_layers=2, lay_norm=False)\n",
    "\n",
    "    def forward(self, edge_index, senders_pos, receivers_pos, vector_a, vector_b, vector_c, interaction_latent, node_latent):\n",
    "        senders, receivers = edge_index\n",
    "\n",
    "        coeff_f = self.i1_decoder(interaction_latent)\n",
    "        coeff_t = self.i2_decoder(interaction_latent)\n",
    "\n",
    "        fij = (coeff_f[:, 0:1] * vector_a + \n",
    "              coeff_f[:, 1:2] * vector_b + \n",
    "              coeff_f[:, 2:] * vector_c)\n",
    "        \n",
    "        \n",
    "        tij = (coeff_t[:, 0:1] * vector_a + \n",
    "              coeff_t[:, 1:2] * vector_b + \n",
    "              coeff_t[:, 2:] * vector_c)        \n",
    "        return fij, tij\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a29b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_Internal_Dv_Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super(Node_Internal_Dv_Decoder, self).__init__()\n",
    "        self.m_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=2, lay_norm=False)\n",
    "        self.i_inv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=2, lay_norm=False)\n",
    "        self.f_ext_dv_decoder = build_mlp_d(latent_size, latent_size, 1, num_layers=2, lay_norm=False)\n",
    "    def forward(self,edge_index,node_latent,fij,tij):\n",
    "        m_inv = self.m_inv_decoder(node_latent) # decode inverse of mass\n",
    "        i_inv = self.i_inv_decoder(node_latent) # decode inverse of inertia\n",
    "        senders,receivers = edge_index   \n",
    "        \n",
    "        out_fij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_fij = out_fij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]).to(device), fij.to(device))\n",
    "        node_dv_int = m_inv * out_fij + self.f_ext_dv_decoder(node_latent)\n",
    "        \n",
    "        out_tij = torch.zeros((node_latent.shape[0], fij.shape[1])).to(device)\n",
    "        out_tij = out_tij.scatter_add(0, receivers.unsqueeze(1).expand(-1, fij.shape[1]).to(device), tij.to(device))\n",
    "        node_dw_int = i_inv * out_tij       \n",
    "\n",
    "        return node_dv_int, node_dw_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70559816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Scaler, self).__init__()\n",
    "        '''\n",
    "        Scales the velocity and angular velocity features by maximum magnitude of respective field in training data\n",
    "        Scales the magnitude of the edge_vector_dx using min-max scaling. (keeping the direction of edge_vector_dx same)\n",
    "        '''\n",
    "\n",
    "    def forward(self, senders_v_t, senders_v_tm1,receivers_v_t, receivers_v_tm1,edge_dx,train_stats):\n",
    "        stat_edge_dx, stat_node_v_t, _,_= train_stats\n",
    "        \n",
    "        senders_v_t_ = senders_v_t/stat_node_v_t[1].detach()\n",
    "        senders_v_tm1_ = senders_v_tm1/stat_node_v_t[1].detach()\n",
    "        receivers_v_t_ = receivers_v_t/stat_node_v_t[1].detach()\n",
    "        receivers_v_tm1_ = receivers_v_tm1/stat_node_v_t[1].detach()\n",
    "        norm_edge_dx = edge_dx.norm(dim=1, keepdim=True)\n",
    "        edge_dx_ = (((norm_edge_dx-stat_edge_dx[0])/(stat_edge_dx[1]-stat_edge_dx[0]))*(edge_dx/norm_edge_dx)).detach()\n",
    "        return senders_v_t_, senders_v_tm1_,receivers_v_t_, receivers_v_tm1_,edge_dx_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60303a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interaction_Block(torch.nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Interaction_Block, self).__init__()\n",
    "        self.interaction_encoder = InteractionEncoder(latent_size)\n",
    "        self.interaction_decoder = InteractionDecoder(latent_size)\n",
    "        self.internal_dv_decoder = Node_Internal_Dv_Decoder(latent_size)\n",
    "        self.layer_norm = nn.LayerNorm(latent_size)\n",
    "\n",
    "    def forward(self, edge_index, senders_pos, receivers_pos, edge_dx_, edge_attr,vector_a, vector_b, vector_c, \n",
    "                senders_v_t_, senders_v_tm1_, senders_w_t_,senders_th_t_,\n",
    "                receivers_v_t_, receivers_v_tm1_, receivers_w_t_,receivers_th_t_,\n",
    "                node_latent, residue=None, latent_history=False):\n",
    "            interaction_latent = self.interaction_encoder(edge_index, edge_dx_,edge_attr,\n",
    "                                                          vector_a, vector_b, vector_c,\n",
    "                                                          senders_v_t_, senders_v_tm1_, senders_w_t_,senders_th_t_,\n",
    "                                                          receivers_v_t_, receivers_v_tm1_, receivers_w_t_,receivers_th_t_,\n",
    "                                                          node_latent)\n",
    "\n",
    "            if latent_history:\n",
    "                interaction_latent = interaction_latent + residue\n",
    "                interaction_latent = self.layer_norm(interaction_latent)\n",
    "            \n",
    "            edge_interaction_force, edge_interaction_tau= self.interaction_decoder(\n",
    "                edge_index, senders_pos, receivers_pos, vector_a, vector_b, vector_c, interaction_latent, node_latent\n",
    "            )\n",
    "            node_dv_int_decoded, node_dw_int_decoded = self.internal_dv_decoder(\n",
    "                edge_index, node_latent, edge_interaction_force, edge_interaction_tau\n",
    "            )\n",
    "        \n",
    "            return node_dv_int_decoded, node_dw_int_decoded, interaction_latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f536404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsSolver(torch.nn.Module):\n",
    "    def __init__(self, sample_step, train_stats, num_jumps=1, num_msgs=1, latent_size=128):\n",
    "        super(DynamicsSolver, self).__init__()\n",
    "        self.refframecalc = RefFrameCalc()\n",
    "        self.scaler = Scaler()\n",
    "        self.node_encoder = NodeEncoder(latent_size)\n",
    "        self.interaction_proc_layer = Interaction_Block(latent_size)\n",
    "        self.interaction_init_layer = Interaction_Block(latent_size)\n",
    "        self.num_messages = num_msgs\n",
    "        self.sub_tstep = sample_step / num_msgs\n",
    "        self.train_stats = train_stats\n",
    "\n",
    "    def forward(self, graph):\n",
    "        # Initialize graph data for processing\n",
    "        device = graph.pos.device\n",
    "        graph = graph.to(device)\n",
    "        node_type = graph.node_type.float()\n",
    "        pos = graph.pos.float()\n",
    "        vel = graph.vel.float()\n",
    "        prev_vel = graph.prev_vel.float()\n",
    "        \n",
    "        edge_index = graph.edge_index.long()\n",
    "        senders, receivers = edge_index\n",
    "        senders_pos = pos[senders]\n",
    "        receivers_pos = pos[receivers]\n",
    "        edge_dx = receivers_pos - senders_pos\n",
    "        edge_attr = graph.edge_attr.float()\n",
    "        \n",
    "        mask_non_refl = (graph.node_type!=2).squeeze()\n",
    "\n",
    "        node_v_t = vel\n",
    "        node_w_t = getattr(graph, 'node_w_t',torch.zeros_like(node_v_t))\n",
    "        node_th_t = getattr(graph, 'node_th_t', torch.zeros_like(node_v_t))\n",
    "\n",
    "        senders_v_t = node_v_t[senders].float()\n",
    "        receivers_v_t = node_v_t[receivers].float()\n",
    "\n",
    "        senders_v_tm1 = prev_vel[senders].float()\n",
    "        receivers_v_tm1 = prev_vel[receivers].float()\n",
    "\n",
    "        senders_w_t = node_w_t[senders]\n",
    "        receivers_w_t = node_w_t[receivers]\n",
    "        \n",
    "        senders_th_t = node_th_t[senders]\n",
    "        receivers_th_t = node_th_t[receivers]        \n",
    "\n",
    "        node_disp = torch.zeros_like(node_v_t)\n",
    "        node_vf = torch.zeros_like(node_v_t)\n",
    "        node_wf = torch.zeros_like(node_v_t)\n",
    "\n",
    "        sum_node_dv = torch.zeros_like(node_v_t)\n",
    "        sum_node_dx = torch.zeros_like(node_v_t)\n",
    "        \n",
    "        node_latent = self.node_encoder(torch.hstack((node_type,pos[:,1:2])))\n",
    "\n",
    "\n",
    "        for i in range(self.num_messages):\n",
    "            (\n",
    "                senders_v_t_,\n",
    "                senders_v_tm1_,\n",
    "                receivers_v_t_,\n",
    "                receivers_v_tm1_,\n",
    "                edge_dx_,\n",
    "            ) = self.scaler(\n",
    "                senders_v_t,\n",
    "                senders_v_tm1,\n",
    "                receivers_v_t,\n",
    "                receivers_v_tm1,\n",
    "                edge_dx,\n",
    "                self.train_stats,\n",
    "            )\n",
    "\n",
    "\n",
    "            vector_a, vector_b, vector_c = self.refframecalc(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    senders_v_t_,\n",
    "                    receivers_v_t_,\n",
    "                    senders_w_t,\n",
    "                    receivers_w_t\n",
    "                    \n",
    "                )\n",
    "\n",
    "            if i == 0:\n",
    "                node_dv_int_decoded, node_dw_int_decoded,residue= self.interaction_init_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    senders_w_t,\n",
    "                    senders_th_t,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    receivers_w_t,\n",
    "                    receivers_th_t,\n",
    "                    node_latent,\n",
    "                    latent_history=False,\n",
    "                )\n",
    "            else:\n",
    "                node_dv_int_decoded, node_dw_int_decoded, residue = self.interaction_proc_layer(\n",
    "                    edge_index,\n",
    "                    senders_pos,\n",
    "                    receivers_pos,\n",
    "                    edge_dx_,\n",
    "                    edge_attr,\n",
    "                    vector_a,\n",
    "                    vector_b,\n",
    "                    vector_c,\n",
    "                    senders_v_t_,\n",
    "                    senders_v_tm1_,\n",
    "                    senders_w_t,\n",
    "                    senders_th_t,\n",
    "                    receivers_v_t_,\n",
    "                    receivers_v_tm1_,\n",
    "                    receivers_w_t,\n",
    "                    receivers_th_t,\n",
    "                    node_latent,\n",
    "                    residue=residue,\n",
    "                    latent_history=True,\n",
    "                )\n",
    "\n",
    "            sum_node_dv [mask_non_refl]= sum_node_dv [mask_non_refl] + node_dv_int_decoded[mask_non_refl]\n",
    "                \n",
    "            node_vf[mask_non_refl]= node_v_t[mask_non_refl] + node_dv_int_decoded[mask_non_refl]\n",
    "            node_wf[mask_non_refl]= node_w_t[mask_non_refl] + node_dw_int_decoded[mask_non_refl]\n",
    "\n",
    "            node_disp= (\n",
    "                (node_v_t + node_vf) * 0.5 * self.sub_tstep\n",
    "            )\n",
    "\n",
    "            node_th_t = node_th_t + (node_wf + node_w_t)* 0.5 * self.sub_tstep\n",
    "\n",
    "            sum_node_dx [mask_non_refl]=sum_node_dx [mask_non_refl]+ ((node_v_t + node_vf) * 0.5 * self.sub_tstep)[mask_non_refl]\n",
    "\n",
    "            senders_disp = node_disp[senders]\n",
    "            receivers_disp = node_disp[receivers]\n",
    "\n",
    "            senders_pos = senders_disp + senders_pos\n",
    "            receivers_pos = receivers_disp + receivers_pos\n",
    "\n",
    "            node_v_tm1 = node_v_t.clone()\n",
    "\n",
    "            node_v_t = node_vf.clone()\n",
    "            node_w_t = node_wf.clone()\n",
    "\n",
    "            senders_v_tm1 = senders_v_t.clone()\n",
    "            senders_v_t = node_v_t[senders].clone()\n",
    "            senders_w_t = node_w_t[senders].clone()\n",
    "            senders_th_t = node_th_t[senders].clone()\n",
    "\n",
    "            receivers_v_tm1 = receivers_v_t.clone()\n",
    "            receivers_v_t = node_v_t[receivers].clone()\n",
    "            receivers_w_t = node_w_t[receivers].clone()\n",
    "            receivers_th_t = node_th_t[receivers].clone()\n",
    "\n",
    "            edge_dx = receivers_pos - senders_pos\n",
    "        return sum_node_dv[mask_non_refl],sum_node_dx[mask_non_refl],node_v_tm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6cd859dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model, device, mode='test', plot=False, frequency=1, model_name='PIGNN', experiment_name='Val'):\n",
    "    with torch.no_grad():\n",
    "        res = 0.\n",
    "        res_counter = 0\n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            for i in range(1):  # Iterates 10 times: 0 -> predicts 31, ..., 9 -> predicts 40\n",
    "                if i == 0:\n",
    "                    graph_t0 = test_batch.to(device)\n",
    "                    # Clone to ensure ground truth remains unchanged\n",
    "                    end_pos = graph_t0.end_pos.clone()  \n",
    "                    mask_non_refl = (graph_t0.node_type!=2).squeeze()\n",
    "                new_vel = torch.zeros_like(graph_t0.vel)\n",
    "                new_pos = graph_t0.pos\n",
    "                node_dv,node_dx,_= model(graph_t0.detach())\n",
    "                new_vel[mask_non_refl] =  graph_t0.vel[mask_non_refl] + node_dv\n",
    "                new_pos[mask_non_refl] =  graph_t0.pos[mask_non_refl] + node_dx\n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = graph_t0.vel.clone()\n",
    "                graph_t0.pos = new_pos.clone()\n",
    "                graph_t0.vel = new_vel.clone()\n",
    "\n",
    "            loss = F.mse_loss(new_pos[mask_non_refl], end_pos)\n",
    "            batch_size = graph_t0.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "\n",
    "        mean_pos_error = res / res_counter\n",
    "    return mean_pos_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc02010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, device, train_stats):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.train_history = []\n",
    "        self.extr_test_history = []\n",
    "        self.gen_test_history = []\n",
    "        self.cur_dir = os.getcwd()\n",
    "        self.model_dir = os.path.join(self.cur_dir, 'saved_models')\n",
    "        self.gen_loss = 0.\n",
    "        self.train_stats = train_stats\n",
    "        self.mean_node_dv = self.train_stats[2][0]\n",
    "        self.mean_node_disp = self.train_stats[3][0]\n",
    "        self.std_node_dv = self.train_stats[2][1]\n",
    "        self.std_node_disp = self.train_stats[3][1]\n",
    "        # Initialize best test loss and best epoch.\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = None        \n",
    "\n",
    "    def train(self, train_graph):\n",
    "        self.model.train()\n",
    "        pred_node_dvel,pred_node_disp,_ = self.model(train_graph.to(self.device))\n",
    "\n",
    "        actual_node_dvel = (train_graph.y_dv).float().to(self.device)\n",
    "        actual_node_disp = (train_graph.y_dx).float().to(self.device)\n",
    "\n",
    "        pred_node_dvel_ = (pred_node_dvel-self.mean_node_dv.detach())/self.std_node_dv.detach()\n",
    "        pred_node_disp_ = (pred_node_disp-self.mean_node_disp.detach())/self.std_node_disp.detach()\n",
    "        actual_node_dvel_ = (actual_node_dvel - self.mean_node_dv.detach())/self.std_node_dv.detach()\n",
    "        actual_node_disp_ = (actual_node_disp-self.mean_node_disp.detach())/self.std_node_disp.detach()\n",
    "\n",
    "        loss = F.mse_loss(pred_node_disp_,actual_node_disp_) + F.mse_loss(pred_node_dvel_,actual_node_dvel_)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.loss = loss.cpu().detach().numpy()\n",
    "        self.train_history.append(self.loss)\n",
    "    def test(self, test_loader, mode='val', epoch=None):\n",
    "        self.model.eval()\n",
    "        mean_pos_error = evaluate(test_loader,\n",
    "                            self.model,\n",
    "                            self.device)\n",
    "        \n",
    "        \n",
    "        if mode == 'val':\n",
    "            self.val_loss_pos = mean_pos_error\n",
    "            self.gen_test_history.append(self.val_loss_pos)\n",
    "            # Update best test loss and best epoch if the current loss is lower\n",
    "            if epoch is not None and self.val_loss_pos < self.best_val_loss:\n",
    "                self.best_val_loss = self.val_loss_pos\n",
    "                self.best_epoch = epoch\n",
    "                self.save_model(epoch)\n",
    "        if mode == 'test':\n",
    "            self.test_loss_pos = mean_pos_error\n",
    "\n",
    "    def save_model(self,iteration):\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "        self.path = os.path.join(self.model_dir, \n",
    "                                 f'GenLoss_{self.val_loss_pos:.5f}mm_iter{iteration}.pth')\n",
    "        torch.save(self.model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3b6dbe9-823b-457f-b6b1-4653ebfe9036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE_TIME_STEP =  30.0\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "if model_settings[\"finite_diff\"]:\n",
    "    SAMPLE_TIME_STEP = 30*model_settings[\"time_step\"]\n",
    "else:\n",
    "    SAMPLE_TIME_STEP = 30*model_settings[\"time_step_actual\"]\n",
    "print(f'SAMPLE_TIME_STEP = ',SAMPLE_TIME_STEP)\n",
    "N= 1\n",
    "MODEL = DynamicsSolver(SAMPLE_TIME_STEP, train_stats, num_jumps=N, num_msgs=5, latent_size=64)\n",
    "optimizer = torch.optim.Adam(MODEL.parameters(), lr = 5e-4)\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def ensure_model_on_device(model, device):\n",
    "    for module in model.modules():\n",
    "        # Check if any parameter of the module is not on the desired device\n",
    "        if any(p.device != device for p in module.parameters()):\n",
    "            # Move the entire module to the device\n",
    "            module.to(device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ensure_model_on_device(MODEL, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9070b11-5732-452a-a2bc-5cd6b4b95c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(MODEL, \n",
    "                  optimizer, \n",
    "                  device,\n",
    "                  train_stats\n",
    "                 )\n",
    "# Clear PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "312bbfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  73%|███████▎  | 1680/2300 [25:09<09:17,  1.11it/s, TrainLoss (*1e3)=4.07719e+01, ValLoss=2.48670158e-01, BestVal=2.27041e-01(Ep1595), TestLoss=1.93546104e-01, LR=5.00e-04]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# ——— Training ———\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 14\u001b[0m         \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# ——— Validation & Test ———\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# trainer.test now returns the mean positional error\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_graph)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_graph):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 23\u001b[0m     pred_node_dvel,pred_node_disp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     actual_node_dvel \u001b[38;5;241m=\u001b[39m (train_graph\u001b[38;5;241m.\u001b[39my_dv)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m     actual_node_disp \u001b[38;5;241m=\u001b[39m (train_graph\u001b[38;5;241m.\u001b[39my_dx)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/gnn/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[23], line 148\u001b[0m, in \u001b[0;36mDynamicsSolver.forward\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    145\u001b[0m senders_pos \u001b[38;5;241m=\u001b[39m senders_disp \u001b[38;5;241m+\u001b[39m senders_pos\n\u001b[1;32m    146\u001b[0m receivers_pos \u001b[38;5;241m=\u001b[39m receivers_disp \u001b[38;5;241m+\u001b[39m receivers_pos\n\u001b[0;32m--> 148\u001b[0m node_v_t \u001b[38;5;241m=\u001b[39m \u001b[43mnode_vf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m node_w_t \u001b[38;5;241m=\u001b[39m node_wf\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    151\u001b[0m senders_v_tm1 \u001b[38;5;241m=\u001b[39m senders_v_t\u001b[38;5;241m.\u001b[39mclone()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 2300\n",
    "\n",
    "# initialize to large values\n",
    "val_loss = np.inf\n",
    "test_loss = np.inf\n",
    "\n",
    "# wrap the epoch loop in tqdm\n",
    "with tqdm(range(1, epochs+1), desc='Training') as pbar:\n",
    "    for epoch in pbar:\n",
    "        # ——— Training ———\n",
    "        for train_batch in train_loader:\n",
    "            trainer.train(train_batch)\n",
    "\n",
    "        # ——— Validation & Test ———\n",
    "        # trainer.test now returns the mean positional error\n",
    "        if epoch%5==0:\n",
    "            trainer.test(val_loader,  mode='val',  epoch=epoch)\n",
    "            val_loss = trainer.val_loss_pos\n",
    "            trainer.test(test_loader, mode='test')\n",
    "            test_loss = trainer.test_loss_pos\n",
    "\n",
    "        # ——— Learning rate ———\n",
    "        current_lr = trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # ——— Update the progress bar ———\n",
    "        pbar.set_postfix({\n",
    "            'TrainLoss (*1e3)': f'{trainer.loss * 1e3:.5e}',\n",
    "            'ValLoss':           f'{val_loss:.8e}',\n",
    "            'BestVal':           f'{trainer.best_val_loss:.5e}(Ep{trainer.best_epoch})',\n",
    "            'TestLoss':          f'{test_loss:.8e}',\n",
    "            'LR':                f'{current_lr:.2e}',\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a029910e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc92d5b7010>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainer.train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560e0ef6",
   "metadata": {},
   "source": [
    "## Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f200b9b",
   "metadata": {},
   "source": [
    "This next cell has the functions that will, given a frame of the human, use the model to predict eventual position of the human after 30 time steps. It then also gives some sample animations of the predictions alongside the ground truth of the final position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1529bd0",
   "metadata": {},
   "source": [
    "### Main loop that runs the test rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f32a7e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best file: GenLoss_0.22704mm_iter1595.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Regular expression pattern to extract loss and epoch\n",
    "pattern = r'GenLoss_(\\d+\\.\\d+)mm_iter(\\d+)\\.pth'\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_epoch = None\n",
    "best_file = None\n",
    "for fn in os.listdir(trainer.model_dir):\n",
    "    m = re.search(pattern, fn)\n",
    "    if m:\n",
    "        l = float(m.group(1))\n",
    "        if l < best_loss:\n",
    "            best_loss = l\n",
    "            best_epoch = int(m.group(2))\n",
    "            best_file = fn\n",
    "\n",
    "if best_file is None:\n",
    "    raise RuntimeError(\"No checkpoint found!\")\n",
    "path = os.path.join(trainer.model_dir, best_file)\n",
    "trainer.model.load_state_dict(torch.load(path))\n",
    "# Print the best result in the desired format.\n",
    "print(f'Best file: {best_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a73d106-a728-4eb9-bbe6-cd5d604b1cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22704078753789267\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "trainer.test(val_loader, mode='val')\n",
    "print(trainer.val_loss_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a2b3f5fe-984f-4a1f-8c4b-c67ff7f2b100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17131339510281882\n"
     ]
    }
   ],
   "source": [
    "trainer.model.eval()\n",
    "trainer.test(test_loader, mode='val')\n",
    "print(trainer.val_loss_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de2f23f8-8495-40d6-8166-ccd767407a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflected_nodes_eval(nodes, z0=0.0, epsilon=1e-3):\n",
    "    # nodes: torch.Tensor [n,3]\n",
    "    reflected = nodes.clone()\n",
    "    reflected[:,1] = 2*z0 - nodes[:,1] - epsilon\n",
    "    distances = reflected[:,1] - nodes[:,1]\n",
    "    return reflected, distances\n",
    "\n",
    "def find_min(nodes):\n",
    "    return torch.tensor(np.min(nodes[:,1:2].detach().cpu().numpy(), axis=0)).to(nodes.device)\n",
    "\n",
    "\n",
    "def evaluate_rollout(test_loader, model, device, nsteps=1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        res = 0.\n",
    "        res_counter = 0\n",
    "\n",
    "        for test_batch in test_loader:\n",
    "            for i in range(nsteps):  # Iterates 10 times: 0 -> predicts 31, ..., 9 -> predicts 40\n",
    "                if i == 0:\n",
    "                    test_graph = test_batch.to(device)\n",
    "                    graph_t0 = test_graph\n",
    "                    # Clone to ensure ground truth remains unchanged\n",
    "                    end_pos = test_graph.end_pos.clone()  \n",
    "                    mask_foot = (graph_t0.node_type==1).squeeze()\n",
    "                    mask_refl = (graph_t0.node_type==2).squeeze()\n",
    "                    mask_non_refl = (graph_t0.node_type!=2).squeeze()\n",
    "\n",
    "                    zmin_0 = find_min(graph_t0.pos[mask_non_refl])\n",
    "                    \n",
    "                node_dv,node_dx,vm1= model(graph_t0.detach())\n",
    "                new_vel_body =  graph_t0.vel[mask_non_refl] + node_dv\n",
    "                new_pos_body =  graph_t0.pos[mask_non_refl] + node_dx\n",
    "                \n",
    "                # Get the foot node positions and indices\n",
    "                new_pos_full = torch.zeros_like(graph_t0.pos)\n",
    "                new_pos_full[mask_non_refl] = new_pos_body#_centered\n",
    "                new_foot_pos = new_pos_full[mask_foot]\n",
    "                zmin_t = find_min(new_pos_body)\n",
    "                new_foot_ref_pos, _ = reflected_nodes_eval(new_foot_pos,zmin_t) \n",
    "\n",
    "                graph_t0.prev_pos = graph_t0.pos.clone()\n",
    "                graph_t0.prev_vel = vm1\n",
    "                \n",
    "                graph_t0.pos[mask_non_refl] = new_pos_body#_centered\n",
    "                graph_t0.pos[mask_refl] = new_foot_ref_pos\n",
    "                graph_t0.vel[mask_non_refl] = new_vel_body.clone()\n",
    "                graph_t0.vel[mask_refl] = graph_t0.vel[mask_refl]*0.0\n",
    "\n",
    "            loss = F.mse_loss(new_pos_body, end_pos)\n",
    "            batch_size = test_graph.num_graphs\n",
    "            res += loss.item() * batch_size\n",
    "            res_counter += batch_size\n",
    "\n",
    "        mean_pos_error = res / res_counter\n",
    "    return mean_pos_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6af236bb-e575-4a07-9666-846ee1f07d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "loss for rollout 1 steps 0.17131340503692627\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "loss for rollout 2 steps 1.1794992685317993\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "loss for rollout 3 steps 2.986961603164673\n",
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "loss for rollout 4 steps 7.442751407623291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for nstep in [1,2,3,4]:\n",
    "    \n",
    "    dataset_eval = HumanDataset(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"],nsteps=nstep)\n",
    "    \n",
    "    dataloader_eval = create_dataloaders_from_raw(dataset_eval,200,shuffle=False)\n",
    "    \n",
    "    eval_error = evaluate_rollout(dataloader_eval, trainer.model, device, nsteps=nstep)\n",
    "    \n",
    "    print(f'loss for rollout {nstep} steps {eval_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de206cc7-9546-4862-9fd5-8552cf8a8993",
   "metadata": {},
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcd3477e-ebf7-453e-8489-fb0e7f0866df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanDatasetSeq(torch.utils.data.Dataset):\n",
    "    def __init__(self, partition='train', max_samples=600, data_dir='', nsteps=1):\n",
    "        self.partition = partition\n",
    "        self.data_dir = data_dir\n",
    "        self.nsteps = nsteps\n",
    "\n",
    "        # --- load raw data --------------------------------------\n",
    "        with open(os.path.join(data_dir, 'motion.pkl'), 'rb') as f:\n",
    "            edges, X = pkl.load(f)\n",
    "\n",
    "        # your smoothing / central_diff code here...\n",
    "        Ps, Vs, As = self.central_diff(X)\n",
    "\n",
    "        # trial IDs must match exactly\n",
    "        train_case_id = [20,1,17,13,14,9,4,2,7,5,16]\n",
    "        val_case_id   = [3,8,11,12,15,18]\n",
    "        test_case_id  = [6,19,21,0,22,10]\n",
    "\n",
    "        # --- load or create competitor splits (fixed for central_diff) ----------\n",
    "        split_path = os.path.join(data_dir, f'split_n{self.nsteps}.pkl')\n",
    "        try:\n",
    "            with open(split_path, 'rb') as f:\n",
    "                train_mapping, val_mapping, test_mapping = pkl.load(f)\n",
    "                print(\"Loaded competitor split!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Generating competitor split…\")\n",
    "\n",
    "            def make_map(case_ids):\n",
    "                mapping = {}\n",
    "                for i in case_ids:\n",
    "                    core_len = Ps[i].shape[0]                    # <<— use length after central_diff\n",
    "                    safe_max = core_len - self.nsteps*30 - 1\n",
    "                    if safe_max < 0:\n",
    "                        raise ValueError(f\"Trial {i} too short for look-ahead of {self.nsteps} steps.\")\n",
    "                    # competitor caps at 300\n",
    "                    itv = min(300, safe_max + 1)                # +1 because j in [0..safe_max]\n",
    "                    pool = np.arange(itv)                       # j ∈ [0..itv-1]\n",
    "                    mapping[i] = np.random.choice(pool, size=100, replace=False)\n",
    "                return mapping\n",
    "\n",
    "            train_mapping = make_map(train_case_id)\n",
    "            val_mapping   = make_map(val_case_id)\n",
    "            test_mapping  = make_map(test_case_id)\n",
    "\n",
    "            with open(split_path, 'wb') as f:\n",
    "                pkl.dump((train_mapping, val_mapping, test_mapping), f)\n",
    "            print(\"Saved competitor split!\")\n",
    "\n",
    "        # pick the mapping you need\n",
    "        if   partition == 'train': mapping = train_mapping\n",
    "        elif partition == 'val'  : mapping = val_mapping\n",
    "        elif partition == 'test' : mapping = test_mapping\n",
    "        else: raise ValueError(f\"Unknown partition {partition!r}\")\n",
    "\n",
    "        # now proceed exactly as before, using `mapping` instead of your make_mapping\n",
    "        each_len = max_samples // len(mapping)\n",
    "        in_graphs = []\n",
    "        for i, pool in mapping.items():\n",
    "            for j in pool[:each_len]:\n",
    "                # note: they use delta_frame; you have nsteps*30, so this is identical\n",
    "                cur_x_t   = Ps[i][j]\n",
    "                cur_v_t   = Vs[i][j]\n",
    "                cur_v_tm1 = Vs[i][j-1]\n",
    "                y_dv      = Vs[i][j + self.nsteps*30] - Vs[i][j]\n",
    "                y_dx      = Ps[i][j + self.nsteps*30] - Ps[i][j]\n",
    "                gt_seq = [ Ps[i][j + k*30] for k in range(self.nsteps+1) ]   # list of (31,3) arrays\n",
    "                y_pos_end = Ps[i][j + self.nsteps*30]\n",
    "                y_vel_end = Vs[i][j + self.nsteps*30]\n",
    "\n",
    "                in_graphs.append(self.create_in_graph(\n",
    "                    edges,\n",
    "                    x=(cur_x_t, cur_v_t, cur_v_tm1),\n",
    "                    y=(y_dv, y_dx, y_pos_end, y_vel_end),\n",
    "                    gt_seq = gt_seq\n",
    "                ))\n",
    "\n",
    "        self.in_graphs = in_graphs\n",
    "        print(f\"[HumanDataset:{partition}] built {len(in_graphs)} samples\")\n",
    "\n",
    "    def central_diff(self, Xs, dt: float = 1.0, window_length: int = 41):\n",
    "        Ps, Vs, As = [], [], []\n",
    "        for x in Xs:\n",
    "            v      = (x[2:] - x[:-2]) / (2*dt)\n",
    "            a      = (x[2:] - 2*x[1:-1] + x[:-2]) / (dt**2)\n",
    "            p      = x[1:-1]                      # align to v,a\n",
    "            Ps.append(p)\n",
    "            Vs.append(v)\n",
    "            As.append(a)\n",
    "        return Ps, Vs, As\n",
    "\n",
    "        \n",
    "    def get_foot_nodes(self, nodes):\n",
    "        foot_indices = np.argsort(nodes[:,1])[:6]\n",
    "        foot_pos = nodes[foot_indices]\n",
    "        return foot_pos, foot_indices\n",
    "    \n",
    "    def reflected_nodes(self, nodes, z0=0, epsilon=1e-3):\n",
    "        reflected = nodes.copy()\n",
    "        reflected[:,1] = 2*z0 - nodes[:,1] - epsilon\n",
    "        distances = reflected[:,1] - nodes[:,1]\n",
    "        return reflected, distances\n",
    "    \n",
    "    def find_min(self, nodes):\n",
    "        return np.min(nodes, axis=0)\n",
    "    \n",
    "\n",
    "    def create_edges(self, N, edges):\n",
    "        atom_edges = torch.zeros(N, N).int()\n",
    "        for edge in edges:\n",
    "            atom_edges[edge[0], edge[1]] = 1\n",
    "            atom_edges[edge[1], edge[0]] = 1\n",
    "\n",
    "        atom_edges2 = atom_edges @ atom_edges\n",
    "        self.atom_edge = atom_edges\n",
    "        self.atom_edge2 = atom_edges2\n",
    "        edge_attr = []\n",
    "        # Initialize edges and edge_attributes\n",
    "        rows, cols = [], []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    if atom_edges[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([0])\n",
    "                        assert not atom_edges2[i][j]\n",
    "                    if atom_edges2[i][j]:\n",
    "                        rows.append(i)\n",
    "                        cols.append(j)\n",
    "                        edge_attr.append([1])\n",
    "                        assert not atom_edges[i][j]\n",
    "\n",
    "        edges = [rows, cols] \n",
    "        edge_attr = torch.Tensor(np.array(edge_attr))  # [edge, 3]\n",
    "        edge_idx =torch.tensor(edges, dtype=torch.long)  # [2, M]   \n",
    "        return edge_idx,edge_attr     \n",
    "    \n",
    "    \n",
    "    def create_in_graph(self, edges,x,y,gt_seq):\n",
    "        pos_t, vel_t, vel_tm1 = x\n",
    "        y_dv,y_dx,y_pos_end,y_vel_end = y\n",
    "\n",
    "        edge_idx,edge_attr = self.create_edges(pos_t.shape[0], edges)\n",
    "\n",
    "        # Get the ground node\n",
    "        z0_t = self.find_min(pos_t)[1]\n",
    "        z0_end = self.find_min(y_pos_end)[1]\n",
    "        # Center the y-positions around z0 for input and target\n",
    "        #pos_t = pos_t - np.array([0, z0_t, 0])\n",
    "        #y_pos_end = y_pos_end - np.array([0, z0_end, 0])\n",
    "\n",
    "        # Get the foot node positions and indices\n",
    "        foot_nodes_positions, foot_nodes_indices = self.get_foot_nodes(pos_t)\n",
    "        foot_nodes_reflected, foot_distances = self.reflected_nodes(foot_nodes_positions,z0=z0_t)\n",
    "        \n",
    "        # Build reflected node indices\n",
    "        current_largest_node_index = pos_t.shape[0]\n",
    "        reflected_nodes_indices = list(range(current_largest_node_index,\n",
    "                                             current_largest_node_index + len(foot_nodes_indices)))\n",
    "        \n",
    "        # Convert arrays to torch\n",
    "        pos_t       = torch.tensor(pos_t, dtype=torch.float32)\n",
    "        vel_t       = torch.tensor(vel_t, dtype=torch.float32)\n",
    "        vel_tm1     = torch.tensor(vel_tm1, dtype=torch.float32)\n",
    "        y_dv        = torch.tensor(y_dv, dtype=torch.float32)\n",
    "        y_dx        = torch.tensor(y_dx, dtype=torch.float32)\n",
    "        y_pos_end   = torch.tensor(y_pos_end, dtype=torch.float32)\n",
    "        y_vel_end   = torch.tensor(y_vel_end, dtype=torch.float32)\n",
    "        foot_nodes_reflected = torch.tensor(foot_nodes_reflected, dtype=torch.float32)\n",
    "                \n",
    "        # Node types: feet=1, reflections=2, others=0\n",
    "        node_type = torch.zeros(pos_t.shape[0], 1)\n",
    "        node_type[foot_nodes_indices] = 1\n",
    "        new_node_type = torch.cat([\n",
    "            node_type,\n",
    "            2 * torch.ones(len(reflected_nodes_indices), 1)\n",
    "        ], dim=0)\n",
    "        \n",
    "        # New bidirectional edges connecting each foot to its reflection\n",
    "        # shape before transpose: [num_foot, 2]\n",
    "        new_edges = torch.stack([\n",
    "            torch.tensor(foot_nodes_indices,            dtype=torch.long),\n",
    "            torch.tensor(reflected_nodes_indices,       dtype=torch.long),\n",
    "        ], dim=0)  # [2, num_foot]\n",
    "        rev_new_edges = new_edges.flip(0)\n",
    "        new_edges_bidir = torch.cat([new_edges, rev_new_edges], dim=1)  # [2, 2*num_foot]\n",
    "\n",
    "        # Combine with original edges\n",
    "        full_edge_idx = torch.cat([edge_idx, new_edges_bidir], dim=1)\n",
    "    \n",
    "        # New edge attributes: all ones for these 1-hop reflections\n",
    "        refl_attr = torch.zeros((new_edges_bidir.shape[1], 1))\n",
    "        full_edge_attr = torch.cat([edge_attr, refl_attr], dim=0)\n",
    "\n",
    "        # new edge attr for differentiating reflected nodes\n",
    "        non_refl_edges = torch.zeros_like(edge_attr)\n",
    "        refl_edges = torch.ones_like(refl_attr)\n",
    "        new_edge_attr  = torch.cat([non_refl_edges, refl_edges], dim=0)\n",
    "\n",
    "        full_edge_attr = torch.hstack((full_edge_attr, new_edge_attr))\n",
    "        \n",
    "        # Extend node features to include reflected nodes\n",
    "        full_pos_t   = torch.cat([pos_t, foot_nodes_reflected], dim=0)\n",
    "        full_vel_t   = torch.cat([vel_t, torch.zeros_like(foot_nodes_reflected)], dim=0)\n",
    "        full_vel_tm1 = torch.cat([vel_tm1, torch.zeros_like(foot_nodes_reflected)], dim=0)\n",
    "    \n",
    "        in_graph = Data(edge_index=full_edge_idx, edge_attr=full_edge_attr)\n",
    "        in_graph.pos       = full_pos_t\n",
    "        in_graph.vel       = full_vel_t\n",
    "        in_graph.prev_vel  = full_vel_tm1\n",
    "        in_graph.y_dv       = y_dv\n",
    "        in_graph.y_dx       = y_dx\n",
    "        in_graph.end_pos   = y_pos_end\n",
    "        in_graph.end_vel   = y_vel_end\n",
    "        in_graph.node_type = new_node_type\n",
    "        in_graph.gt_seq = gt_seq\n",
    "    \n",
    "        return in_graph   \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.in_graphs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.in_graphs[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e82a9b27-e158-47e0-98da-1097e22d148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def find_min(vals):\n",
    "    return np.min(vals)\n",
    "\n",
    "def reflect_nodes(foot_nodes: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reflect the foot nodes (shape [n,3]) across the horizontal plane \n",
    "    at y = min(foot_nodes[:,1]), with a small epsilon offset.\n",
    "    \"\"\"\n",
    "    reflected = foot_nodes.copy()\n",
    "    y0 = find_min(foot_nodes[:,1])\n",
    "    reflected[:,1] = 2*y0 - foot_nodes[:,1] - epsilon\n",
    "    return reflected\n",
    "\n",
    "def visualize_multi_step(\n",
    "    test_loader,\n",
    "    model: torch.nn.Module,\n",
    "    device: torch.device,\n",
    "    steps=(1,2,3,4),\n",
    "    num_graphs=10,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    For each of `num_graphs` random graphs from test_loader:\n",
    "      • Plot initial_vs_gt.png once (vs GT at step=1)\n",
    "      • Then do a single graph rollout, saving pred_vs_gt_step{n}.png\n",
    "        for each n in `steps`, comparing to GT at that same step.\n",
    "    Uses absolute coordinates (no centering).\n",
    "    Assumes each Data has `gt_seq` as a list of length T+1,\n",
    "    each element shape (n_nodes,3).\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    model.eval()\n",
    "\n",
    "    # flatten loader\n",
    "    all_graphs = []\n",
    "    for batch in test_loader:\n",
    "        all_graphs.extend(batch.to_data_list())\n",
    "    if not all_graphs:\n",
    "        raise RuntimeError(\"No graphs in loader\")\n",
    "\n",
    "    # sample indices\n",
    "    chosen = random.sample(range(len(all_graphs)), min(num_graphs, len(all_graphs)))\n",
    "\n",
    "    # skeleton edges for the 31-body joints\n",
    "    skeleton31 = [\n",
    "      [1,0],[2,1],[3,2],[4,3],[5,4],\n",
    "      [6,0],[7,6],[8,7],[9,8],[10,9],\n",
    "      [11,0],[12,11],[13,12],[14,13],[15,14],\n",
    "      [16,15],[17,13],[18,17],[19,18],[20,19],\n",
    "      [21,20],[22,21],[23,20],[24,13],[25,24],\n",
    "      [26,25],[27,26],[28,27],[29,28],[30,27]\n",
    "    ]\n",
    "\n",
    "    for idx in chosen:\n",
    "        data = all_graphs[idx].to(device)\n",
    "        base = f\"./RESULTS/TrajectoryPlotsRefNodes/graph_{idx}\"\n",
    "        os.makedirs(base, exist_ok=True)\n",
    "\n",
    "        # — stack gt_seq list → np array [T+1,31,3] —\n",
    "        seq_list = data.gt_seq\n",
    "        seq_np = []\n",
    "        for arr in seq_list:\n",
    "            if torch.is_tensor(arr):\n",
    "                seq_np.append(arr.cpu().numpy())\n",
    "            else:\n",
    "                seq_np.append(np.array(arr))\n",
    "        gt_seq = np.stack(seq_np, axis=0)[:, :31, :]  # shape [T+1,31,3]\n",
    "\n",
    "        # initial pose (absolute)\n",
    "        init31 = data.pos[:31].cpu().numpy()\n",
    "\n",
    "        # compute axis limits from initial + all selected GT steps\n",
    "        all_pts = np.vstack([init31] + [gt_seq[k] for k in steps])\n",
    "        pad = 2.0\n",
    "        x_min, x_max = all_pts[:,2].min() - pad, all_pts[:,2].max() + pad\n",
    "        y_min, y_max = all_pts[:,0].min() - pad, all_pts[:,0].max() + pad\n",
    "        z_min, z_max = all_pts[:,1].min() - pad, all_pts[:,1].max() + pad\n",
    "\n",
    "        # — Plot initial_vs_gt.png (vs GT at step=1) —\n",
    "        gt1 = gt_seq[0]\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax  = fig.add_subplot(111, projection='3d')\n",
    "        xx, yy = np.meshgrid([x_min,x_max], [y_min,y_max])\n",
    "        ax.plot_surface(xx, yy, np.zeros_like(xx), color='gray', alpha=0.2, linewidth=0)\n",
    "        ax.scatter(init31[:,2], init31[:,0], init31[:,1],\n",
    "                   c='blue', s=30, edgecolors='k', alpha=0.5, label='Initial')\n",
    "        ax.scatter(gt1[:,2], gt1[:,0], gt1[:,1],\n",
    "                   c='red',  s=30, edgecolors='k', alpha=0.5, label='GT (0)')\n",
    "        for a,b in skeleton31:\n",
    "            ax.plot([init31[a,2], init31[b,2]],\n",
    "                    [init31[a,0], init31[b,0]],\n",
    "                    [init31[a,1], init31[b,1]],\n",
    "                    c='blue', alpha=0.6, linewidth=2)\n",
    "            ax.plot([gt1[a,2], gt1[b,2]],\n",
    "                    [gt1[a,0], gt1[b,0]],\n",
    "                    [gt1[a,1], gt1[b,1]],\n",
    "                    c='red', alpha=0.6, linestyle='-', linewidth=2)\n",
    "\n",
    "        mid_x = (x_min + x_max)/2\n",
    "        mid_y = (y_min + y_max)/2\n",
    "        mid_z = (z_min + z_max)/2\n",
    "        \n",
    "        ax.set_xlim(mid_x-20, mid_x+20)\n",
    "        ax.set_ylim(mid_y-20, mid_y+20)\n",
    "        ax.set_zlim(mid_z-20, mid_z+20)\n",
    "        ax.set_box_aspect((1,1,1))\n",
    "        ax.set_xlabel(\"X\",fontsize = 16); ax.set_ylabel(\"Y\",fontsize = 16); ax.set_zlabel(\"Z\",fontsize = 16)\n",
    "        ax.set_title(\"Initial\",fontsize = 18)\n",
    "        ax.legend(loc='upper left',fontsize = 18)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(base, 'initial_vs_gt.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # — rollout & per-step plotting —\n",
    "        graph = data.clone().to(device)\n",
    "        for step in steps:\n",
    "            m_refl = (graph.node_type==2).squeeze()\n",
    "            m_non  = ~m_refl\n",
    "            dv, dx,vm1 = model(graph.detach())\n",
    "            new_pos = graph.pos.clone()\n",
    "            new_vel = graph.vel.clone()\n",
    "            new_pos[m_non] = graph.pos[m_non] + dx\n",
    "            new_vel[m_non] = graph.vel[m_non] + dv\n",
    "            new_vel[m_refl] = 0.0        \n",
    "            # reflect foot→refl (detach before numpy)\n",
    "            foot_mask = (graph.node_type==1).squeeze().cpu().numpy()\n",
    "            foot_pts  = new_pos.detach()[foot_mask].cpu().numpy()\n",
    "            foot_ref  = reflect_nodes(foot_pts)\n",
    "            arr = new_pos.detach().cpu().numpy()\n",
    "            arr[m_refl.cpu().numpy()] = foot_ref\n",
    "            new_pos = torch.from_numpy(arr).to(device).type_as(graph.pos)\n",
    "    \n",
    "            graph.prev_vel = vm1\n",
    "            graph.vel      = new_vel\n",
    "            graph.pos      = new_pos\n",
    "\n",
    "            pred31 = graph.pos[:31].detach().cpu().numpy()\n",
    "            gt_k   = gt_seq[step]\n",
    "\n",
    "            fig = plt.figure(figsize=(6,6))\n",
    "            ax  = fig.add_subplot(111, projection='3d')\n",
    "            # optional ground plane: uncomment if desired\n",
    "            ax.plot_surface(xx, yy, np.zeros_like(xx), color='gray', alpha=0.2, linewidth=0)\n",
    "\n",
    "            ax.scatter(pred31[:,2], pred31[:,0], pred31[:,1],\n",
    "                       c='blue', s=30, edgecolors='k', alpha=0.5,\n",
    "                       label=f'Pred (step={step})')\n",
    "            ax.scatter(gt_k[:,2], gt_k[:,0], gt_k[:,1],\n",
    "                       c='red',  s=30, edgecolors='k', alpha=0.5,\n",
    "                       label=f'GT (step={step})')\n",
    "            for a,b in skeleton31:\n",
    "                ax.plot([pred31[a,2], pred31[b,2]],\n",
    "                        [pred31[a,0], pred31[b,0]],\n",
    "                        [pred31[a,1], pred31[b,1]],\n",
    "                        c='blue', alpha=0.6, linewidth=2)\n",
    "                ax.plot([gt_k[a,2], gt_k[b,2]],\n",
    "                        [gt_k[a,0], gt_k[b,0]],\n",
    "                        [gt_k[a,1], gt_k[b,1]],\n",
    "                        c='red', alpha=0.6, linestyle='-',linewidth=2)\n",
    "\n",
    "            ax.set_xlim(mid_x-20, mid_x+20)\n",
    "            ax.set_ylim(mid_y-20, mid_y+20)\n",
    "            ax.set_zlim(mid_z-20, mid_z+20)\n",
    "            ax.set_box_aspect((1,1,1))\n",
    "            ax.set_xlabel(\"X\",fontsize = 16); ax.set_ylabel(\"Y\",fontsize = 16); ax.set_zlabel(\"Z\",fontsize = 16)\n",
    "            ax.set_title(f\"Prediction vs GT — {step} steps\",fontsize = 18)\n",
    "            ax.legend(loc='upper left',fontsize = 18)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(base, f'pred_vs_gt_step{step}.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "        # optional: compute final MSE in absolute frame\n",
    "        mask_cuda = (data.node_type[:31] != 2).squeeze()\n",
    "        mask_cpu  = mask_cuda.cpu().numpy()\n",
    "        final_pred = graph.pos[:31][mask_cuda]\n",
    "        final_gt_np = gt_seq[steps[-1]][mask_cpu]\n",
    "        final_gt    = torch.from_numpy(final_gt_np).to(device)\n",
    "        mse = F.mse_loss(final_pred, final_gt).item()\n",
    "        print(f\"Graph {idx}, final step={steps[-1]}, MSE={mse:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7bf0188c-9c47-4a42-a7ae-525a72397ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded competitor split!\n",
      "[HumanDataset:test] built 600 samples\n",
      "Graph 149, final step=3, MSE=1.5524e+00\n",
      "Graph 470, final step=3, MSE=2.3620e+00\n",
      "Graph 465, final step=3, MSE=3.0457e+00\n",
      "Graph 178, final step=3, MSE=2.4498e+00\n",
      "Graph 402, final step=3, MSE=3.1716e+00\n",
      "Graph 358, final step=3, MSE=2.6889e+00\n",
      "Graph 443, final step=3, MSE=3.2106e+00\n",
      "Graph 519, final step=3, MSE=2.5725e+00\n",
      "Graph 112, final step=3, MSE=1.8241e+00\n",
      "Graph 545, final step=3, MSE=7.4355e+00\n",
      "Graph 124, final step=3, MSE=2.2987e+00\n",
      "Graph 82, final step=3, MSE=2.0036e+00\n",
      "Graph 466, final step=3, MSE=2.6060e+00\n",
      "Graph 269, final step=3, MSE=5.7925e+00\n",
      "Graph 49, final step=3, MSE=2.5588e+00\n",
      "Graph 209, final step=3, MSE=4.4556e+00\n",
      "Graph 343, final step=3, MSE=5.6653e+00\n",
      "Graph 235, final step=3, MSE=2.1201e+00\n",
      "Graph 316, final step=3, MSE=3.6286e+00\n",
      "Graph 208, final step=3, MSE=4.0845e+00\n"
     ]
    }
   ],
   "source": [
    "dataset_eval = HumanDatasetSeq(partition='test', max_samples=model_settings[\"max_testing_samples\"], data_dir=model_settings[\"data_dir\"],nsteps=4)\n",
    "\n",
    "loader = create_dataloaders_from_raw(dataset_eval,200,shuffle=False)\n",
    "\n",
    "visualize_multi_step(\n",
    "    loader,\n",
    "    trainer.model,\n",
    "    device,\n",
    "    steps=[1,2,3],\n",
    "    num_graphs=20,\n",
    "    seed=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d30ca4b3-ab35-48a6-9bf2-298bd055e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3214203/976766914.py:23: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  img = imageio.imread(png)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_343/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_519/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_269/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_82/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_316/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_112/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_49/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_545/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_465/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_358/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_470/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_402/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_178/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_209/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_235/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_466/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_149/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_124/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_208/rollout.gif with 4 frames\n",
      "Created ./RESULTS/TrajectoryPlotsRefNodes/graph_443/rollout.gif with 4 frames\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import imageio\n",
    "\n",
    "# Base directory containing graph folders\n",
    "base_dir = './RESULTS/TrajectoryPlotsRefNodes'\n",
    "\n",
    "# Iterate through each subfolder (graph_*)\n",
    "for graph_folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, graph_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    # Collect all PNG files in sorted order\n",
    "    png_files = sorted(glob.glob(os.path.join(folder_path, '*.png')))\n",
    "    if not png_files:\n",
    "        continue\n",
    "    \n",
    "    # Read each image\n",
    "    images = []\n",
    "    for png in png_files:\n",
    "        try:\n",
    "            img = imageio.imread(png)\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not read {png}: {e}\")\n",
    "    \n",
    "    # Save as infinite-loop GIF\n",
    "    gif_path = os.path.join(folder_path, 'rollout.gif')\n",
    "    imageio.mimsave(gif_path, images, duration=0.7, loop=0)\n",
    "    print(f\"Created {gif_path} with {len(images)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c489d-66a7-495f-b0d2-a18d17e088cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
